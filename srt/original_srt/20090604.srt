1
00:00:09,420 --> 00:00:11,186
Speaker 1: It's Thursday, June 4th.

2
00:00:11,487 --> 00:00:11,868
Speaker 1: I'm Rem.

3
00:00:11,888 --> 00:00:12,450
Speaker 1: I'm Scott.

4
00:00:12,510 --> 00:00:13,655
Speaker 1: And this is Geek Nights.

5
00:00:13,695 --> 00:00:15,160
Speaker 1: Tonight, levels of discourse.

6
00:00:25,950 --> 00:00:26,908
Speaker 0: Let's do this.

7
00:00:31,242 --> 00:00:33,444
Speaker 0: Today is Thursday, what's the day?

8
00:00:33,544 --> 00:00:34,747
Speaker 0: January 4th.

9
00:00:34,787 --> 00:00:35,529
Speaker 0: January 4th.

10
00:00:35,830 --> 00:00:36,912
Speaker 1: As I just said a second ago.

11
00:00:36,933 --> 00:00:38,236
Speaker 0: Wait, it's not January 4th.

12
00:00:38,296 --> 00:00:38,857
Speaker 1: June 4th.

13
00:00:39,118 --> 00:00:39,720
Speaker 0: June 4th.

14
00:00:40,563 --> 00:00:41,551
Speaker 1: It's one of the J months.

15
00:00:41,592 --> 00:00:42,459
Speaker 0: A few months off.

16
00:00:44,162 --> 00:00:53,600
Speaker 0: And on June 6th and 7th 2009 is the Museum of Comic and Cartoon Art Arts Festival.

17
00:00:55,341 --> 00:00:56,405
Speaker 0: In a new place.

18
00:00:56,545 --> 00:00:59,575
Speaker 0: It's not in the Puck building where it has previously been.

19
00:00:59,595 --> 00:01:06,192
Speaker 1: It is at the 69th Regiment Armory at 68 Lexington Avenue between 25th and 26th.

20
00:01:06,693 --> 00:01:07,977
Speaker 0: Ooh, in an armory.

21
00:01:08,017 --> 00:01:09,140
Speaker 0: I hope there's arms there.

22
00:01:09,320 --> 00:01:09,702
Speaker 1: I don't know.

23
00:01:09,722 --> 00:01:14,040
Speaker 1: As cool as arms might be, I'm really gonna miss that vintage elevator in the Puck building.

24
00:01:14,100 --> 00:01:15,780
Speaker 0: There might be a vintage elevator in the armory.

25
00:01:15,921 --> 00:01:18,999
Speaker 0: It might actually be a really big elevator meant to transport arms.

26
00:01:20,201 --> 00:01:21,245
Speaker 1: They might not let me ride it.

27
00:01:21,325 --> 00:01:28,216
Speaker 0: No, actually a lot of the armory places are like just buildings that used to be armories But now are not armories in any way shape or form.

28
00:01:28,277 --> 00:01:31,469
Speaker 0: other than that They are big buildings that were previously armories.

29
00:01:31,590 --> 00:01:37,226
Speaker 1: But suffice it to say if you listen to Geek Nights You will probably enjoy the mocha.

30
00:01:37,286 --> 00:01:42,400
Speaker 0: If you're a comic person, even if you just like reading the Sunday comics in the newspaper.

31
00:01:43,362 --> 00:01:45,899
Speaker 0: The mocha is a place to go and it is cheap.

32
00:01:46,322 --> 00:01:48,696
Speaker 0: It is like ten bucks to go there.

33
00:01:48,736 --> 00:01:49,320
Speaker 1: It's ten bucks.

34
00:01:49,560 --> 00:01:52,233
Speaker 0: And basically all you do is you don't even have to go there for that long.

35
00:01:52,273 --> 00:02:07,139
Speaker 0: You just show up, you pay your ten bucks You walk around, you look at every single table, and you talk to everybody and you hang out a little bit And then you go home and with a wallet empty and a sack full of comics that were probably not available many other places in the world.

36
00:02:08,202 --> 00:02:11,900
Speaker 1: You also pretty much get to meet the artists of any indie comic that you're reading.

37
00:02:12,001 --> 00:02:20,440
Speaker 0: It's like you go to a comic book convention, like a typical one And all the comic creators, some of them might be in the artist alley, but most of them, the high level ones anyway.

38
00:02:20,781 --> 00:02:25,340
Speaker 0: It's like you gotta pay for an autograph or maybe you gotta wait in line or they're on a panel.

39
00:02:25,781 --> 00:02:42,940
Speaker 0: At the mocha, you'll have some indie guy who printed his comic at Kinko sitting at one table And the table immediately next to him is a super famous guy And every table, the person sitting there is pretty much 90% of the time the person who made the comic and not someone else.

40
00:02:43,301 --> 00:02:48,016
Speaker 0: So the mocha is the awesome, and the win is ten bucks.

41
00:02:48,417 --> 00:02:49,160
Speaker 0: It's in New York City.

42
00:02:50,303 --> 00:02:52,600
Speaker 0: You can go Saturday or Sunday or both.

43
00:02:53,242 --> 00:02:55,309
Speaker 1: Are there any after parties that we should go to?

44
00:02:55,389 --> 00:02:58,860
Speaker 1: Because we missed everything last year, but the year before we went to that after party.

45
00:02:59,061 --> 00:03:01,111
Speaker 0: Oh yeah, I don't know about any after parties this year.

46
00:03:01,131 --> 00:03:02,720
Speaker 0: I did not investigate that situation.

47
00:03:02,800 --> 00:03:07,900
Speaker 1: We should go after party though, because kind of at the last minute half the crew is coming out to go to this thing with us.

48
00:03:07,940 --> 00:03:12,780
Speaker 0: Yeah, it being up in the 20 something-ish area, I think we should go to the conveyor belt sushi restaurant.

49
00:03:12,860 --> 00:03:16,412
Speaker 1: I think we will get some Kaiten sushi and then everyone but Scott will do karaoke.

50
00:03:16,453 --> 00:03:18,500
Speaker 1: that is also in the Kaiten sushi place.

51
00:03:18,983 --> 00:03:20,820
Speaker 0: Is karaoke in the Kaiten sushi place?

52
00:03:21,060 --> 00:03:22,639
Speaker 0: Yeah, upstairs I did not know this.

53
00:03:22,743 --> 00:03:25,168
Speaker 0: Yeah But I will not do any karaoke.

54
00:03:25,188 --> 00:03:28,019
Speaker 0: There's a lot more than ten dollars and it's stupid.

55
00:03:29,646 --> 00:03:31,113
Speaker 1: I don't know how you cannot like karaoke.

56
00:03:31,173 --> 00:03:32,660
Speaker 1: No human cannot like karaoke.

57
00:03:33,923 --> 00:03:35,259
Speaker 0: Most humans don't like karaoke.

58
00:03:35,724 --> 00:03:37,240
Speaker 1: I would argue that humans like karaoke.

59
00:03:37,300 --> 00:03:40,140
Speaker 0: Karaoke isn't the same thing as disco only I like disco.

60
00:03:40,222 --> 00:03:45,160
Speaker 0: I like disco too But I don't like karaoke and most people dislike disco and karaoke.

61
00:03:45,221 --> 00:03:47,180
Speaker 1: See the thing is there are two kinds of karaoke.

62
00:03:47,301 --> 00:03:56,120
Speaker 1: There's the lame American, you know It's a bar and there are drunk women at the front singing the same song over and over again And then there's rent a private room and hang out with friends singing.

63
00:03:56,882 --> 00:03:58,640
Speaker 0: Both kinds suck for the same reason.

64
00:03:58,661 --> 00:03:59,238
Speaker 0: Oh, you suck.

65
00:03:59,723 --> 00:04:01,876
Speaker 0: Anyway, so news time.

66
00:04:02,521 --> 00:04:03,670
Speaker 1: Yeah, I guess we don't have to do news.

67
00:04:03,710 --> 00:04:04,275
Speaker 1: It's Thursday.

68
00:04:04,315 --> 00:04:05,285
Speaker 1: It's the lounge Whatever.

69
00:04:05,326 --> 00:04:07,239
Speaker 0: I got something I want to talk about Something you want to talk about.

70
00:04:07,501 --> 00:04:11,840
Speaker 0: So there was an article in Newsweek, which is I guess which is real news, right?

71
00:04:12,040 --> 00:04:13,519
Speaker 0: Serious news, not internet news.

72
00:04:13,823 --> 00:04:22,219
Speaker 0: All right, one could say that The article was about Oprah and it called her out on all her snake oil bullshit hardcore Really?

73
00:04:22,381 --> 00:04:23,872
Speaker 0: So I was like, really?

74
00:04:24,113 --> 00:04:24,979
Speaker 0: In the real news?

75
00:04:25,362 --> 00:04:31,260
Speaker 0: And it was a big long article that got everything right And it talked about how there are these two good doctors who are on Oprah.

76
00:04:31,401 --> 00:04:35,600
Speaker 0: But there's all these other bullshit people and Oprah defends anyone who comes on her show.

77
00:04:35,700 --> 00:04:41,960
Speaker 0: And it really like picked apart Oprah's psychology And like, you know, how she why she's all fucked up in the head.

78
00:04:42,041 --> 00:04:43,700
Speaker 1: Oprah has already responded.

79
00:04:44,581 --> 00:04:50,340
Speaker 0: Oh, she responded because there was a. there was something about like she wouldn't come.

80
00:04:50,542 --> 00:04:55,360
Speaker 0: It said in the article that she wouldn't comment But there was like an official like two sentence statement or something.

81
00:04:55,360 --> 00:04:55,521
Speaker 0: Yes,

82
00:04:55,561 --> 00:04:59,846
Speaker 1: the statement is, quote, for 23 years, my show has presented thousands of That's

83
00:04:59,886 --> 00:05:00,921
Speaker 0: the statement, yes Yes,

84
00:05:00,941 --> 00:05:10,780
Speaker 1: that reflect the human experience, including doctor's medical advice and personal health stories that have prompted conversations between our audience and members of their health care providers, audience members and their health care providers.

85
00:05:11,581 --> 00:05:17,299
Speaker 1: I trust the viewers and I know that they are smart and discerning enough to seek out medical opinions to determine what may be best for them.

86
00:05:17,564 --> 00:05:22,620
Speaker 0: So she said nothing Right, and the thing is, right, she she says I trust the viewers, right?

87
00:05:22,660 --> 00:05:35,160
Speaker 0: But at the same time, right, one time she had someone on promoting some like Randy, you know, shit, and someone then like wrote her a letter that was like, yeah, because you I'm not getting chemo, I'm going to do the Randy shit instead for my cancer.

88
00:05:35,501 --> 00:05:41,160
Speaker 0: And then Oprah had to tell the person, you know, actually, you should probably go get your chemo and your radiation.

89
00:05:41,722 --> 00:05:47,218
Speaker 0: So you trust the viewers, but then you there you realize that you told them wrong and you have to correct them.

90
00:05:47,641 --> 00:06:01,340
Speaker 1: You see, the thing is, while people argue that Oprah has done a lot for communities in general, I still cannot forgive her for the fact that she constantly and blatantly just kowtows to idiocy and Randy bullshit.

91
00:06:01,461 --> 00:06:06,959
Speaker 0: Well, basically anything that's on her show is something she's promoting and endorsing and defending, right?

92
00:06:07,100 --> 00:06:09,278
Speaker 1: And she endorses and promotes way too much BS.

93
00:06:09,460 --> 00:06:30,760
Speaker 0: She'll just endorse and promote whatever, you know, and she's so, you know, she's not I mean, what really are Oprah's credentials other than I have a lot of money and I've been on TV for a long time and I know how to speak in a way that, you know, is makes my, you know, she's just a really good salesperson to a very specific audience.

94
00:06:30,861 --> 00:06:34,220
Speaker 1: That's that's our only came out of the woodwork already.

95
00:06:34,501 --> 00:06:39,940
Speaker 1: All the news I can find all the opinion pieces are people lambasting this article.

96
00:06:40,302 --> 00:06:42,820
Speaker 1: How dare they attack Oprah, a pillar of the community?

97
00:06:43,143 --> 00:06:48,180
Speaker 1: They're just attacking her because they're racist Racist, okay Or sexist or sexist?

98
00:06:48,281 --> 00:06:50,880
Speaker 1: It seems like it's equal parts race, equal parts sex Right.

99
00:06:51,403 --> 00:06:54,840
Speaker 0: Read the Newsweek article and know that's spelled wrong too.

100
00:06:54,860 --> 00:06:55,260
Speaker 0: That's great.

101
00:06:55,301 --> 00:06:56,340
Speaker 0: Read the Newsweek article.

102
00:06:56,420 --> 00:07:01,098
Speaker 0: The Newsweek article is 100% correct and anything that disagrees with it is wrong.

103
00:07:03,264 --> 00:07:07,224
Speaker 0: And if you disagree with it, you're stupid Stupid.

104
00:07:08,443 --> 00:07:09,550
Speaker 0: So Oprah's a bitch.

105
00:07:10,305 --> 00:07:11,098
Speaker 0: She has too much money.

106
00:07:11,421 --> 00:07:37,720
Speaker 1: So in some personal news, since we want to get right to the main bit, I think it's an interesting topic, but I'm sure many of you in the forums have seen all the rumblings and people who follow my tweet, tweet, Twitter, tweeter that I don't update that often because I hate Twitter, but I use it occasionally and I've been in the market for a house and I found a house and things are going well and it looks like very soon I'm going to be writing a very, very, very large check and becoming a member of the landowning gentry.

107
00:07:37,902 --> 00:07:39,780
Speaker 1: Finally, I will be able to vote.

108
00:07:40,285 --> 00:07:41,680
Speaker 1: I will be able to speak before the Senate.

109
00:07:41,700 --> 00:07:44,820
Speaker 1: I will get all the privileges of the landowning upper class.

110
00:07:45,241 --> 00:07:50,866
Speaker 0: That's, that, it's, it's 19, it's, actually no, it's 2009, not 1709.

111
00:07:50,866 --> 00:07:52,040
Speaker 0: So that When did this happen?

112
00:07:52,381 --> 00:07:56,980
Speaker 1: Yeah So you're saying that people who don't even own land have the right to vote?

113
00:07:57,320 --> 00:07:57,738
Speaker 0: I've voted.

114
00:07:58,686 --> 00:07:59,640
Speaker 0: You don't own any land.

115
00:07:59,660 --> 00:08:02,138
Speaker 1: I know That, that's a travesty.

116
00:08:02,300 --> 00:08:08,460
Speaker 1: Well How can, without the noblesse obligé, how can one possibly make complex decisions?

117
00:08:08,581 --> 00:08:15,180
Speaker 0: Well, in fact, the people who own land get fucked more because you have to pay the land tax that we, uh, we voted in, us non-landowners.

118
00:08:15,361 --> 00:08:17,960
Speaker 1: I thought taxes only applied to poor people and immigrants.

119
00:08:18,221 --> 00:08:21,760
Speaker 0: No, the taxes, in fact, apply more to the people with the land.

120
00:08:21,941 --> 00:08:24,639
Speaker 1: Well, luckily I have land so I can do something about it.

121
00:08:25,343 --> 00:08:28,160
Speaker 0: Uh, you prepared that well in advance.

122
00:08:28,961 --> 00:08:32,940
Speaker 1: Actually, no, I was going to make a different set of jokes But I can't tell you why, it's too stupid.

123
00:08:33,783 --> 00:08:35,198
Speaker 1: You still won't tell me what that is.

124
00:08:35,421 --> 00:08:37,259
Speaker 0: I wonder if anyone listening knows what that's for.

125
00:08:38,140 --> 00:08:41,258
Speaker 1: It's, uh, it's weird because I feel like I know what that's from, but I don't.

126
00:08:41,359 --> 00:08:42,620
Speaker 0: actually You don't, there's no way you know.

127
00:08:42,620 --> 00:08:44,459
Speaker 1: No, I know I don't, it's just too stupid.

128
00:08:46,321 --> 00:08:54,780
Speaker 1: But no, it's almost like I have an unreferenced pointer, I have this pointer and I see the pointer there and I say, all right, that pointer means I know what this is.

129
00:08:54,800 --> 00:08:57,900
Speaker 1: So I go in my brain to the pointer and it's not pointing anything It's a null pointer.

130
00:08:58,001 --> 00:09:00,279
Speaker 1: It's a null pointer, but I keep trying to reference it.

131
00:09:00,501 --> 00:09:02,839
Speaker 0: I will, uh, I will de-nullify your pointer soon enough.

132
00:09:03,552 --> 00:09:05,760
Speaker 1: Uh, okay You gonna wait for the crew to come out?

133
00:09:05,920 --> 00:09:08,520
Speaker 1: Because we have to show them a story from North America first.

134
00:09:08,944 --> 00:09:11,560
Speaker 0: No, no, it's not, it's not a video It's not a video?

135
00:09:11,780 --> 00:09:14,640
Speaker 0: No All right It's not, it's not something that can be gotten quickly.

136
00:09:21,273 --> 00:09:22,809
Speaker 1: But anyway, things of the day.

137
00:09:23,272 --> 00:09:26,930
Speaker 1: So you ever wonder what happens to all those hats on the ice after a hat trick?

138
00:09:27,711 --> 00:09:29,970
Speaker 0: You don't get them back, I know that much.

139
00:09:30,172 --> 00:09:34,090
Speaker 0: I don't wonder what happens If you're throwing a hat, just, this is a note for people, right?

140
00:09:34,190 --> 00:09:47,047
Speaker 0: I was at the New York Giants Super Bowl victory parade Uh, wow, already two years ago And, uh, a kid in front of me, like, took his friend's, you know, Super Bowl victory hat And threw it to get it autographed Asshole.

141
00:09:47,851 --> 00:09:52,370
Speaker 0: The hat does not come back in, you know, it just doesn't happen.

142
00:09:52,571 --> 00:09:59,570
Speaker 0: So if you're getting ready to throw a hat on the ice, not an expensive hat Do not throw your fancy hat.

143
00:09:59,610 --> 00:10:10,510
Speaker 1: Well apparently the majority of hats that are thrown are either old, worn hats Like one that someone's been wearing forever Or they're just a throwaway hat that someone brought in a bag to throw in case there was a hat trick.

144
00:10:10,531 --> 00:10:17,069
Speaker 1: Exactly I used to, whenever I went to a hockey game, I always carried with me I had this really dumb hat that I hated.

145
00:10:17,693 --> 00:10:20,650
Speaker 1: It was just like, it had this, like, metal mesh on the brim.

146
00:10:21,210 --> 00:10:23,850
Speaker 0: Yeah, so you had a hat in one bag and an octopus in the other?

147
00:10:24,531 --> 00:10:25,550
Speaker 1: I didn't carry the octopus.

148
00:10:25,671 --> 00:10:29,050
Speaker 1: Octopi are too smart for me to feel happy throwing one on the ice.

149
00:10:29,334 --> 00:10:30,128
Speaker 0: They like it on the ice.

150
00:10:30,718 --> 00:10:31,450
Speaker 1: Nah, they're already dead.

151
00:10:31,491 --> 00:10:32,489
Speaker 0: They live in cold waters.

152
00:10:34,874 --> 00:10:41,950
Speaker 1: But I've seen octopuses thrown on the ice because I know it's a hockey tradition But it's much more prevalent in the midwest than it is everywhere else.

153
00:10:42,030 --> 00:10:56,569
Speaker 1: I mean, when I went to the Red Wing Stanley Cup playoffs back in the day, like '96, '97 They had a giant octopus thing called Stanley that would come down from the ceiling over the ice Called Stanley?

154
00:10:56,850 --> 00:10:58,728
Speaker 1: It was called Stanley, Stanley the Octopus.

155
00:11:00,074 --> 00:11:01,189
Speaker 0: That's not the thing to name it.

156
00:11:01,331 --> 00:11:13,710
Speaker 1: Little known fact, all you non-hockey fans You throw an octopus on the ice because there used to be eight games in the series Now it's seven, but they still throw octopuses because there are no seven-legged animals.

157
00:11:13,851 --> 00:11:23,530
Speaker 1: Yep But apparently, well, not apparently I found this article that explains what happens to the hats and it's kind of interesting.

158
00:11:23,997 --> 00:11:24,769
Speaker 0: What happens to the hats?

159
00:11:25,191 --> 00:11:27,090
Speaker 1: Oh, should I tell anyone or should I read the article?

160
00:11:27,170 --> 00:11:29,310
Speaker 0: Well, is it really that complex that you can't tell it quickly?

161
00:11:29,531 --> 00:11:39,970
Speaker 1: One, the players keep them, usually they take all the hats that aren't disgusting And they leave them in the dressing room of the player who scored the hat trick And he can do with them whatever he wants.

162
00:11:39,990 --> 00:11:40,426
Speaker 0: That's nice.

163
00:11:41,372 --> 00:11:45,929
Speaker 0: They throw away the scary hats The nasty hats the guy doesn't want to keep.

164
00:11:46,674 --> 00:11:48,950
Speaker 0: I would want all the hats, every single one.

165
00:11:49,332 --> 00:11:50,240
Speaker 1: Some teams actually.

166
00:11:50,280 --> 00:11:55,250
Speaker 1: what they do is they take all the hats They send them to a company just to get them steam cleaned really quick And then they donate them to charity.

167
00:11:55,515 --> 00:11:56,026
Speaker 0: That's good too.

168
00:11:56,431 --> 00:11:59,090
Speaker 0: You know, that's what happens to the, because you know the Super Bowl, right?

169
00:11:59,170 --> 00:12:11,610
Speaker 0: When the team wins, suddenly there's all these hats and t-shirts that say, you know New York Giants, Super Bowl 42 champions And all the hats and t-shirts that say New England Patriots, Super Bowl 42 champions.

170
00:12:12,012 --> 00:12:16,810
Speaker 0: They send those to Africa and places where, you know, no one in America will ever see them.

171
00:12:17,231 --> 00:12:21,229
Speaker 1: There was one time I did not, I had the hat, I did not throw it at the hat trick.

172
00:12:21,990 --> 00:12:22,094
Speaker 1: Whoa?

173
00:12:22,530 --> 00:12:28,870
Speaker 1: Because Sergei Fedorov scored two goals and he accidentally scored one against the Red Wings.

174
00:12:29,793 --> 00:12:31,590
Speaker 1: So it was technically a hat trick.

175
00:12:31,610 --> 00:12:32,346
Speaker 0: No, it's not.

176
00:12:34,573 --> 00:12:36,210
Speaker 1: The commentators counted it as one of the least.

177
00:12:36,210 --> 00:12:37,187
Speaker 0: It's not a hat trick.

178
00:12:37,896 --> 00:12:38,828
Speaker 1: Well, they lost the game too.

179
00:12:39,072 --> 00:12:47,608
Speaker 1: That's another kind of trick Of course, my brother, if you think I was into hockey, my brother, I remember he was Like the noise that came out of him was like a wookie dying.

180
00:12:51,571 --> 00:12:55,010
Speaker 1: There's also a picture here of a case, apparently the Columbus Blue Jackets.

181
00:12:55,931 --> 00:13:11,750
Speaker 1: Every time there's a hat trick, they put all the hats from the ice in this bin And they keep the bin on display And they've put every hat from every hat trick they've ever gotten since their first hat trick Which was in 2001 and there are not that many hats in there.

182
00:13:11,951 --> 00:13:14,430
Speaker 0: Well, it's not, it's a relatively new team and they're not that good.

183
00:13:14,653 --> 00:13:16,987
Speaker 0: Yeah, there aren't that many hats You know, they're not terrible, but they're not that good.

184
00:13:17,861 --> 00:13:18,748
Speaker 1: Okay That's all I got.

185
00:13:19,192 --> 00:13:22,630
Speaker 0: So I'm pretty sure this is not a duplicate thing of the day.

186
00:13:22,691 --> 00:13:24,390
Speaker 0: If it is, shoot me, right?

187
00:13:25,234 --> 00:13:28,469
Speaker 0: I checked the thing of the day feed and it was not in there.

188
00:13:29,113 --> 00:13:33,230
Speaker 0: And also I think I posted it in a forum maybe Just because I couldn't wait to post it somewhere.

189
00:13:33,331 --> 00:13:35,188
Speaker 0: But I'm pretty sure it was not a thing of the day.

190
00:13:35,793 --> 00:13:41,910
Speaker 0: Basically what this is, is a game called Wayfarer And what it is, it's a roguelike game.

191
00:13:42,132 --> 00:13:50,470
Speaker 0: If you've never played a roguelike game I think we've discussed them on the show previously It's pretty much a game just like NetHack or like Rogue, the original roguelike.

192
00:13:51,333 --> 00:13:52,790
Speaker 0: It's basically a dungeon crawl.

193
00:13:52,971 --> 00:13:55,730
Speaker 0: You walk around, you kill bad guys, you get treasure.

194
00:13:55,831 --> 00:14:00,450
Speaker 0: You know, you explore the dungeon, you know You see more dungeon as you explore.

195
00:14:00,551 --> 00:14:10,050
Speaker 0: The dungeon is randomly created You know, it's pretty much, you know, a straight up kind of RPG But not RPG in the good way, RPG in the dean.

196
00:14:11,074 --> 00:14:12,430
Speaker 0: It's like hero quest, one player.

197
00:14:12,716 --> 00:14:14,228
Speaker 0: That's what it is All right Right.

198
00:14:14,572 --> 00:14:21,390
Speaker 0: So Wayfarer is a roguelike Because most of them are sort of difficult to play and or annoying.

199
00:14:22,111 --> 00:14:46,590
Speaker 0: This is a web-based one that is Java powered, I think It's got some 3D action, but not really And it actually has a really good user interface And it makes it like a lot more pleasant to play A lot more accessible And actually sort of maybe a little bit fun And definitely a million times more fun than other roguelikes As far as I'm concerned Even though it's maybe less complex than other roguelikes.

200
00:14:46,630 --> 00:14:49,509
Speaker 0: But I mean, it's still alpha and it's already made it that far.

201
00:14:49,932 --> 00:15:00,728
Speaker 0: So I think there is actually a significant potential in this game And if you are bored and you got nothing better to do And you're on a computer with the web browser And you know, all the other games aren't really exciting.

202
00:15:00,769 --> 00:15:02,568
Speaker 0: you Why not?

203
00:15:02,730 --> 00:15:04,789
Speaker 0: It's free and it's an alpha, what the hell?

204
00:15:06,150 --> 00:15:07,149
Speaker 1: So onto the topic.

205
00:15:07,312 --> 00:15:08,250
Speaker 1: And we were inspired.

206
00:15:08,432 --> 00:15:12,190
Speaker 1: There was an article linked on Slashshot The perils of pop philosophy.

207
00:15:13,115 --> 00:15:14,190
Speaker 1: You should all read the article.

208
00:15:14,230 --> 00:15:17,761
Speaker 1: We're not really going to talk about the article in depth But it's a fantastic article Pretty

209
00:15:17,782 --> 00:15:17,908
Speaker 0: good

210
00:15:18,393 --> 00:15:37,575
Speaker 1: And it got us to talking a lot about the topic And then from there to this kind of broader concern That it's one of those things where the more I think about it The more I realize just how much it controls Or at least influences the interactions I have every day With every information source or destination in my entire life Including

211
00:15:37,636 --> 00:15:38,044
Speaker 0: this show.

212
00:15:38,105 --> 00:15:40,247
Speaker 0: even Definitely This show is a victim, right?

213
00:15:40,831 --> 00:15:43,050
Speaker 0: So here is the basic problem here, right?

214
00:15:43,250 --> 00:15:43,534
Speaker 0: Is?

215
00:15:43,574 --> 00:15:51,470
Speaker 0: the guy talks about The article references another article And the other article is discussing some philosophical topic, right?

216
00:15:52,152 --> 00:15:59,210
Speaker 0: And the guy who's writing this article Says basically Yeah, that other article is a load of shit, right?

217
00:15:59,890 --> 00:16:04,290
Speaker 0: But you wouldn't really know unless you were, you know, like a philosophy expert.

218
00:16:04,330 --> 00:16:13,830
Speaker 0: And he says here is my counter argument to that other article And it's a whole bunch of big fancy philosophy words That nobody who doesn't have a philosophy degree will be able to understand.

219
00:16:13,931 --> 00:16:15,209
Speaker 0: I didn't understand what he was saying at all.

220
00:16:15,512 --> 00:16:25,370
Speaker 1: What he basically, the gist of what he points out Is that if someone, it's very easy to make a stupid claim Or a fallacious argument It is very easy to refute that argument.

221
00:16:25,715 --> 00:16:36,310
Speaker 1: If you know about the topic It is almost impossible to refute that argument In a way that people who aren't familiar with the field Will see or understand.

222
00:16:36,350 --> 00:16:44,190
Speaker 0: Yeah, I mean by using all his big philosophy words And all his philosophy knowledge He was able to easily and trivially refute this other argument.

223
00:16:44,290 --> 00:16:59,230
Speaker 0: However, the only people who could understand and comprehend the refutation Were other philosophy experts And for him to make that refutation accessible to the mass populace He'd basically have to write an entire book Worth of normal non-philosophy talk, right?

224
00:16:59,730 --> 00:17:01,670
Speaker 1: And it goes further than this when you really think about it.

225
00:17:01,670 --> 00:17:11,710
Speaker 1: Because say someone makes a fallacious claim And even you're not trying to convince anyone else You don't have to explain the refutation to anyone else Just you're trying to argue with the person who made the fallacious claim.

226
00:17:12,851 --> 00:17:16,530
Speaker 1: Alright, you have ten different ways to refute them.

227
00:17:16,957 --> 00:17:21,198
Speaker 1: So you use one And they don't understand it Or you use two and they don't understand it Or you can't-.

228
00:17:21,930 --> 00:17:26,690
Speaker 1: Basically it comes down to the fact that you can't engage them On the level where the claim is fallacious.

229
00:17:27,194 --> 00:17:28,686
Speaker 1: If they're not able to engage you back.

230
00:17:29,253 --> 00:17:30,362
Speaker 1: They're not able to reciprocate Right?

231
00:17:30,613 --> 00:17:37,250
Speaker 1: So if I engage- If I make a claim at level one And someone else refutes it at level one Basically they're equal.

232
00:17:37,331 --> 00:17:40,350
Speaker 1: There's no real way to determine which one is right.

233
00:17:40,370 --> 00:17:42,470
Speaker 0: It happens with science a lot, right?

234
00:17:42,510 --> 00:17:51,590
Speaker 0: You get like some person who comes around And they're not an expert in science They don't know anything about, you know They know the world is made of atoms and molecules And that's about it, right?

235
00:17:51,910 --> 00:17:57,549
Speaker 0: So they come and say some bullshit about evolution or something And they're completely wrong.

236
00:17:58,215 --> 00:18:01,830
Speaker 0: But you go to explain why they're wrong And you're a biologist.

237
00:18:02,192 --> 00:18:05,490
Speaker 0: So the only people who understand what you're saying are biologists.

238
00:18:05,671 --> 00:18:13,168
Speaker 0: And for you to get this non-biologist person to understand why they're wrong You basically have to give them a university biology education.

239
00:18:13,228 --> 00:18:13,509
Speaker 0: first.

240
00:18:13,972 --> 00:18:27,290
Speaker 1: It comes down to the fact that the refutation of some kinds of fallacies Is in a higher order than the fallacy itself And it's not- So it's one order to make the fallacy But it is impossible to refute the fallacy without making a higher order argument.

241
00:18:27,452 --> 00:18:31,850
Speaker 0: And it's not just in arguing either It's just in general comprehension, right?

242
00:18:31,990 --> 00:18:41,489
Speaker 0: I mean, you know, some guy writes a newspaper article that speaks very colloquially And is completely wrong because it, you know, summarizes and generalizes.

243
00:18:41,792 --> 00:18:46,670
Speaker 0: But people understand it And it sort of makes sense on the level at which people are thinking.

244
00:18:46,791 --> 00:18:56,030
Speaker 0: And these people have never thought about the topic on a higher level And don't have the education or expertise or whatever they need To understand the higher levels of the particular topic.

245
00:18:56,713 --> 00:19:00,030
Speaker 0: And as it stands in that state, it makes perfect sense.

246
00:19:00,151 --> 00:19:01,589
Speaker 0: And they go on believing this thing.

247
00:19:02,552 --> 00:19:11,248
Speaker 0: And that's that, you know People think in their minds about things, you know People- some guy who's, say, I don't know, an engineer, right?

248
00:19:11,890 --> 00:19:23,430
Speaker 0: Will think about, say, you know, medicine in some simple way Because he's not an engineer But he's thinking about medicine in the highest order that he can think about medicine And he'll come up with something.

249
00:19:24,132 --> 00:19:28,667
Speaker 0: And thus he now, you know, believes something about medicine.

250
00:19:28,707 --> 00:19:35,470
Speaker 0: that's probably wrong Because, you know, he doesn't have the knowledge in that higher order in that department.

251
00:19:36,631 --> 00:19:38,450
Speaker 1: But what I really want to talk about?

252
00:19:38,511 --> 00:19:56,289
Speaker 1: We could go on and on about this specific topic But there's not a lot to say But I think the more general interesting topic is the interactions between people on the internet Or in real life where one person wants to engage on one level Another person can't or won't engage on that level.

253
00:19:57,373 --> 00:20:04,930
Speaker 1: Or the situation where you have a topic Where the only way you can have a meaningful conversation is to engage at a higher level.

254
00:20:05,211 --> 00:20:08,950
Speaker 0: Right, because all of the low-level things are sort of already figured out.

255
00:20:09,050 --> 00:20:16,310
Speaker 0: I mean, when you try to talk philosophy with your friends You might think you're all smart and stuff, but really It's all just shadows on a wall.

256
00:20:16,435 --> 00:20:23,510
Speaker 0: Yeah, right The thing is, right, you know, real philosophers, you know People who really know this shit and write books.

257
00:20:23,913 --> 00:20:26,090
Speaker 0: And, well, not everyone, some people write bullshit books.

258
00:20:26,191 --> 00:20:28,130
Speaker 0: But, you know, real philosophers, right?

259
00:20:28,752 --> 00:20:31,166
Speaker 0: Wait, wait, real Real We're getting some Scotsman in the house.

260
00:20:31,531 --> 00:20:33,669
Speaker 0: I understand that's a Faujo Andrew Scotsman.

261
00:20:33,951 --> 00:20:40,568
Speaker 1: But, oh, someone made a website of a graphic for every logical fallacy And the Andrew Scotsman was just an angry willy.

262
00:20:40,853 --> 00:20:41,228
Speaker 0: That's good.

263
00:20:41,552 --> 00:20:48,830
Speaker 0: But they, you know, they figured out everything you were saying And you're basically treading over well-tread ground.

264
00:20:48,991 --> 00:20:52,230
Speaker 0: You're rebuilding already built wheels, you know.

265
00:20:52,712 --> 00:21:05,470
Speaker 0: So, in order to sort of get into new areas of thought You really have to sort of learn all the, you know, things that have come before And crawl up to the, you know, to the higher level.

266
00:21:05,530 --> 00:21:10,910
Speaker 0: Imagine if, say, we're all, all of humanity together is building a ladder, right?

267
00:21:10,990 --> 00:21:13,849
Speaker 0: And the ladder is 10 steps tall and we need the 11th step.

268
00:21:14,252 --> 00:21:17,330
Speaker 0: But only people on the 10th step can build the 11th step.

269
00:21:17,434 --> 00:21:17,809
Speaker 0: You know what?

270
00:21:17,870 --> 00:21:23,269
Speaker 0: You're down on the third step You don't know the fourth step exists and you think you're all smart Figuring out the fourth step.

271
00:21:23,411 --> 00:21:24,588
Speaker 1: You know, you know what you just said?

272
00:21:25,030 --> 00:21:27,770
Speaker 1: It's almost word for word from the fourth way.

273
00:21:28,119 --> 00:21:30,609
Speaker 0: I know Stupid fourth way.

274
00:21:31,034 --> 00:21:40,750
Speaker 0: Oh, so close, but still crazy Right, you know, and it's sort of like, hey, if you don't, you know, see all the steps that are already there You're just, you know, what are you doing?

275
00:21:40,810 --> 00:21:41,910
Speaker 1: I don't think we need more analogies.

276
00:21:41,910 --> 00:21:43,429
Speaker 0: I think it's I love analogies.

277
00:21:43,531 --> 00:21:44,570
Speaker 0: I'm the analogy master.

278
00:21:44,771 --> 00:21:47,630
Speaker 1: It's a real world thing where I have this problem personally.

279
00:21:48,092 --> 00:21:51,790
Speaker 1: My interest in the topic is that many times I'll be in.

280
00:21:52,413 --> 00:21:54,384
Speaker 1: I guess I'm in low level mode.

281
00:21:54,404 --> 00:22:00,970
Speaker 1: when I'm walking around in the street I don't expect anyone to a stranger to out of the blue while I'm like walking and eating my baguette.

282
00:22:01,393 --> 00:22:04,050
Speaker 1: to engage me suddenly on a high level.

283
00:22:04,070 --> 00:22:08,330
Speaker 1: There's usually kind of like, we all go through this When you talk to someone, you don't know where there's the probe.

284
00:22:08,391 --> 00:22:09,170
Speaker 1: You probe each other.

285
00:22:09,170 --> 00:22:16,790
Speaker 1: It's like a protocol, a handshake Where you both feel out what the level of discourse is going to be for the remainder of this new conversation This new encounter.

286
00:22:17,113 --> 00:22:18,990
Speaker 1: You know, you make the comment about the weather.

287
00:22:19,615 --> 00:22:21,250
Speaker 1: They say something back about the weather.

288
00:22:21,411 --> 00:22:22,910
Speaker 1: You've established that baseline level.

289
00:22:23,534 --> 00:22:25,670
Speaker 1: You mention, you make an offering about politics.

290
00:22:26,234 --> 00:22:29,330
Speaker 1: You see if they make a high or low level statement And then you respond accordingly.

291
00:22:29,451 --> 00:22:31,850
Speaker 1: And then the discourse is set and you have your discussion.

292
00:22:31,971 --> 00:22:35,810
Speaker 1: But much like in Burning Wheel, the blossoms are falling setting.

293
00:22:37,878 --> 00:22:41,330
Speaker 1: I was thinking about this a lot In that setting, it's based in like old Japan.

294
00:22:42,212 --> 00:22:52,044
Speaker 1: So there are different levels of formality of the Japanese language And the rules in the game are that you test against another player to see what you as a group set the level of discourse.

295
00:22:52,285 --> 00:23:01,050
Speaker 1: as for your discussion And depending on the level of discourse Some topics are or are not allowed to be broached without a serious breach of etiquette.

296
00:23:02,500 --> 00:23:04,030
Speaker 1: And the same thing happens in real life.

297
00:23:04,111 --> 00:23:09,829
Speaker 1: I mean, say I'm sitting on the train and guy next to me says something about politics Like blah, blah, Obama.

298
00:23:10,535 --> 00:23:13,870
Speaker 1: Now I have a choice to engage them on their level Higher level, lower level.

299
00:23:13,971 --> 00:23:15,309
Speaker 1: I don't know really what their level is at.

300
00:23:16,034 --> 00:23:46,390
Speaker 1: But if they won't engage on the higher level Like if someone is just a Democrat or a Republican And the only argument they have for being a Democrat or a Republican is like the talking point of their pundit of choice on either side There is no possible way I can have a conversation that'll be meaningful with this person Unless I either force them to engage on a higher level Or I don't know, not talk to them and not have the... I guess it might be meaningful for them But how is it meaningful for me if they only have talking points?

301
00:23:46,754 --> 00:23:48,050
Speaker 1: I've already heard every talking point.

302
00:23:48,333 --> 00:23:50,070
Speaker 1: I already have a reputation for every talking point.

303
00:23:50,131 --> 00:23:51,348
Speaker 0: They can't comprehend any of them.

304
00:23:51,713 --> 00:23:53,330
Speaker 1: So what's the point of even discoursing?

305
00:23:53,490 --> 00:23:55,149
Speaker 1: What's the point of even engaging at that point?

306
00:23:55,450 --> 00:24:00,170
Speaker 0: Well, I mean, you know, to give you, you know In that example, I can't think of a reason to engage.

307
00:24:00,311 --> 00:24:04,490
Speaker 0: But for example, on our show We will often discuss things related to game theory, right?

308
00:24:04,610 --> 00:24:20,370
Speaker 0: But we will not engage the game theory topics on the highest level You know, and the point is that even though the audience Some of the audience, I'm sure, would understand it But most of the audience, we assume, does not And we try to, you know, say things in a way that they will comprehend them.

309
00:24:20,430 --> 00:24:22,629
Speaker 1: You know, we won't say "Oh, that's...".

310
00:24:22,891 --> 00:24:30,370
Speaker 0: You know, because I mean, we could end the show in like a second Like we're discussing a game, we'll be like "Yeah, that's just, you know, zero sum perfect information game".

311
00:24:30,571 --> 00:24:31,129
Speaker 0: "Oh, okay".

312
00:24:31,830 --> 00:24:33,650
Speaker 1: Yeah, well, how many states does it have?

313
00:24:33,650 --> 00:24:34,669
Speaker 1: 10 to the sixth?

314
00:24:35,070 --> 00:24:35,745
Speaker 0: Yeah, all right.

315
00:24:36,591 --> 00:24:56,250
Speaker 0: So, all right, but, you know, instead, you know When we're discussing on the show, sometimes we either A) We take the route of giving all the full We engage on the high level, but we break the words down And go on longer so that they get the full idea Or we simplify to get just given the gist, right?

316
00:24:56,410 --> 00:25:00,290
Speaker 0: And hopefully we can educate them over time and bring them up to the higher level.

317
00:25:00,614 --> 00:25:02,609
Speaker 1: How noble of you I know, right?

318
00:25:03,210 --> 00:25:06,370
Speaker 0: But, you know, there are many, I'm sure, you know That's on the topics that we're familiar with.

319
00:25:06,611 --> 00:25:09,630
Speaker 0: There are many topics for which we cannot engage on the high level.

320
00:25:09,670 --> 00:25:11,990
Speaker 1: For example, we'll say something about, we talk about viruses.

321
00:25:11,990 --> 00:25:14,289
Speaker 0: How many times do we say something stupid on the show?

322
00:25:14,690 --> 00:25:17,890
Speaker 0: Every day, we've probably already said 20 stupid things on the show.

323
00:25:18,051 --> 00:25:19,810
Speaker 1: How many times do we say something about viruses?

324
00:25:20,370 --> 00:25:32,329
Speaker 1: And then, you know, the next morning I get an email from Scott Johnson and Pete and Lisa Yeah, right Viruses do not work that way You know, or we say stupid things about like economics or who knows what else.

325
00:25:32,732 --> 00:25:44,970
Speaker 1: I mean, that other article really points it out is that The long form debate almost never happens in modern society Because no one has the patience and no one's going to pay attention.

326
00:25:45,351 --> 00:25:53,730
Speaker 1: And generally the people who are up for a long time debate or a long form debate Are the ones who are already trying to engage at a higher level in the first place.

327
00:25:53,870 --> 00:25:59,049
Speaker 0: Exactly, I mean, you know, and this goes back to that Neil Postman book that you disagreed with, right?

328
00:25:59,430 --> 00:26:02,510
Speaker 0: Where he was talking about how, you know, the medium is the message.

329
00:26:02,590 --> 00:26:08,890
Speaker 0: He talked about how Lincoln Douglas would, you know, debate for reals And people would sit there for hours and they would debate at a very high level.

330
00:26:09,392 --> 00:26:13,330
Speaker 0: And if you look at the old newspapers, you know, it was much higher level.

331
00:26:13,390 --> 00:26:17,389
Speaker 1: I guess I disagree with a lot of his conclusions Because a lot of what he predicted did not come to pass.

332
00:26:18,892 --> 00:26:31,230
Speaker 0: I don't know, that book was from the 80s, I think Yeah Right, so, I mean, you know, the major point of the book, though, was that, you know, The medium in which you are communicating, right, you know, modifies the message.

333
00:26:31,431 --> 00:26:39,170
Speaker 0: On TV, you can't have these long, boring things Because when you do, you get, like, that boring PBS news show that nobody watches.

334
00:26:39,170 --> 00:26:40,589
Speaker 1: But, Scott, I love things like that.

335
00:26:40,771 --> 00:26:42,610
Speaker 0: You do, but most people don't, right?

336
00:26:42,651 --> 00:26:43,390
Speaker 1: Yes, but I don't think it's.

337
00:26:43,390 --> 00:26:49,069
Speaker 0: And he talks about how, you know, this medium is, you know, The mediums of the olden days are better for the long-form, higher level.

338
00:26:49,512 --> 00:26:52,110
Speaker 0: And you look at the internets, right, the internet is good for Twitter.

339
00:26:52,634 --> 00:26:54,730
Speaker 0: They just shorten everything, the shorter the better.

340
00:26:54,790 --> 00:26:57,310
Speaker 1: Scott, I disagree that the medium forces the message.

341
00:26:57,330 --> 00:27:00,410
Speaker 0: It doesn't force, but it has optimized a particular message.

342
00:27:00,510 --> 00:27:06,549
Speaker 0: And even though you could put any sort of message in any sort of medium, It won't, no one will go for it.

343
00:27:06,650 --> 00:27:09,870
Speaker 1: No, see, Scott, I also disagree that it's even optimized for a particular message.

344
00:27:10,071 --> 00:27:17,665
Speaker 1: All it is, is that society itself, people generally are lazy And would rather not engage on a high level.

345
00:27:17,685 --> 00:27:26,808
Speaker 1: in most cases And even back then, I would wager that not that many people in the US Understood what was going on in Lincoln vs.

346
00:27:26,828 --> 00:27:27,130
Speaker 1: Douglas.

347
00:27:27,170 --> 00:27:32,910
Speaker 1: I don't know, do you How many people were all for the Civil War and the South?

348
00:27:34,110 --> 00:27:35,009
Speaker 1: They clearly couldn't win.

349
00:27:35,595 --> 00:27:37,389
Speaker 1: Any high level debate would have pointed that out.

350
00:27:37,450 --> 00:27:44,490
Speaker 0: Well, that was just because in those days, right, they didn't have the, you know, The amount of information that we have today.

351
00:27:44,954 --> 00:27:46,650
Speaker 0: Everyone knows everything about everything everywhere.

352
00:27:46,670 --> 00:27:53,810
Speaker 0: I don't want to get into Neil Postman because I disagree with a great majority But it's the same thing that you're saying now, talking about the high level and the low level.

353
00:27:54,092 --> 00:27:56,149
Speaker 1: But I don't think it has anything to do with the medium, it's just people.

354
00:27:57,297 --> 00:27:57,948
Speaker 0: I don't know about that.

355
00:27:58,252 --> 00:28:03,350
Speaker 1: People generally avoid the high level discourse Partly because high level discourse takes a long time.

356
00:28:03,531 --> 00:28:06,210
Speaker 0: The high level discourse was commonplace in the olden days.

357
00:28:06,390 --> 00:28:06,870
Speaker 1: Yeah, it was.

358
00:28:06,952 --> 00:28:07,687
Speaker 1: I don't think it was.

359
00:28:07,830 --> 00:28:21,210
Speaker 1: You look at the, you know, the "reading level" of, you know, as you go back in time Scott, I point out as you go back in time, you're looking at the reading level of a smaller And smaller subset of the literate population.

360
00:28:22,257 --> 00:28:22,809
Speaker 0: Is this true?

361
00:28:23,210 --> 00:28:25,990
Speaker 1: I mean, the further back you go, who is literate?

362
00:28:26,271 --> 00:28:31,529
Speaker 1: The wealthy aristocrats and the philosophers and the people like that The people with schooling, learning and knowledge.

363
00:28:32,633 --> 00:28:35,070
Speaker 1: You had a smaller, more literate set.

364
00:28:35,293 --> 00:28:36,070
Speaker 0: So what are you saying?

365
00:28:36,190 --> 00:28:37,690
Speaker 0: So you're saying that now, right?

366
00:28:38,411 --> 00:28:44,770
Speaker 0: You're basically saying it's no different now That even though the people are literate, they're still not as smart as the, you know.

367
00:28:44,870 --> 00:28:54,110
Speaker 0: So really, still, even though everyone can read the words technically Only the people who are actually educated and wealthy and whatnot are really literate.

368
00:28:54,233 --> 00:28:54,909
Speaker 0: Is that what you're saying?

369
00:28:55,415 --> 00:28:56,750
Speaker 1: I think there's a strong correlation.

370
00:28:57,576 --> 00:29:06,710
Speaker 1: Sadly Alrighty then You said that like you're going to use this in some sort of argument against me No, I just wanted to put that on the table.

371
00:29:07,002 --> 00:29:08,110
Speaker 0: Alright It's out there.

372
00:29:08,150 --> 00:29:08,190
Speaker 0: I

373
00:29:08,230 --> 00:29:28,489
Speaker 1: think it's more that the general, as we democratize access to distribution of ideas And consumption of ideas, that does not necessarily precipitate the full dissemination of those ideas Because every person, regardless of their abilities, is exposed to them But not everyone can get the same thing out of them.

374
00:29:29,432 --> 00:29:39,350
Speaker 1: I mean, an idea is a meme And a meme can spread And some people can take on and then retransmit larger memes than other people.

375
00:29:40,590 --> 00:29:48,509
Speaker 1: So maybe the same percentage of people throughout history just have the innate ability to handle large memes And some people can be trained to.

376
00:29:48,876 --> 00:29:50,444
Speaker 1: Some people can't Some people refuse to.

377
00:29:50,991 --> 00:30:00,610
Speaker 1: But the result is that if you add more people to the water where the meme virus is spreading That does not necessarily mean that that meme virus will spread to all those people.

378
00:30:01,953 --> 00:30:04,070
Speaker 1: And I think the problem with this pop philosophy idea?

379
00:30:04,435 --> 00:30:05,450
Speaker 1: Well not the problem with the idea.

380
00:30:05,551 --> 00:30:12,050
Speaker 1: The problem it portends Is that memes spread if they're compact little concise things.

381
00:30:12,613 --> 00:30:16,110
Speaker 1: That if more people can spread a meme, it's going to spread more rapidly.

382
00:30:16,696 --> 00:30:21,250
Speaker 0: Because if only one percent It's like, you know, if you go to baseball at any stadium, right?

383
00:30:21,330 --> 00:30:26,744
Speaker 0: The more people that you want to get to do something in unison The simpler it has to be, right?

384
00:30:27,470 --> 00:30:32,329
Speaker 0: If you want to get, say, 10 people to like sing a song You could do that.

385
00:30:32,410 --> 00:30:34,749
Speaker 0: You just give everyone the lyrics on a piece of paper, right?

386
00:30:35,230 --> 00:30:37,310
Speaker 0: But try to get a hundred people to sing a song.

387
00:30:37,330 --> 00:30:42,870
Speaker 1: There's one answer, soccer hooligans Ten thousand, they can be so drunk they can't see five feet in front of their faces.

388
00:30:43,194 --> 00:30:48,630
Speaker 0: They can sing that song Right, but I mean, you know, they can't sing, you know, say a complicated song.

389
00:30:48,750 --> 00:30:55,890
Speaker 0: And they have to, you know, getting them, they have to know that song in advance And they've trained and learned the song over a great period of time.

390
00:30:55,950 --> 00:31:08,690
Speaker 0: If you just get a bunch of brand new people, you can do the wave And maybe you can clap in unison And maybe you can say like one or two words in unison But it's very difficult to get them to say, you know, do a complex thing.

391
00:31:08,750 --> 00:31:20,330
Speaker 1: But if you let me finish my point and what I think was really happening I think what Postman was observing But now we can see it with hindsight more clearly Is that it's not the particular merits of any kind of media.

392
00:31:20,712 --> 00:31:24,789
Speaker 1: It's simply that more media is more bi-directional and unidirectional.

393
00:31:25,453 --> 00:31:33,790
Speaker 1: So now we kind of have a wider pool Where smaller pieces of information spread more rapidly and have more enduring power.

394
00:31:34,554 --> 00:31:47,830
Speaker 1: So simply because we have more low-power transmitters Instead of a few high-power transmitters Smaller, more low-powered pieces of information Are the ones that are more fit to survive in the broad population.

395
00:31:48,952 --> 00:32:00,490
Speaker 1: That's exactly what, you're not disagreeing with him at all No, because he's saying that the way the kind of media does it And he made all these arguments that television forces The transmission of the media is part of its kind.

396
00:32:00,594 --> 00:32:02,390
Speaker 0: No, but Is an aspect of its kind.

397
00:32:03,073 --> 00:32:11,989
Speaker 1: I'm making the much broader argument that it's the fact that it's transmitted at all Is the only factor Doesn't matter how it's transmitted All that matters is that it is transmitted in any form.

398
00:32:12,492 --> 00:32:19,010
Speaker 1: And because more information can be transmitted more quickly in general We self-select toward the smaller pieces of information.

399
00:32:20,073 --> 00:32:23,010
Speaker 0: That is one aspect of it, yes That's my whole argument.

400
00:32:23,890 --> 00:32:26,669
Speaker 0: Alright, so I think you're on a lower level then?

401
00:32:27,631 --> 00:32:34,370
Speaker 1: No, I really, I just, I don't, I think his book was kind of crap And I think he made a lot of good points But I think he made a lot of spurious conclusions.

402
00:32:34,695 --> 00:32:35,730
Speaker 0: Yeah, anyway.

403
00:32:36,733 --> 00:32:41,570
Speaker 0: So yeah, it's just, you know What is there, can we come up with any sort of solution for this problem, right?

404
00:32:41,610 --> 00:32:44,950
Speaker 0: We want more PR, if we have this great thing, right?

405
00:32:45,010 --> 00:32:47,310
Speaker 0: That we have basically the whole world almost, right?

406
00:32:47,330 --> 00:32:55,470
Speaker 0: Is able to communicate with each other Without, you know, any significant hampering of time Or amount of information.

407
00:32:55,591 --> 00:32:59,310
Speaker 0: Or, you know, we can basically get on webcams with everyone all at once, right?

408
00:33:00,171 --> 00:33:06,670
Speaker 0: So we want people to sort of get new things Instead of just, you know, continuing to mull down low.

409
00:33:06,750 --> 00:33:17,050
Speaker 0: We want, you know, the highest level, most useful, you know, conversations to be happening As opposed to people just, you know, basically bullshitting Thinking that they're on the high level.

410
00:33:17,131 --> 00:33:18,930
Speaker 0: It might be impossible.

411
00:33:19,275 --> 00:33:22,550
Speaker 0: It might be impossible Alright, so let's assume it's not impossible.

412
00:33:23,332 --> 00:33:27,110
Speaker 0: In what scenario would we, any chance of it occurring?

413
00:33:27,270 --> 00:33:29,870
Speaker 1: I think what it shows more is some of the, like.

414
00:33:30,071 --> 00:33:37,150
Speaker 1: I think it is either the root or the symptom of the root Of a lot of the problems we have in the world today.

415
00:33:37,150 --> 00:33:39,470
Speaker 1: I mean, look at the inherent problems of democracy.

416
00:33:39,610 --> 00:33:45,630
Speaker 1: Look at the national dialogue about any popular topic in the United States.

417
00:33:45,711 --> 00:33:47,890
Speaker 0: That is definitely, absolutely true.

418
00:33:48,931 --> 00:33:52,630
Speaker 1: Almost all the discussion of any topic boils down to.

419
00:33:53,054 --> 00:33:54,410
Speaker 1: I wouldn't even call them talking points.

420
00:33:54,450 --> 00:33:56,410
Speaker 1: They're just, like, the abortion debate.

421
00:33:57,151 --> 00:34:03,409
Speaker 1: A lot of the people who are anti-abortion don't have any argument or anything All they have.

422
00:34:03,651 --> 00:34:14,969
Speaker 0: A lot of people who are pro-choice don't really have much of an argument either You know, they just boil everything down to the, you know, these incredibly simple things When the issue is ludicrously complex.

423
00:34:15,110 --> 00:34:24,550
Speaker 1: Well, Scott, the thing is, with abortion in particular If you move to the higher levels, I think it's clear that one side has a Well, I'm not saying, I also agree that the pro-choice side is correct.

424
00:34:24,670 --> 00:34:40,965
Speaker 0: I'm saying is that many of the people who argue, even though they're on the correct side Their argument is not on the highest level And that, you know, I could actually, you know, in some cases I could argue for the wrong side using a higher level argument And beat some of the people on the right side who have a

425
00:34:41,005 --> 00:34:41,690
Speaker 1: lower level argument.

426
00:34:41,730 --> 00:35:02,970
Speaker 1: No, you can't beat them because someone who makes a Remember, someone who makes, in one particular field We're not arguing about the intelligence of our human Just about the capability to transmit and relay a single meme In the abortion debate, someone who can only transmit these small, pro- or anti-meme Cannot be affected by the larger meme in any capacity.

427
00:35:03,712 --> 00:35:05,650
Speaker 1: It cannot get to them unless you boil it down.

428
00:35:05,731 --> 00:35:10,930
Speaker 1: And it is possible to take a large, complex meme Make it compact and still make it true.

429
00:35:11,112 --> 00:35:21,610
Speaker 1: If you make a good argument at the high level And you boil the argument down to a very small, compact thing The end argument is still absolutely true And your little, compact thing is right.

430
00:35:22,092 --> 00:35:26,570
Speaker 1: The problem is, it's very powerful, but it loses kind of the redundancy.

431
00:35:26,791 --> 00:35:29,810
Speaker 1: It loses, like, its own self-referential metadata.

432
00:35:30,232 --> 00:35:33,550
Speaker 1: So it's really easy to corrupt it and use it for the wrong purposes.

433
00:35:33,650 --> 00:35:45,710
Speaker 1: Like, I make a boiled-down argument that is pro-choice, completely And while pro-choice, I think, is the absolutely inarguably correct philosophical And practical argument on all high level.

434
00:35:46,532 --> 00:35:52,290
Speaker 1: I think most intelligent people agree on that point But I can make a boiled-down argument that is pro-choice.

435
00:35:52,896 --> 00:35:54,410
Speaker 1: That is equally valid.

436
00:35:54,872 --> 00:36:09,969
Speaker 1: However, because it's boiled down, it's so broad and tiny now Broad in scope, tiny in reference It is now a powerful philosophical hammer that people can use To back up very poor, unrelated ideas.

437
00:36:11,553 --> 00:36:16,270
Speaker 1: It's the whole thing of libertarianism on the high level Is a great and fantastic field of debate.

438
00:36:16,752 --> 00:36:24,248
Speaker 1: On the low level, it effectively turns into the rationale for I do what I want, I do what I want Right And that's bad.

439
00:36:24,532 --> 00:36:28,830
Speaker 0: Well, I mean, you know, this makes me think of people Like, say, Ben Stein or Michael Moore, right?

440
00:36:28,950 --> 00:36:34,110
Speaker 0: Two people who are basically completely wrong about a great, great many things To the majority of the time, right?

441
00:36:34,510 --> 00:36:45,690
Speaker 0: And what they tend to do is they go and they make, you know, they make, you know, movies Or they get on TV and they talk And they're slightly above, you know, the lowest level, right?

442
00:36:45,770 --> 00:36:49,670
Speaker 0: They get lots of information in there, you know, usually And they're smart people.

443
00:36:49,972 --> 00:36:52,310
Speaker 0: Yeah, they're very smart, intelligent, right?

444
00:36:52,330 --> 00:37:03,170
Speaker 0: They know what's going on And, you know, they get their information, you know Even if their opinions or stances might be wrong Most of their information is correct, you know Their, you know, their core facts.

445
00:37:03,332 --> 00:37:06,410
Speaker 1: I'll argue the technically correct Right, technically correct, right.

446
00:37:06,451 --> 00:37:06,986
Speaker 0: That's what I mean.

447
00:37:08,291 --> 00:37:12,510
Speaker 0: But, you know, what they'll do is then they'll then deliver it in some sort of package.

448
00:37:13,054 --> 00:37:26,429
Speaker 0: You know, that supports basically anything You could take, you know, you could, like, take all the facts out of, say, a Michael Moore movie And make a movie that argues against Michael Moore 100% With the same exact facts just by changing, you know, the other parts.

449
00:37:27,179 --> 00:37:39,190
Speaker 1: And it's like The libertarian thing is the best example Where regardless of the high level rationale The low level pop philosophy meme is the rationale to do whatever you want.

450
00:37:39,291 --> 00:37:40,669
Speaker 1: It's the Hitler rationale.

451
00:37:41,241 --> 00:37:50,689
Speaker 0: Exactly So, and unless people are engaging at the high level If they're down at the low level, they can't really, that's it It's done.

452
00:37:50,852 --> 00:37:52,530
Speaker 0: There's nothing, they can't contribute.

453
00:37:52,631 --> 00:37:57,470
Speaker 1: It's almost like these low level memes At least these philosophy memes that people use to back bad ideas.

454
00:37:58,135 --> 00:38:04,470
Speaker 1: It's like you have a responsible gun owner And, you know, philosophers are all effectively philosophical brain gun owners.

455
00:38:04,912 --> 00:38:08,250
Speaker 1: They all have, they're responsible owners of very powerful guns.

456
00:38:08,852 --> 00:38:13,150
Speaker 1: But if you boil down your philosophy to a meme and you're not extremely careful.

457
00:38:13,712 --> 00:38:23,627
Speaker 1: That's the equivalent of taking your gun and giving it to someone who is blind and stupid And showing them where the trigger is and saying have fun I'll come back in a week.

458
00:38:24,152 --> 00:38:27,310
Speaker 0: Yeah, because, you know, to give, you know, I don't know, I can't come up with any.

459
00:38:27,350 --> 00:38:28,589
Speaker 1: But now look at this, look at what I just did.

460
00:38:28,892 --> 00:38:36,090
Speaker 1: If I take this argument and I boil what I just said down into a meme You could trivially use that to rationalize ivory tower bullshit.

461
00:38:36,520 --> 00:38:38,290
Speaker 1: That's true Keeping knowledge from the masses.

462
00:38:38,290 --> 00:38:46,950
Speaker 1: And it would be trivial to rationalize that with the pop philosophy thing I just spouted Because the high order argument is so long that it won't spread in the memeosphere.

463
00:38:47,373 --> 00:38:49,649
Speaker 1: I'll never use that word again in my life and I apologize for saying it.

464
00:38:51,911 --> 00:38:58,790
Speaker 0: Well, yeah, but yeah, a high level, a high level idea has lots of ifs and ands and buts And this and that, right?

465
00:38:58,870 --> 00:39:02,750
Speaker 0: I mean, look at something like, you know, discussion about like, you know, swine flu.

466
00:39:02,811 --> 00:39:04,430
Speaker 0: It's like, well, it could spread like this.

467
00:39:04,530 --> 00:39:10,930
Speaker 0: And, you know, the actual reality is incredibly complex Ludicrously complex, you know.

468
00:39:11,051 --> 00:39:22,090
Speaker 0: Actually, random aside People boil, you know, that complexity down into, you know, these simple ideas, you know Random aside And they use those simple ideas to make their decisions.

469
00:39:22,291 --> 00:39:30,910
Speaker 0: And really, no, it's very complex And your decision making, you know, is not taking the, you know, your, your is basically effective.

470
00:39:31,131 --> 00:39:34,590
Speaker 0: You might as well be acting randomly the way you're making decisions, you know.

471
00:39:35,012 --> 00:39:37,590
Speaker 1: So I guess there are two immediate questions.

472
00:39:37,810 --> 00:39:53,329
Speaker 1: One, is there a way to foster a society where the either the intellectually large memes can spread Or where the intellectually large memes have more weight regardless of how many people wield them?

473
00:39:54,253 --> 00:40:05,329
Speaker 1: Or where people, where these memes can be boiled down in such a way That people who aren't going to engage on the top level can still engage on a meaningful level?

474
00:40:07,116 --> 00:40:09,990
Speaker 1: Uh, people, uh Can any of these be accomplished?

475
00:40:10,610 --> 00:40:14,124
Speaker 0: Well, I don't know if you can engage at a meaningful level in an area.

476
00:40:14,144 --> 00:40:19,430
Speaker 0: if you are, you know If you haven't just learned all, you know, the things you need to know to get up there.

477
00:40:19,470 --> 00:40:24,269
Speaker 0: It's like if you haven't climbed the stair 10, you're not gonna, you're not involved in the 11th stair building.

478
00:40:24,691 --> 00:40:33,053
Speaker 1: I have run into many people in my life who I know are perfectly capable Of engaging me on a higher level than they do And they just don't Mmm.

479
00:40:33,671 --> 00:40:38,068
Speaker 1: There is definitely a difference between the people, between the will nots and the can nots.

480
00:40:39,095 --> 00:40:42,949
Speaker 1: Mmm, that's very interesting And I don't know what to do about that.

481
00:40:43,333 --> 00:40:47,089
Speaker 1: And then that raises the whole other ethical question of Say there are.

482
00:40:47,171 --> 00:40:48,669
Speaker 0: That's just the issue of, you know, apathy.

483
00:40:48,891 --> 00:40:53,767
Speaker 1: Yes, but even say there are will nots and there are can nots What do you do with the will nots?

484
00:40:54,111 --> 00:40:55,288
Speaker 1: What do you do with the can nots?

485
00:40:55,550 --> 00:40:59,810
Speaker 1: And how do you not make a meme for dealing with them that turns into genocide or eugenics?

486
00:41:03,072 --> 00:41:08,430
Speaker 0: Uh, you know, whenever we do anything like this, you know, it's always like, well, how can we fix the world?

487
00:41:08,530 --> 00:41:10,989
Speaker 0: Well, you know, you do what you can and that's it.

488
00:41:11,070 --> 00:41:11,431
Speaker 0: But it really

489
00:41:11,471 --> 00:41:45,090
Speaker 1: bothers me because it seems like every philosophy, no matter what it is No matter what it really says is boiled down generally and widely into I don't want to say a soundbite, but a meme so small in reference and so wide in scope That any possible course of action that any person could take in the entire world Whether it's good, whether it's bad, genocide, killing babies, whatever Is eminently justifiable if you use only low level memes.

490
00:41:45,170 --> 00:41:47,530
Speaker 0: Yeah, go and look at like, you know, people always.

491
00:41:48,173 --> 00:41:51,190
Speaker 0: One thing that people really like, I really like are like quotes, right?

492
00:41:51,350 --> 00:41:52,550
Speaker 0: You know, aphorisms perhaps?

493
00:41:52,830 --> 00:41:58,570
Speaker 0: Yes, you know, you find some famous person, a Thomas Jefferson or a Benjamin Franklin.

494
00:41:58,751 --> 00:42:02,550
Speaker 1: Those who would give up liberty for security deserve neither.

495
00:42:02,972 --> 00:42:09,410
Speaker 0: You find some quote and you say the quote and then someone else goes, Oh, that is very wise and makes perfect sense.

496
00:42:09,450 --> 00:42:12,910
Speaker 0: And then you say some other quote, Oh, that is very wise and makes perfect sense, right?

497
00:42:13,350 --> 00:42:18,649
Speaker 0: But it's great when you say like two quotes that are both so wise, like, you know, But they completely disagree.

498
00:42:19,571 --> 00:42:23,490
Speaker 1: Or, I mean, look, like those who give up liberty for security deserve neither.

499
00:42:23,931 --> 00:42:29,930
Speaker 1: You could use that to rationalize the taking away of security from people.

500
00:42:30,153 --> 00:42:34,570
Speaker 1: If you really wanted to Yeah You could probably argue for taking away liberty.

501
00:42:36,693 --> 00:42:43,250
Speaker 0: You know, I mean, you could look, Oh, the tree of liberty must, you know, From time to time be refreshed with the blood of patriots.

502
00:42:43,411 --> 00:42:45,450
Speaker 0: Well, you could take that to mean Vampires.

503
00:42:45,831 --> 00:42:50,770
Speaker 0: Yeah, you know, once in a while, you've just got to kill your own people.

504
00:42:51,553 --> 00:42:52,649
Speaker 0: That's just how it's got to be.

505
00:42:52,872 --> 00:43:01,250
Speaker 1: Maybe that is how it's got to be You know, maybe I think the problem is the academics always look for the ivory tower.

506
00:43:01,310 --> 00:43:03,166
Speaker 1: I think we need the crimson tower.

507
00:43:03,951 --> 00:43:08,630
Speaker 0: The crimson tower must be frequently refreshed with the blood of the academics.

508
00:43:09,550 --> 00:43:10,630
Speaker 0: Come here, Roux style.

509
00:43:10,873 --> 00:43:12,288
Speaker 1: No, I think the blood should flow upward.

510
00:43:13,811 --> 00:43:16,510
Speaker 0: So it's like Ganesh, the milk just goes up.

511
00:43:16,650 --> 00:43:20,249
Speaker 1: But that means we got to beat Newton because he's going to fuck us on this one.

512
00:43:21,412 --> 00:43:23,299
Speaker 1: All right I think we've exhausted this topic.

513
00:43:25,110 --> 00:43:29,149
Speaker 1: I think I've said every low level piece of discourse I have.

514
00:43:32,475 --> 00:43:34,609
Speaker 1: And don't forget this weekend, Saturday, we'll be at the mocha.

515
00:43:34,814 --> 00:43:36,329
Speaker 1: You'll find us Saturday, not Sunday.

516
00:43:37,032 --> 00:43:40,190
Speaker 1: Very likely Saturday We will post somewhere if it ends up being Sunday.

517
00:43:46,713 --> 00:43:48,790
Speaker 1: This has been Geek Nights with Rim and Scott.

518
00:43:48,991 --> 00:43:51,430
Speaker 1: Special thanks to DJ Pretzel for the opening music.

519
00:43:52,173 --> 00:44:03,290
Speaker 0: Be sure to visit our website at www.frontroadcrew.com Where you'll find show notes, links, our awesome forum, a link to our Frapper map, and links to all the RSS feeds.

520
00:44:03,933 --> 00:44:09,569
Speaker 1: We say feeds plural because Geek Nights airs four nights a week covering four different brands of geekery.

521
00:44:10,135 --> 00:44:14,270
Speaker 1: Mondays are science and technology Tuesdays we have video games, board games, and RPGs.

522
00:44:14,814 --> 00:44:20,249
Speaker 1: Wednesdays are anime, manga, comic nights And Thursdays are the catch-alls for various rants and tomfoolery.

523
00:44:20,691 --> 00:44:28,729
Speaker 0: You can send us feedback by email to geeknights@frontroadcrew.com Or you can send audio feedback via Odeo.

524
00:44:29,112 --> 00:44:32,330
Speaker 0: Just click the link that says "send me an Odeo" on the right side of our website.

525
00:44:32,691 --> 00:44:37,790
Speaker 1: If you like what you hear, you can catch the last 100 episodes in iTunes or in your favorite podcatcher.

526
00:44:38,051 --> 00:44:41,150
Speaker 1: For the complete archives, visit the website which has everything.

527
00:44:41,733 --> 00:44:46,663
Speaker 0: Geek Nights is distributed under a creative commons attribution non-commercial share alike 2.5 license.

528
00:44:48,254 --> 00:44:54,290
Speaker 0: This means you can do whatever you want with it as long as you give us credit, don't make money, and share it in kind.

529
00:44:55,111 --> 00:45:02,130
Speaker 0: Geek Nights is recorded live with no studio and no audience But unlike those other late shows, it's actually recorded at night.

