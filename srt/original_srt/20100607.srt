1
00:00:08,780 --> 00:00:10,835
Speaker 0: It's Monday, June 7th.

2
00:00:11,156 --> 00:00:11,718
Speaker 0: I'm Scott.

3
00:00:12,039 --> 00:00:12,520
Speaker 0: I'm Rip.

4
00:00:12,581 --> 00:00:14,005
Speaker 0: And this is Geek Nights.

5
00:00:14,065 --> 00:00:17,593
Speaker 0: Tonight, what if Ghost in the Shell, oh shit.

6
00:00:21,900 --> 00:00:23,002
Speaker 0: Let's do this.

7
00:00:26,367 --> 00:00:28,370
Speaker 1: So I got a new computer at work, finally.

8
00:00:28,650 --> 00:00:29,615
Speaker 0: What was wrong with the computer?

9
00:00:29,635 --> 00:00:30,238
Speaker 0: You had to work before?

10
00:00:30,278 --> 00:00:31,765
Speaker 1: It was slow as balls.

11
00:00:31,785 --> 00:00:32,790
Speaker 1: Alright.

12
00:00:33,210 --> 00:00:39,022
Speaker 1: So I got a new one and I'm setting it all up, get my new email, and I didn't check my email for like a couple hours while I'm setting this thing up.

13
00:00:39,726 --> 00:00:43,911
Speaker 1: And all my coworkers around me are laughing at something, but I'm kind of busy, so I'm going to ask them what.

14
00:00:44,393 --> 00:00:45,797
Speaker 1: And I finally get my email back on.

15
00:00:45,918 --> 00:00:50,232
Speaker 1: Now, I'm going to not say who these emails came from.

16
00:00:50,874 --> 00:00:58,800
Speaker 1: Suffice it to say, there's some big names in the financial world, some very big individual names, and names of exchanges.

17
00:00:59,302 --> 00:01:00,426
Speaker 0: Famous people know?

18
00:01:00,586 --> 00:01:02,855
Speaker 0: Would these be someone normal people would know?

19
00:01:02,875 --> 00:01:09,281
Speaker 1: Normal people wouldn't know, but imagine if the head of a technology group of a stock exchange was involved.

20
00:01:09,983 --> 00:01:10,746
Speaker 1: I'm not going to say who.

21
00:01:11,068 --> 00:01:12,213
Speaker 1: I'm not going to say who, however.

22
00:01:12,233 --> 00:01:14,041
Speaker 0: There aren't that many stock exchanges.

23
00:01:14,524 --> 00:01:16,572
Speaker 1: Can you name the stock exchanges in America?

24
00:01:16,773 --> 00:01:19,882
Speaker 0: NASDAQ, NICE, Amex.

25
00:01:19,962 --> 00:01:20,363
Speaker 0: That's it.

26
00:01:20,524 --> 00:01:21,547
Speaker 1: No, there's more.

27
00:01:21,567 --> 00:01:22,490
Speaker 0: What about bats?

28
00:01:22,510 --> 00:01:23,656
Speaker 0: Do you want me to do foreign ones?

29
00:01:23,777 --> 00:01:24,682
Speaker 1: No, what about bats?

30
00:01:24,802 --> 00:01:25,687
Speaker 0: I don't know what bats is.

31
00:01:25,707 --> 00:01:25,909
Speaker 0: Yeah.

32
00:01:26,070 --> 00:01:26,411
Speaker 1: And you know what?

33
00:01:26,431 --> 00:01:29,765
Speaker 1: NASDAQ was not a stock exchange until surprisingly recently.

34
00:01:29,785 --> 00:01:32,795
Speaker 0: Yeah, well, there's also, I can do foreign ones.

35
00:01:33,356 --> 00:01:38,008
Speaker 1: Anyway, so basically what happened here was I get an email.

36
00:01:38,550 --> 00:01:56,978
Speaker 1: In fact, as far as I can tell, pretty much every tech in the financial world got the same email and it was someone inside of an unnamed company who obviously had opened a ticket with some other unnamed company and they were at, they were having questions about this, uh, an upcoming technological issue.

37
00:01:57,199 --> 00:02:08,192
Speaker 1: I'm trying to be vague because I don't want to get in trouble for talking about the specifics, but basically there's an upcoming change and there's this little email chain and somebody responds and he's like, yeah, that doesn't really answer my question.

38
00:02:08,213 --> 00:02:09,939
Speaker 1: I think I need more information, blah, blah, blah.

39
00:02:11,084 --> 00:02:15,518
Speaker 1: He must've done the famous reply to all instead of simply reply.

40
00:02:16,440 --> 00:02:23,285
Speaker 1: And the party involved that had originally addressed the issue did not have any protection on their internal mailing lists.

41
00:02:23,646 --> 00:02:30,753
Speaker 1: So his reply to all, as far as I can tell, went to every high level technology worker in the financial sector in the United States.

42
00:02:31,796 --> 00:02:47,202
Speaker 1: This is a big fucking email chain and you're, you're a tech, you've been in that situation where some jackass replies to all and then someone else starts replying to all and then the third person's like, I don't want to get these emails anymore, but they reply to all and then someone else does and it just goes on forever.

43
00:02:48,627 --> 00:02:51,362
Speaker 1: So this was actually the two moments of hilarious.

44
00:02:51,382 --> 00:02:54,094
Speaker 0: If you don't want to get them anymore, click the market spam button.

45
00:02:54,254 --> 00:03:10,850
Speaker 1: So after the filter expanding the two hilarious moments, one, the director of a big department at a very large company respond, replied to all and basically said, I'm a very busy person and I really shouldn't be receiving these emails.

46
00:03:10,950 --> 00:03:12,797
Speaker 1: You're clogging up my inbox, so please stop it.

47
00:03:13,480 --> 00:03:20,442
Speaker 1: And immediately, immediately some other guy from some other company responds and is like, yes, I concur.

48
00:03:21,786 --> 00:03:29,469
Speaker 1: But what took the cake was that the guy who originally did this obviously realized he had done fucked up.

49
00:03:30,311 --> 00:03:31,115
Speaker 0: Oh, he realized it.

50
00:03:31,175 --> 00:03:31,376
Speaker 0: Okay.

51
00:03:31,396 --> 00:03:34,130
Speaker 1: So he replied to all and he's like, I'm sorry, I made a mistake.

52
00:03:36,591 --> 00:03:40,202
Speaker 1: Now, nevermind the fact that they're all continuing to reply to all.

53
00:03:40,363 --> 00:03:40,724
Speaker 1: Yes.

54
00:03:41,466 --> 00:03:46,125
Speaker 1: And someone else in his company replied to all to say, yeah, also it was a mistake.

55
00:03:46,165 --> 00:03:48,274
Speaker 1: Please just no one respond and this will just be done.

56
00:03:48,936 --> 00:03:55,900
Speaker 1: But then the big up who had replied to everyone happened to be the same company as the person who fucked up in the first place.

57
00:03:57,285 --> 00:04:02,873
Speaker 1: And there's all spiraled and spiraled and I don't even know where it left because I eventually left work and it was still going on.

58
00:04:02,933 --> 00:04:07,050
Speaker 1: But my favorite moment was not this executive lady.

59
00:04:07,230 --> 00:04:11,348
Speaker 0: You should have just replied to all like you had all the email addresses, right?

60
00:04:11,368 --> 00:04:12,894
Speaker 1: I had the lists, right?

61
00:04:12,914 --> 00:04:21,769
Speaker 0: So take the list of email addresses, make a new email from like a Gmail account that goes to all those email addresses and put up goat C or something like that.

62
00:04:21,970 --> 00:04:22,392
Speaker 1: Yeah, yeah, yeah.

63
00:04:22,412 --> 00:04:24,866
Speaker 1: The industry I work in, the FBI would be knocking on my door.

64
00:04:24,886 --> 00:04:26,674
Speaker 0: I wouldn't do it for goat C. Goats.

65
00:04:26,695 --> 00:04:27,538
Speaker 0: He's not against the law.

66
00:04:28,282 --> 00:04:30,671
Speaker 0: It is not illegal to email someone goat C.

67
00:04:31,394 --> 00:04:36,910
Speaker 1: I could probably get in trouble nonetheless because because the mailing list, it's complicated.

68
00:04:37,031 --> 00:04:39,043
Speaker 1: The way the mailing list works, I would get in trouble.

69
00:04:39,063 --> 00:04:42,177
Speaker 0: All right, well how, what if you emailed a little cat?

70
00:04:42,558 --> 00:04:46,090
Speaker 0: So all that aside, that's not, it's definitely not illegal.

71
00:04:46,410 --> 00:04:49,852
Speaker 1: My favorite, but what I just, you know, we're at work, we're like, what's the best thing we could do?

72
00:04:49,892 --> 00:04:55,378
Speaker 1: And I suggested, why don't we send an outlook meeting notice to all meet up in Bryant Park and have lunch together.

73
00:04:57,044 --> 00:04:58,932
Speaker 0: Is everyone in New York or were there people on the list?

74
00:04:58,952 --> 00:05:00,077
Speaker 0: Not in the whole world.

75
00:05:00,138 --> 00:05:06,465
Speaker 1: As far as I can tell, one of my other coworkers, like I should just put my resume, see what happens.

76
00:05:06,806 --> 00:05:09,217
Speaker 1: Other guy was like, I should just put a personal ad up right now.

77
00:05:09,237 --> 00:05:11,448
Speaker 0: Well, did that have any ladies on the list?

78
00:05:11,469 --> 00:05:12,352
Speaker 0: I don't know.

79
00:05:12,432 --> 00:05:13,114
Speaker 1: I don't even know.

80
00:05:13,495 --> 00:05:25,517
Speaker 1: But my favorite moment was some guy from some like third yet unmentioned company who responded well after things was going on, who was like, yeah, you know, everyone says they don't want to receive these emails.

81
00:05:25,557 --> 00:05:31,105
Speaker 1: But frankly, I also have this question that was raised originally and I'd really like to know the answers.

82
00:05:31,125 --> 00:05:32,491
Speaker 1: So could we keep everyone in the loop?

83
00:05:36,460 --> 00:05:46,230
Speaker 1: Now I got to tell all of you people who have ever been in one of these situations, if you ever replied all in a situation where that has already started, you're a jackass.

84
00:05:46,470 --> 00:05:47,715
Speaker 1: You do not deserve email.

85
00:05:48,056 --> 00:05:49,923
Speaker 1: You're only no, no, no.

86
00:05:50,204 --> 00:05:54,525
Speaker 0: I think that unless you prank them all, there's nothing wrong with replying to all in that situation.

87
00:05:54,545 --> 00:05:55,390
Speaker 0: In fact, it's awesome.

88
00:05:55,571 --> 00:06:00,330
Speaker 0: And if you are someone who can't just make a mail filter, you shouldn't be using it differently.

89
00:06:00,692 --> 00:06:08,063
Speaker 1: There are two kinds of people who reply to all people who have no business using email and people like us who are trolling everybody else.

90
00:06:08,525 --> 00:06:16,523
Speaker 0: You, if you don't know how to make an email filter or mark something as spam, then this is pretty much inevitably going to happen to you and you need to be able to deal with it.

91
00:06:16,563 --> 00:06:17,750
Speaker 0: Otherwise you shouldn't be using email.

92
00:06:18,010 --> 00:06:27,990
Speaker 1: The real trouble is that this only happens because pretty much everyone responding to this list clearly has some sort of failing of understanding and technology in some manner.

93
00:06:28,311 --> 00:06:31,920
Speaker 1: A lot of the emails were surprisingly poorly formed.

94
00:06:33,264 --> 00:06:33,905
Speaker 1: I'm sorry.

95
00:06:34,286 --> 00:06:36,208
Speaker 1: You are confused and such like that.

96
00:06:36,350 --> 00:06:39,244
Speaker 0: You know, for example, I bet you a lot of money, right?

97
00:06:39,264 --> 00:06:43,182
Speaker 0: Like Steve Jobs, everyone knows his email address is like sjobs@apple.com, right?

98
00:06:43,544 --> 00:06:45,292
Speaker 0: I bet he doesn't have another email address.

99
00:06:45,353 --> 00:06:46,359
Speaker 0: I bet that's the only one.

100
00:06:46,379 --> 00:06:47,747
Speaker 0: And you know what he probably does?

101
00:06:47,788 --> 00:06:54,090
Speaker 0: He probably has a filter that says if the sender is from at apple.com, meaning it's an employee, right?

102
00:06:54,672 --> 00:06:55,614
Speaker 0: Or then delete it.

103
00:06:55,634 --> 00:07:02,193
Speaker 0: Or if it's from someone on this contact list, you know, wife, kids, et cetera, right?

104
00:07:02,555 --> 00:07:04,401
Speaker 0: Then bring it in the invite, right?

105
00:07:04,863 --> 00:07:16,486
Speaker 0: Otherwise put it in the separate folder and his iPhone probably only goes to when someone on his contact list emails him, you know, which probably includes like other CEOs and other important people, right?

106
00:07:17,249 --> 00:07:21,030
Speaker 0: And he doesn't need to have a separate email address or keep any secrets or anything like that.

107
00:07:21,070 --> 00:07:27,462
Speaker 0: And then when he wants to, when he's bored, he can go and he can click, you know, intentionally on the customer email folder, which is probably flooded.

108
00:07:27,924 --> 00:07:32,685
Speaker 0: and you know, just look at the subject lines and just see if there's anything fun in there.

109
00:07:32,725 --> 00:07:33,870
Speaker 0: You know, it's like an RSS feed.

110
00:07:33,930 --> 00:07:35,058
Speaker 0: It's like, Oh, that one looks cool.

111
00:07:35,118 --> 00:07:35,964
Speaker 0: Let's see what's in there.

112
00:07:36,085 --> 00:07:36,266
Speaker 0: Okay.

113
00:07:36,346 --> 00:07:40,511
Speaker 0: I'll reply to, you know, it's like a little pastime when he's bored instead of playing Farmville, you know?

114
00:07:40,692 --> 00:07:47,421
Speaker 1: I mean like all the emails you guys send us to geek nights, they go into my email account, I can read them, but they don't make my phone buzz.

115
00:07:47,681 --> 00:07:48,003
Speaker 0: Yeah.

116
00:07:48,725 --> 00:07:49,146
Speaker 0: All right.

117
00:07:49,227 --> 00:07:53,030
Speaker 0: So that's, you know, if you don't know how to do email like that, you shouldn't be using email.

118
00:07:53,270 --> 00:07:59,321
Speaker 1: It's the same situation, the same, you know, people use Gmail basically stopped having to deal with spam because the spam filters are so good.

119
00:07:59,341 --> 00:08:02,416
Speaker 0: I would reply to all with a tutorial on making outlook filters.

120
00:08:02,436 --> 00:08:08,798
Speaker 0: I mean, outlook has it right there in the left hand sidebar, rules, rules, and actions.

121
00:08:08,818 --> 00:08:10,344
Speaker 0: It's a top level item there.

122
00:08:10,424 --> 00:08:13,616
Speaker 1: I know for a fact that a good number of the people on that list use Lotus Notes.

123
00:08:14,720 --> 00:08:15,141
Speaker 0: Oh shit.

124
00:08:15,222 --> 00:08:19,537
Speaker 0: That's a much more powerful engine.

125
00:08:19,597 --> 00:08:21,544
Speaker 0: Lotus Notes can do it even better than Outlook.

126
00:08:21,564 --> 00:08:25,505
Speaker 1: That's one thing I can say about Lotus Notes because I'm one of those people who use Lotus.

127
00:08:25,525 --> 00:08:26,270
Speaker 1: You know, I worked at IBM.

128
00:08:26,751 --> 00:08:28,698
Speaker 1: I use Lotus Notes for like three years.

129
00:08:29,761 --> 00:08:31,326
Speaker 1: It has a lot of pros.

130
00:08:31,647 --> 00:08:33,032
Speaker 1: Like Outlook is good at what it does.

131
00:08:33,433 --> 00:08:40,297
Speaker 1: Lotus is a lot more powerful in the end user's hands, but it also gives them much more rope with which to fuck themselves.

132
00:08:42,283 --> 00:08:49,443
Speaker 1: So anyway, I guess we can't get around talking about fricking Apple and their new renouncements about phones.

133
00:08:50,265 --> 00:08:51,207
Speaker 0: Yeah.

134
00:08:51,228 --> 00:08:51,308
Speaker 1: Okay.

135
00:08:51,328 --> 00:08:54,625
Speaker 1: I think we really need to go through like everything that was announced because...

136
00:08:54,645 --> 00:08:56,977
Speaker 0: No, why would you need to do... Anyone can go and look it up.

137
00:08:57,279 --> 00:08:57,400
Speaker 1: Yeah.

138
00:08:57,420 --> 00:09:03,045
Speaker 1: You guys don't come to us for the news because it's at least four hours after the news happens that we talk about it.

139
00:09:03,587 --> 00:09:03,688
Speaker 0: Yeah.

140
00:09:03,788 --> 00:09:04,049
Speaker 0: Okay.

141
00:09:04,069 --> 00:09:07,162
Speaker 0: So let me just, let me just, you know, remember the things.

142
00:09:07,243 --> 00:09:07,484
Speaker 0: Okay.

143
00:09:07,504 --> 00:09:08,287
Speaker 0: So first of all...

144
00:09:08,488 --> 00:09:08,930
Speaker 1: Farmville.

145
00:09:09,631 --> 00:09:10,053
Speaker 0: Oh shit.

146
00:09:10,113 --> 00:09:10,856
Speaker 0: It's the end of the world.

147
00:09:10,936 --> 00:09:11,901
Speaker 0: Basically...

148
00:09:12,001 --> 00:09:14,090
Speaker 1: I think Farmville is already on the downswing.

149
00:09:14,951 --> 00:09:15,674
Speaker 0: I don't know about that.

150
00:09:15,694 --> 00:09:18,386
Speaker 0: I don't know about that at all.

151
00:09:18,406 --> 00:09:19,470
Speaker 1: I just, I have this feeling.

152
00:09:19,590 --> 00:09:20,314
Speaker 1: It's hard to say.

153
00:09:20,434 --> 00:09:21,459
Speaker 0: It's higher than Bejeweled.

154
00:09:21,499 --> 00:09:22,946
Speaker 0: It's like, it's like shit.

155
00:09:23,268 --> 00:09:27,572
Speaker 0: And as soon as now that people can play it when they're walking around, right, they're going to play it even more.

156
00:09:28,054 --> 00:09:35,794
Speaker 0: And then there's going to be people who didn't play it before, but now because you had to like sit at a computer or Facebook or something, but now Farmville is going to be its own app.

157
00:09:35,814 --> 00:09:44,234
Speaker 0: So these people who just have iPhones or whatever, iPod touches can just play Farmville while sitting on the couch, laying back and they can just play it as they walk around town.

158
00:09:44,254 --> 00:09:51,764
Speaker 1: See now, that is an entirely, it's a fine point, but I'm going to make a highly generalized, possibly offensive counterpoint.

159
00:09:51,925 --> 00:09:52,286
Speaker 0: Alright.

160
00:09:52,366 --> 00:09:52,929
Speaker 1: You ready for this?

161
00:09:52,989 --> 00:09:59,453
Speaker 1: A little bit offensive possibly, but I, I'm pulling this out of my butt and I'm, this is just me speculating a hundred percent.

162
00:09:59,694 --> 00:10:00,818
Speaker 1: I did not mean to offend anyone.

163
00:10:00,838 --> 00:10:03,830
Speaker 1: I will put forth supposition the first.

164
00:10:04,450 --> 00:10:09,101
Speaker 1: The majority of people who play Farmville regularly are not terribly wealthy.

165
00:10:10,444 --> 00:10:17,470
Speaker 0: I don't mean, yeah, but I would say that on average you can afford three kids to go to college.

166
00:10:17,751 --> 00:10:24,199
Speaker 1: I would say on average, the people who are most susceptible to a game like Farmville are people who don't have a lot going on in their lives.

167
00:10:25,826 --> 00:10:36,978
Speaker 1: I mean, on average, like many people, you know, the kinds of people are like, they're sitting there and they just need a serotonin boost constantly and they're on Facebook and their friend sends them a poke and then they make the farm and all that BS.

168
00:10:37,982 --> 00:10:47,917
Speaker 1: I don't think there is that big of an overlap between current iPhone owners in the current economy and the people who are seriously actually addicted to Farmville.

169
00:10:48,861 --> 00:10:55,705
Speaker 0: Yes, but maybe the Farmville people will buy iPhones and maybe the iPhone people will start playing Farmville.

170
00:10:55,805 --> 00:11:03,473
Speaker 0: Now, my guess is that a large number of people who play Farmville and remember like the top selling app on iPhone was like fart machine.

171
00:11:03,493 --> 00:11:03,574
Speaker 0: Yeah.

172
00:11:03,594 --> 00:11:06,381
Speaker 1: So machine is universal.

173
00:11:07,584 --> 00:11:09,970
Speaker 0: No, it's the same as Farmville.

174
00:11:10,050 --> 00:11:10,814
Speaker 1: Here's the thing, though.

175
00:11:10,955 --> 00:11:12,121
Speaker 0: They might as well call it far.

176
00:11:12,824 --> 00:11:14,371
Speaker 1: So we're sitting in a restaurant, right?

177
00:11:14,391 --> 00:11:17,892
Speaker 1: And you got your iPhone out and someone has an iPhone out and they just use the fart machine.

178
00:11:17,992 --> 00:11:19,157
Speaker 1: You would laugh at least a little bit.

179
00:11:19,659 --> 00:11:21,106
Speaker 1: Tiny amusement happens.

180
00:11:21,367 --> 00:11:25,530
Speaker 0: No, it's more like they're sitting there, they're playing Farmville and they say, hey, I got some corn.

181
00:11:25,710 --> 00:11:26,112
Speaker 0: You want some?

182
00:11:26,172 --> 00:11:26,815
Speaker 0: You want some brick?

183
00:11:26,835 --> 00:11:27,738
Speaker 0: I'm like, oh, shit.

184
00:11:28,160 --> 00:11:36,391
Speaker 1: In fact, I would argue that fart machine is more of a game than Farmville because fart machine at least causes who farted to happen.

185
00:11:36,411 --> 00:11:38,278
Speaker 0: Now it's a really bad.

186
00:11:38,298 --> 00:11:39,823
Speaker 1: Well, it's like who Farmville?

187
00:11:40,304 --> 00:11:40,645
Speaker 1: I don't know.

188
00:11:40,665 --> 00:11:53,418
Speaker 1: I don't think this is going to be nearly as bad as a lot of people are making it out to be, because I don't think the people who are most affected by Farmville are the same people who in the next year or two will have an iPad.

189
00:11:54,141 --> 00:11:58,901
Speaker 0: The things I think are most notable are, A, they didn't announce anything but the iPhone 4.

190
00:11:58,901 --> 00:12:00,327
Speaker 0: They didn't announce a single thing.

191
00:12:00,468 --> 00:12:00,608
Speaker 0: Yeah.

192
00:12:00,669 --> 00:12:03,859
Speaker 1: What about all your speculation?

193
00:12:03,960 --> 00:12:05,063
Speaker 0: There was no Safari.

194
00:12:05,103 --> 00:12:06,909
Speaker 0: There was no standalone touch pad.

195
00:12:06,949 --> 00:12:07,692
Speaker 0: There was no nothing.

196
00:12:07,732 --> 00:12:10,522
Speaker 1: The iPad mini, the iPad nano.

197
00:12:10,602 --> 00:12:11,305
Speaker 1: It's called the touch.

198
00:12:11,666 --> 00:12:12,268
Speaker 0: Yeah, there was.

199
00:12:12,630 --> 00:12:15,163
Speaker 0: Well, no, but they didn't have a single thing except the phone.

200
00:12:15,224 --> 00:12:15,465
Speaker 0: Right.

201
00:12:15,525 --> 00:12:17,235
Speaker 0: And all the stuff that they had was in the phone.

202
00:12:17,255 --> 00:12:18,542
Speaker 1: They didn't announce Verizon.

203
00:12:18,824 --> 00:12:20,030
Speaker 0: No, well, that's not going to happen.

204
00:12:20,271 --> 00:12:21,375
Speaker 0: I can pretty much guarantee that.

205
00:12:21,816 --> 00:12:29,161
Speaker 0: But the the one thing that I think the overall thing here is that, well, look, every other phone still sucks.

206
00:12:29,341 --> 00:12:30,946
Speaker 0: Yes, Apple is evil.

207
00:12:31,367 --> 00:12:33,372
Speaker 0: It's closed, whatever.

208
00:12:33,473 --> 00:12:34,516
Speaker 0: It's, you know, this is bad.

209
00:12:34,557 --> 00:12:34,978
Speaker 0: This is bad.

210
00:12:34,998 --> 00:12:35,761
Speaker 0: This is bad.

211
00:12:36,544 --> 00:12:40,799
Speaker 0: But every other phone sucks a dick compared to the iPhone.

212
00:12:40,819 --> 00:12:43,068
Speaker 1: It depends on what you're looking to do.

213
00:12:43,108 --> 00:12:47,427
Speaker 1: I mean, I still I like the form factor of the somewhat smaller smartphones compared to the iPhone.

214
00:12:47,488 --> 00:12:49,961
Speaker 0: The iPhone is smaller than all the other smartphones.

215
00:12:49,981 --> 00:12:50,826
Speaker 1: Not smaller than my pre.

216
00:12:50,887 --> 00:12:53,118
Speaker 0: Have you looked at the size comparisons?

217
00:12:53,158 --> 00:12:54,263
Speaker 1: I'm not talking about thickness.

218
00:12:54,283 --> 00:12:55,550
Speaker 1: I'm talking about height and width.

219
00:12:55,950 --> 00:12:59,807
Speaker 0: What do you need more or less height and width for like a tiny smartphone?

220
00:12:59,828 --> 00:13:02,283
Speaker 1: I carry my phone everywhere, but I carry it like in my pocket while I'm running.

221
00:13:02,323 --> 00:13:03,892
Speaker 0: Mine fits in my pocket while I'm running.

222
00:13:03,912 --> 00:13:04,897
Speaker 1: It's bigger than mine, though.

223
00:13:05,258 --> 00:13:07,608
Speaker 0: So, yeah, it's it's it's.

224
00:13:07,669 --> 00:13:09,576
Speaker 0: it doesn't matter how big it just matters.

225
00:13:09,596 --> 00:13:12,949
Speaker 0: You know what the whether that size affects anything.

226
00:13:12,969 --> 00:13:14,574
Speaker 0: You could have an iPhone in your pocket and run.

227
00:13:14,594 --> 00:13:15,335
Speaker 0: It would be no different.

228
00:13:15,395 --> 00:13:15,817
Speaker 0: It would not.

229
00:13:15,897 --> 00:13:24,584
Speaker 0: I you know, the size is not really that important either.

230
00:13:24,624 --> 00:13:24,845
Speaker 0: Really?

231
00:13:24,906 --> 00:13:25,528
Speaker 1: It's not so much.

232
00:13:25,548 --> 00:13:31,896
Speaker 1: I mean, while the iPhone, the new iPhone and even the older iPhones do a lot of things very well compared to other smartphones.

233
00:13:32,277 --> 00:13:38,930
Speaker 1: And while the lack of openness really right now only affects people like us who ravel about it and it doesn't really affect the average user.

234
00:13:40,031 --> 00:13:44,425
Speaker 1: Most of what they have is just polish, which matters a fuck ton.

235
00:13:44,826 --> 00:13:46,574
Speaker 0: That's, you know, that's the whole point.

236
00:13:46,634 --> 00:13:47,801
Speaker 0: It's like, oh, it's like me.

237
00:13:47,841 --> 00:13:49,611
Speaker 0: If I bought it, I'd look at it like, wow, right.

238
00:13:49,671 --> 00:13:50,998
Speaker 0: It's like, OK, so you look at, wow.

239
00:13:51,158 --> 00:13:51,339
Speaker 0: Right.

240
00:13:51,379 --> 00:13:55,979
Speaker 0: Compared to all the other MMOs that all the MMOs suck every single one.

241
00:13:56,000 --> 00:13:57,626
Speaker 0: They're all fundamentally flawed.

242
00:13:57,706 --> 00:13:57,947
Speaker 0: Right.

243
00:13:58,188 --> 00:13:58,811
Speaker 0: Every single one.

244
00:13:58,831 --> 00:14:00,094
Speaker 0: There is not one out there.

245
00:14:00,456 --> 00:14:01,559
Speaker 0: that is that is awesome.

246
00:14:01,739 --> 00:14:02,080
Speaker 0: OK.

247
00:14:02,923 --> 00:14:03,484
Speaker 0: And wow.

248
00:14:03,805 --> 00:14:04,588
Speaker 0: Has the polish.

249
00:14:05,169 --> 00:14:07,742
Speaker 1: So from a business perspective, I agree with you 100 percent.

250
00:14:08,063 --> 00:14:13,268
Speaker 0: So therefore, if you're going to play one, it's going to be wild because all the other ones are intolerable.

251
00:14:13,429 --> 00:14:14,553
Speaker 0: Like it's like they're all bad.

252
00:14:14,614 --> 00:14:15,537
Speaker 0: This one's intolerable.

253
00:14:15,577 --> 00:14:20,094
Speaker 0: So all smartphones, all phones, period, are fucked up and they suck.

254
00:14:20,174 --> 00:14:23,063
Speaker 1: You keep saying things are so bad.

255
00:14:23,605 --> 00:14:29,223
Speaker 1: What would the new iPhone buy me that I don't have on my pre iMovie?

256
00:14:29,685 --> 00:14:38,424
Speaker 0: I don't really use that anyway because you go to a convention, you could take a movie of a cosplayer, push a button, it'll be on YouTube and you could edit the video right on the phone right there.

257
00:14:38,464 --> 00:14:39,027
Speaker 0: That moment.

258
00:14:39,569 --> 00:14:39,750
Speaker 1: Yeah.

259
00:14:39,990 --> 00:14:42,238
Speaker 0: And nothing else even anything else.

260
00:14:42,358 --> 00:14:42,820
Speaker 0: But it's a it's 720p.

261
00:14:42,840 --> 00:14:43,763
Speaker 0: Yeah, that's 30 frames a second.

262
00:14:45,850 --> 00:14:46,914
Speaker 0: That's fucking awesome.

263
00:14:47,757 --> 00:14:48,700
Speaker 0: Right.

264
00:14:48,720 --> 00:14:50,847
Speaker 0: So you got you can't even do videos over there.

265
00:14:51,328 --> 00:14:52,271
Speaker 1: Not actually I can.

266
00:14:52,652 --> 00:14:53,154
Speaker 0: Can you do this?

267
00:14:53,214 --> 00:14:59,250
Speaker 0: It is anywhere remotely close to to what to iMovie at in HD videos.

268
00:14:59,811 --> 00:15:00,214
Speaker 0: No, no.

269
00:15:00,355 --> 00:15:03,272
Speaker 1: OK, I'm saying for me, it's not a game changer because that's not something I need yet.

270
00:15:03,855 --> 00:15:05,000
Speaker 0: OK, you said the same thing.

271
00:15:05,060 --> 00:15:07,350
Speaker 0: I go I don't you know, I never need a smartphone.

272
00:15:08,072 --> 00:15:12,052
Speaker 1: I always said I want a smartphone, but they're not at the right price point and functionality point yet.

273
00:15:12,293 --> 00:15:13,238
Speaker 0: And the day they were.

274
00:15:13,258 --> 00:15:13,982
Speaker 1: Yeah, I bought a smartphone.

275
00:15:14,002 --> 00:15:21,255
Speaker 0: If you had the iPhone with the with the HD, you know, immediate YouTube, you would be using it like crazy at every convention.

276
00:15:21,356 --> 00:15:22,502
Speaker 0: It is absolutely something.

277
00:15:22,542 --> 00:15:27,672
Speaker 1: Probably not, because I mean, I use Twitter a lot of conventions and I tweet and do stuff like that less and less and less because I'm not really in.

278
00:15:28,274 --> 00:15:34,179
Speaker 1: I've given up on the immediate cataloging of stuff I do and I'm much more interested in the post-hoc cataloging instead.

279
00:15:34,862 --> 00:15:35,927
Speaker 0: So what about the.

280
00:15:35,947 --> 00:15:37,273
Speaker 1: That's why I bought the Flip camera.

281
00:15:37,393 --> 00:15:39,785
Speaker 0: So why don't you just get no smartphone?

282
00:15:39,805 --> 00:15:45,993
Speaker 1: You'd save a lot of money because I use the smartphone for web browsing, Google Maps, and like I track my GPS, like my jogging, stuff like that.

283
00:15:46,013 --> 00:15:48,729
Speaker 0: But there are phones that have that that are not smartphones.

284
00:15:48,769 --> 00:15:51,000
Speaker 1: That's just a nice high resolution web browser.

285
00:15:51,020 --> 00:15:52,306
Speaker 0: You can just.

286
00:15:52,326 --> 00:15:54,719
Speaker 0: Well, I mean, you're not going to get better resolution than the iPhone.

287
00:15:54,739 --> 00:15:55,282
Speaker 0: That's for sure.

288
00:15:55,503 --> 00:15:57,533
Speaker 1: It's high enough to read slash out and stuff on the subway.

289
00:15:57,894 --> 00:15:59,058
Speaker 0: Why are you reading slash that?

290
00:15:59,178 --> 00:15:59,760
Speaker 0: Oh, God.

291
00:15:59,861 --> 00:15:59,921
Speaker 0: OK.

292
00:15:59,941 --> 00:16:02,891
Speaker 0: So did it's still good.

293
00:16:02,951 --> 00:16:03,835
Speaker 0: It's doing fine.

294
00:16:04,718 --> 00:16:08,997
Speaker 0: You always seem to say this like is good enough is, you know, whatever you have is good enough.

295
00:16:09,077 --> 00:16:13,680
Speaker 0: And then what's I mean, slash doesn't have like no ambition to get something that is better ever.

296
00:16:13,901 --> 00:16:14,926
Speaker 0: So how long did it take?

297
00:16:14,966 --> 00:16:15,670
Speaker 0: Wait, wait, wait.

298
00:16:15,730 --> 00:16:25,410
Speaker 0: This is the same rim who was like, oh, I don't need to upgrade my my shitty old Mozilla to Firefox until like Firefox two or three came out to stop crashing my shitty old computer.

299
00:16:25,630 --> 00:16:28,180
Speaker 0: No, no, it did not crash a shitty old computer.

300
00:16:28,201 --> 00:16:30,952
Speaker 0: You're just far reluctant to go for anything new.

301
00:16:30,972 --> 00:16:34,991
Speaker 0: And you'll always say that whatever you have is adequate until you actually get something new.

302
00:16:35,011 --> 00:16:37,804
Speaker 0: You're just returning to your old something new.

303
00:16:37,884 --> 00:16:41,018
Speaker 1: It's because something that I had before I stopped, you know, it was a killer app.

304
00:16:41,520 --> 00:16:42,645
Speaker 0: You break your D.S.

305
00:16:42,725 --> 00:16:43,810
Speaker 0: on purpose to get a D.S.

306
00:16:43,930 --> 00:16:46,227
Speaker 0: lie and then you'll and then you'll say that the D.S.

307
00:16:47,110 --> 00:16:49,461
Speaker 1: Guys, you seriously think I broke my D.S.

308
00:16:49,582 --> 00:16:50,064
Speaker 1: on purpose.

309
00:16:50,084 --> 00:16:51,732
Speaker 0: You break plenty of things on purpose.

310
00:16:52,234 --> 00:16:52,896
Speaker 0: Uh huh.

311
00:16:54,221 --> 00:16:55,265
Speaker 1: I remember when I broke my

312
00:16:55,285 --> 00:16:55,505
Speaker 1: D.S.,

313
00:16:55,526 --> 00:16:58,537
Speaker 1: we were doing the show and you were talking about something and I was ignoring you.

314
00:16:58,557 --> 00:17:04,882
Speaker 1: I was loving up my Pokemon and the screen just like I said that I set it down on the desk because you stopped talking.

315
00:17:04,942 --> 00:17:06,869
Speaker 1: I had to talk and the top screen shattered.

316
00:17:08,071 --> 00:17:08,315
Speaker 0: Uh huh.

317
00:17:09,326 --> 00:17:09,548
Speaker 0: Yeah.

318
00:17:11,231 --> 00:17:14,825
Speaker 0: The point is, is that nothing else is even remotely close to the eye.

319
00:17:14,885 --> 00:17:15,106
Speaker 0: I don't.

320
00:17:15,165 --> 00:17:16,170
Speaker 0: why can't anyone do it?

321
00:17:16,210 --> 00:17:20,670
Speaker 0: I mean, all of these improvements right right down the line are just completely obvious.

322
00:17:21,171 --> 00:17:21,452
Speaker 0: Right.

323
00:17:21,813 --> 00:17:26,933
Speaker 0: Like put a gyroscope in there, make the camera not suck, make the video not suck.

324
00:17:27,355 --> 00:17:31,492
Speaker 0: You know, it's like nothing here is like nothing no one thought of before.

325
00:17:31,572 --> 00:17:34,163
Speaker 0: It wasn't like they had an original idea anywhere in here.

326
00:17:34,384 --> 00:17:36,936
Speaker 0: You know, everything was just obvious ideas.

327
00:17:37,338 --> 00:17:38,182
Speaker 0: They actually did it.

328
00:17:38,584 --> 00:17:39,147
Speaker 0: That's all they did.

329
00:17:39,167 --> 00:17:39,267
Speaker 0: Right.

330
00:17:39,288 --> 00:17:39,750
Speaker 1: That's the difference.

331
00:17:40,952 --> 00:17:45,167
Speaker 1: Will do stuff like that, but they'll do it 80 percent there and then we'll give up on it.

332
00:17:45,247 --> 00:17:50,670
Speaker 0: I could go to Google and I could tell them every single one of these ideas and I could tell them a hundred other obvious ideas.

333
00:17:51,211 --> 00:17:56,692
Speaker 0: And if they did every single one of those, all they had, you know, just do them all, then they would destroy Apple.

334
00:17:56,752 --> 00:17:57,696
Speaker 0: Apple would have no chance.

335
00:17:58,580 --> 00:18:00,769
Speaker 1: I thought that Apple had Monopoly and all the good designers.

336
00:18:01,911 --> 00:18:04,281
Speaker 0: This is what they just hired that design guy.

337
00:18:04,381 --> 00:18:04,542
Speaker 0: Right.

338
00:18:04,562 --> 00:18:05,687
Speaker 0: Why do they actually put them to work?

339
00:18:05,908 --> 00:18:06,410
Speaker 1: Maybe they are.

340
00:18:06,570 --> 00:18:07,435
Speaker 1: Maybe he's working on stuff.

341
00:18:07,676 --> 00:18:08,339
Speaker 0: And that's another thing.

342
00:18:08,380 --> 00:18:09,486
Speaker 0: How come no one can design?

343
00:18:09,506 --> 00:18:10,330
Speaker 0: We've talked about this before.

344
00:18:10,390 --> 00:18:11,876
Speaker 0: How come no one can design anything?

345
00:18:12,920 --> 00:18:13,202
Speaker 0: There are.

346
00:18:13,222 --> 00:18:16,796
Speaker 0: there can't be only one designer of phones in the whole world.

347
00:18:17,077 --> 00:18:22,017
Speaker 1: I do want to point out that HTC is not there yet by any stretch, but they're there.

348
00:18:22,117 --> 00:18:26,938
Speaker 1: They have the most promise of any hardware phone maker outside of Apple right now.

349
00:18:27,379 --> 00:18:27,741
Speaker 1: Straight up.

350
00:18:27,781 --> 00:18:35,893
Speaker 1: Like if there if anyone is ever going to dethrone Apple in the hardware realm on phones, it's got to be HTC because HP is not going to do anything, even with their Web OS.

351
00:18:37,179 --> 00:18:45,733
Speaker 1: But in terms of just software, Web OS was probably, I think, a better contender than Android just in terms of polish and flashing functionality.

352
00:18:45,773 --> 00:18:48,483
Speaker 0: But then again, they had the fucking iPhone guys.

353
00:18:48,624 --> 00:18:48,905
Speaker 0: Yep.

354
00:18:49,547 --> 00:18:55,430
Speaker 1: Which is so crazy to me that despite no one can do it, it seems I don't know, they're just retarded.

355
00:18:55,690 --> 00:18:56,935
Speaker 0: I don't know what's up with that.

356
00:18:57,016 --> 00:18:58,682
Speaker 0: But here's someone actually wrote it.

357
00:18:58,723 --> 00:19:02,154
Speaker 0: There was an article I was reading today about Android fragmentation.

358
00:19:02,195 --> 00:19:02,335
Speaker 0: Right.

359
00:19:02,355 --> 00:19:09,426
Speaker 0: How people are complaining that, oh, if I make an Android app, I've got to either make it for Android one point five and not use any new features.

360
00:19:09,446 --> 00:19:13,884
Speaker 1: Google announced that like two point two is going to be like the last quick release and then they're going to have a much longer.

361
00:19:13,924 --> 00:19:14,266
Speaker 0: I didn't.

362
00:19:14,306 --> 00:19:15,290
Speaker 0: I didn't say anything about that.

363
00:19:15,571 --> 00:19:16,755
Speaker 1: Someone we were talking about that at work.

364
00:19:16,796 --> 00:19:23,683
Speaker 1: Someone said that it looks like the plan is two point two will be a huge update and then there will be much longer cycles between like.

365
00:19:23,864 --> 00:19:26,012
Speaker 0: that's not good because they need to catch up to Apple.

366
00:19:26,033 --> 00:19:27,760
Speaker 0: They need big changes real fast.

367
00:19:27,820 --> 00:19:31,977
Speaker 1: Maybe this pure speculation if you don't know what the time frame is on two point two.

368
00:19:31,997 --> 00:19:32,861
Speaker 1: I haven't really looked into it.

369
00:19:33,142 --> 00:19:35,691
Speaker 1: What if they got that designer guy working hardcore in an Apple style?

370
00:19:35,851 --> 00:19:36,152
Speaker 0: I don't know.

371
00:19:36,172 --> 00:19:36,613
Speaker 0: We'll see.

372
00:19:36,673 --> 00:19:46,796
Speaker 0: But the thing is, right, is that people are talking about fragmentation and it's like it sucks if I have phones and it sucks.

373
00:19:46,936 --> 00:19:47,157
Speaker 0: Right.

374
00:19:47,659 --> 00:19:52,539
Speaker 0: Whereas it's much easier with the iPhone because only, you know, the so few and they all have the same OS and it just works.

375
00:19:53,282 --> 00:19:53,603
Speaker 0: Whatever.

376
00:19:54,286 --> 00:19:56,193
Speaker 0: So the one guy was like, look right.

377
00:19:56,213 --> 00:20:02,519
Speaker 0: The way the cell phone market used to work for a long time, you know, like HTC and all these guys, Samsung, Motorola.

378
00:20:02,800 --> 00:20:03,021
Speaker 0: Right.

379
00:20:03,342 --> 00:20:06,655
Speaker 0: They really didn't have any control over the software that they were putting on their phones.

380
00:20:07,157 --> 00:20:10,490
Speaker 0: So what they did is they would just keep iterating on the hardware really quickly.

381
00:20:10,570 --> 00:20:12,879
Speaker 0: That's why they're like 16 different razors.

382
00:20:13,542 --> 00:20:13,803
Speaker 0: Right.

383
00:20:13,863 --> 00:20:16,614
Speaker 0: All running the same software because they couldn't do anything about the software.

384
00:20:16,634 --> 00:20:18,361
Speaker 0: So they just kept updating the phone constantly.

385
00:20:19,304 --> 00:20:24,004
Speaker 0: So these guys are really good at updating phone hardware rapidly, but they're not good at software.

386
00:20:24,044 --> 00:20:25,169
Speaker 0: They suck ass at software.

387
00:20:25,189 --> 00:20:25,972
Speaker 0: They have no idea.

388
00:20:26,032 --> 00:20:26,293
Speaker 0: Right.

389
00:20:26,614 --> 00:20:27,679
Speaker 0: So now they're moving.

390
00:20:27,699 --> 00:20:29,064
Speaker 0: You know, now they get Android.

391
00:20:29,466 --> 00:20:36,213
Speaker 0: They're actually moving to a different situation where you don't have to keep upgrading the hardware constantly of one hardware and use you upgrading the software.

392
00:20:36,935 --> 00:20:37,859
Speaker 0: And they're not used to that.

393
00:20:37,960 --> 00:20:41,072
Speaker 0: So these like as soon as they get used to it, it'll be all good.

394
00:20:41,534 --> 00:20:41,755
Speaker 0: Right.

395
00:20:42,237 --> 00:20:54,807
Speaker 0: But I think the big the core problem here is that you have this Android, which is designed to work on pretty much any, you know, phone hardware that meets like the minimum specs compared to the iPhone, where the OS is.

396
00:20:55,068 --> 00:20:58,562
Speaker 0: the software is made specifically for one hardware, you know, and that's it.

397
00:20:59,004 --> 00:20:59,807
Speaker 0: And on a computer.

398
00:20:59,827 --> 00:20:59,927
Speaker 0: Right.

399
00:21:00,691 --> 00:21:01,273
Speaker 0: You know, this.

400
00:21:01,494 --> 00:21:03,481
Speaker 0: this buys you a little bit on the computer.

401
00:21:03,502 --> 00:21:03,702
Speaker 0: Right.

402
00:21:03,722 --> 00:21:04,947
Speaker 0: You know, the Mac.

403
00:21:05,188 --> 00:21:05,409
Speaker 0: Right.

404
00:21:05,449 --> 00:21:09,305
Speaker 0: They have the software that is meant to go on that hardware and it buys them a little bit.

405
00:21:09,365 --> 00:21:09,506
Speaker 0: Right.

406
00:21:09,526 --> 00:21:12,257
Speaker 0: They have the nice like multi gesture touchpad on there.

407
00:21:12,277 --> 00:21:14,184
Speaker 0: That's like the only thing on the MacBook that I like.

408
00:21:14,746 --> 00:21:14,947
Speaker 0: Right.

409
00:21:15,348 --> 00:21:16,433
Speaker 0: And it buys them that much.

410
00:21:16,814 --> 00:21:22,959
Speaker 0: But it doesn't really buy them anything else because every computer, even Macs are still a mouse, a keyboard and a flat screen.

411
00:21:22,979 --> 00:21:28,099
Speaker 0: And, you know, you know, all software Linux, Mac, Windows is made for that set of interfaces.

412
00:21:28,179 --> 00:21:32,135
Speaker 0: And there isn't really much you can do to change the hardware, really.

413
00:21:32,175 --> 00:21:34,042
Speaker 0: So that software wouldn't integrate well.

414
00:21:34,424 --> 00:21:38,359
Speaker 0: But on a phone, you know, something small, the hardware is crazy different.

415
00:21:38,399 --> 00:21:38,620
Speaker 0: Right.

416
00:21:38,761 --> 00:21:42,275
Speaker 0: You know, the thing that got the iPod so big is the fact that has that little circle in the middle.

417
00:21:42,316 --> 00:21:45,470
Speaker 0: And every other freaking one has something stupid that tries to knock it off.

418
00:21:46,390 --> 00:21:48,097
Speaker 1: Nintendo's revolution was that D-pad.

419
00:21:48,117 --> 00:21:49,061
Speaker 0: Yeah, it was a D-pad.

420
00:21:49,121 --> 00:21:49,382
Speaker 0: Right.

421
00:21:49,483 --> 00:21:53,538
Speaker 0: So all their stuff is made for the D-pad and everyone else has ghetto D-pad.

422
00:21:53,618 --> 00:21:57,050
Speaker 1: To this day, the Xbox 360 directional pad is a piece of shit.

423
00:21:57,170 --> 00:21:57,752
Speaker 0: It's the worst.

424
00:21:58,034 --> 00:21:58,556
Speaker 0: Oh, God.

425
00:21:58,576 --> 00:22:02,030
Speaker 0: I guess, you know, even at least the PlayStation has like the four separate buttons.

426
00:22:02,511 --> 00:22:06,026
Speaker 0: So you can't fuck it up, you know, even though it's not as good as a D-pad on the Nintendo.

427
00:22:06,106 --> 00:22:06,709
Speaker 0: But it's still.

428
00:22:06,829 --> 00:22:07,231
Speaker 0: it's all right.

429
00:22:07,251 --> 00:22:08,536
Speaker 0: It's better than the Xbox one.

430
00:22:09,721 --> 00:22:15,425
Speaker 0: So the fact that Apple integrates the hardware and the software, you know, they don't have the fragmentation issue.

431
00:22:15,486 --> 00:22:15,807
Speaker 0: They don't.

432
00:22:15,887 --> 00:22:16,108
Speaker 0: Right.

433
00:22:16,149 --> 00:22:18,056
Speaker 0: They solve all these problems at once.

434
00:22:18,116 --> 00:22:19,422
Speaker 0: It gives them a huge.

435
00:22:19,522 --> 00:22:22,293
Speaker 0: it's like imagine you're playing a board game.

436
00:22:22,333 --> 00:22:22,574
Speaker 0: Right.

437
00:22:22,775 --> 00:22:24,823
Speaker 0: And you've got like, you know, something.

438
00:22:24,903 --> 00:22:29,482
Speaker 0: And it's like, OK, this thing is plus five, but only plus five in this situation.

439
00:22:29,562 --> 00:22:30,767
Speaker 0: Otherwise, plus one.

440
00:22:30,787 --> 00:22:34,863
Speaker 0: So in PCs, integrating the hardware and the software gives Apple plus one.

441
00:22:34,903 --> 00:22:42,093
Speaker 0: But on phones plus five and the other people can't really compete with the plus five because they're not making the hardware and the software together.

442
00:22:42,113 --> 00:22:43,819
Speaker 0: And that's it's.

443
00:22:44,040 --> 00:22:45,947
Speaker 0: I think it's going to stay that way for a while.

444
00:22:45,988 --> 00:22:47,011
Speaker 0: And I'm just like, oh, fuck it.

445
00:22:47,433 --> 00:22:48,999
Speaker 0: I hate all this closed shit.

446
00:22:49,019 --> 00:22:51,730
Speaker 0: I'll just but all the other phones suck so bad.

447
00:22:52,151 --> 00:22:53,798
Speaker 0: This phone still sucks the least.

448
00:22:54,360 --> 00:22:55,987
Speaker 0: I haven't made a mobile app yet.

449
00:22:56,007 --> 00:22:58,096
Speaker 0: I don't see me making a mobile app in the future.

450
00:22:58,116 --> 00:23:00,084
Speaker 1: It's possible where people like us might end up.

451
00:23:00,264 --> 00:23:03,919
Speaker 1: I mean, I always say it, but the dark horse is still windows.

452
00:23:04,542 --> 00:23:09,180
Speaker 1: Yeah, it's it's such a dark horse, but they're going in a different direction from everybody else.

453
00:23:09,200 --> 00:23:11,530
Speaker 0: But they're still not going to have the hardware software integration.

454
00:23:11,890 --> 00:23:15,749
Speaker 1: Now, unless they make some deal with HTC, that'd be my dream team.

455
00:23:17,231 --> 00:23:17,673
Speaker 0: It's rough.

456
00:23:18,115 --> 00:23:23,818
Speaker 0: Or if only HTC had gotten Web OS, because Web OS, if only HTC would learn how to do some fucking software.

457
00:23:23,858 --> 00:23:24,099
Speaker 0: Right.

458
00:23:24,159 --> 00:23:27,392
Speaker 0: I mean, they keep trying to modify it, like do the sense UI thing.

459
00:23:27,412 --> 00:23:29,219
Speaker 1: HTC needs to give up on all that.

460
00:23:29,279 --> 00:23:34,541
Speaker 1: And they need to just get into a direct hundred percent partnership with someone who makes software and agree.

461
00:23:34,561 --> 00:23:36,127
Speaker 0: Well, that's the thing is that they all.

462
00:23:36,448 --> 00:23:38,335
Speaker 0: HTC only cares about selling hardware.

463
00:23:38,395 --> 00:23:38,636
Speaker 0: Right.

464
00:23:38,958 --> 00:23:40,001
Speaker 0: So what are they going to do?

465
00:23:40,343 --> 00:23:49,359
Speaker 0: They're going to make phones that, you know, for every possible software, no matter which one you're using, you're going to buy an HTC phone will be the best.

466
00:23:49,440 --> 00:23:53,175
Speaker 1: But I think they might sell more hardware if they combine with someone else.

467
00:23:53,275 --> 00:23:54,801
Speaker 1: Like, you know, it's too late for Web OS.

468
00:23:54,841 --> 00:24:04,198
Speaker 1: But if they bought Palm and then those two together made the unified environment like Apple did, and then they just they push that to try to make themselves the number two thing is to make unified.

469
00:24:04,218 --> 00:24:05,302
Speaker 0: You'd have to start over again.

470
00:24:05,322 --> 00:24:08,575
Speaker 0: You know, the palm is already sort of integrated with the devices.

471
00:24:08,615 --> 00:24:08,756
Speaker 0: Right.

472
00:24:08,776 --> 00:24:15,342
Speaker 0: They've got sort of that touch screen below the screen area, you know, and they've got the pixie really like that, like that little touch.

473
00:24:15,643 --> 00:24:16,989
Speaker 1: I really like on the free.

474
00:24:17,450 --> 00:24:19,077
Speaker 0: Yeah, it's not that exciting.

475
00:24:19,398 --> 00:24:19,820
Speaker 1: I like it.

476
00:24:19,860 --> 00:24:20,522
Speaker 1: It works really well.

477
00:24:20,562 --> 00:24:23,313
Speaker 1: Also, the kind of feedback you get with that glowing orb and everything.

478
00:24:24,317 --> 00:24:36,326
Speaker 1: Let's talk at least a little bit about some of the other things, because they announced I ads and it made me realize that I for as long as I've had a smartphone, anytime I've installed an app and it's so much you showed me an ad, I deleted it immediately.

479
00:24:36,346 --> 00:24:36,567
Speaker 0: Yep.

480
00:24:37,109 --> 00:24:44,959
Speaker 1: And I can't imagine myself ever being at a point where I'll ever, ever, ever want to see an ad in anything ever again.

481
00:24:44,979 --> 00:24:46,444
Speaker 0: There's ads in your mobile browser.

482
00:24:46,464 --> 00:24:48,010
Speaker 0: You don't have ad block on your phone, do you?

483
00:24:48,311 --> 00:24:49,016
Speaker 1: Yeah, but you know what?

484
00:24:49,137 --> 00:24:50,888
Speaker 1: The ads, the Slashdot ads don't show up.

485
00:24:53,891 --> 00:24:54,413
Speaker 0: That's pretty good.

486
00:24:54,734 --> 00:24:57,607
Speaker 1: Plus, I use RSS readers whenever I can, too.

487
00:24:57,787 --> 00:24:59,153
Speaker 1: I block the ads wherever I can.

488
00:24:59,173 --> 00:25:04,394
Speaker 0: And an ad like being someone should make some sort of like Web proxy you can use on your phone.

489
00:25:04,434 --> 00:25:07,145
Speaker 1: that would block the ads would be a good way to do it.

490
00:25:08,230 --> 00:25:17,628
Speaker 1: But I had is interesting because I think it's going to cause and there's going to be a lot more submissions to the store initially, I think, from scummy, crappy companies making crappy little apps.

491
00:25:17,688 --> 00:25:21,262
Speaker 0: Well, no, the thing is, right, I ads, it bothers me and it doesn't bother me.

492
00:25:21,322 --> 00:25:21,543
Speaker 0: Right.

493
00:25:21,623 --> 00:25:23,812
Speaker 0: It bothers me that they're trying to push ads on the phone.

494
00:25:23,852 --> 00:25:25,258
Speaker 0: It's like, I don't want ads on my phone.

495
00:25:25,359 --> 00:25:26,022
Speaker 0: Get the fuck away.

496
00:25:26,102 --> 00:25:26,584
Speaker 0: But you know what?

497
00:25:26,604 --> 00:25:29,998
Speaker 0: There's already ads all over pretty much a ton of free iPhone apps.

498
00:25:30,058 --> 00:25:30,821
Speaker 0: all have ads in them.

499
00:25:31,222 --> 00:25:36,081
Speaker 0: And the ads are coming from other companies that aren't Apple and Apple, I think, said they're going to continue to allow those.

500
00:25:36,102 --> 00:25:40,740
Speaker 0: They're not going to kick them off as long as they don't break the other rules that are going on there.

501
00:25:40,821 --> 00:25:41,041
Speaker 0: Right.

502
00:25:41,644 --> 00:25:46,021
Speaker 0: But the thing is, those ads are just really annoying and shitty and they don't work.

503
00:25:46,081 --> 00:25:50,118
Speaker 0: Like I was once playing this free game and this ad showed up and then I uninstalled the game, of course.

504
00:25:50,178 --> 00:25:57,748
Speaker 0: But the ad like pauses in your middle of the game, goes full screen and they're like, you try to push the next button and it barely works in there.

505
00:25:57,808 --> 00:25:58,009
Speaker 0: Right.

506
00:25:58,069 --> 00:26:02,288
Speaker 0: It's it tries to play this video and you got to pause it and then wait for the look.

507
00:26:03,951 --> 00:26:07,284
Speaker 1: They're polishing the experience of putting shit in your face.

508
00:26:07,404 --> 00:26:07,866
Speaker 0: Exactly.

509
00:26:07,886 --> 00:26:16,902
Speaker 0: And then they're selling it to like, you know, much bigger brands instead of, you know, hey, play some online poker now, you know, which is what the ads are doing on the phones.

510
00:26:16,983 --> 00:26:17,184
Speaker 0: Right.

511
00:26:17,224 --> 00:26:20,296
Speaker 0: You know, they're going to be like, hey, go watch the latest movie in the theaters.

512
00:26:20,939 --> 00:26:31,363
Speaker 0: You know, so it's good that the ads will be less, you know, bothersome, but it's bad that there's going to be more ads and I'll just have to avoid it as long as they're only in apps.

513
00:26:31,383 --> 00:26:31,504
Speaker 0: Right.

514
00:26:31,524 --> 00:26:32,569
Speaker 0: They're not putting them in the OS.

515
00:26:33,330 --> 00:26:35,721
Speaker 0: So as long as you don't have an app with ads in it, you'll be all right.

516
00:26:36,665 --> 00:26:42,510
Speaker 0: Which usually, I guess, means paying for apps or finding free ones that don't have ads.

517
00:26:43,211 --> 00:26:46,104
Speaker 1: I don't think there's a lot more really important to talk about with this thing.

518
00:26:46,144 --> 00:26:49,156
Speaker 0: No, it's just, you know, it's just more of a repeat story.

519
00:26:49,216 --> 00:26:52,990
Speaker 0: It's like, look, yet again, everyone, why does everyone suck?

520
00:26:53,390 --> 00:27:05,620
Speaker 0: This is, you know, if I just had a bunch of high quality designers and programmers and I was the boss of them and I could just, all I'd have to do is walk in on the first day, write a checklist and say, when you've done everything on this checklist, you can release the phone.

521
00:27:05,720 --> 00:27:13,393
Speaker 0: Oh, and it has to be done within, you know, six months because if you don't, I'm going to have to add some more things to the checklist in six months as technology advances.

522
00:27:13,795 --> 00:27:16,105
Speaker 0: And all they had to do is finish the list and Apple would be...

523
00:27:16,125 --> 00:27:17,330
Speaker 1: All they had to do was write the software.

524
00:27:17,630 --> 00:27:19,557
Speaker 0: And then Apple would be dead and that would be it.

525
00:27:19,617 --> 00:27:21,022
Speaker 0: But no one can do it.

526
00:27:21,323 --> 00:27:21,684
Speaker 0: Why not?

527
00:27:21,745 --> 00:27:26,721
Speaker 0: And then they come out and they release phones where like the checklist is half checked and it's like, dude, no one wants that.

528
00:27:27,062 --> 00:27:29,450
Speaker 0: You're not even half as good as what the other guy already has.

529
00:27:29,570 --> 00:27:33,370
Speaker 1: See, I think part of the issue might be, look at what Apple does every time they release a new iPhone or something.

530
00:27:33,870 --> 00:27:36,198
Speaker 1: They release missing a lot of functionality.

531
00:27:36,218 --> 00:27:51,090
Speaker 1: people say they want, but because of the brand and because of the power and the polish, people will buy at much higher margins things that don't do everything on the checklist, which probably subsidizes development of the other things on the checklist to bring more of the people in and always stay ahead of everybody.

532
00:27:51,752 --> 00:27:52,735
Speaker 0: But that's the thing is like...

533
00:27:52,896 --> 00:28:00,481
Speaker 1: If you were like, I got my new phone, no one's going to pay what they pay for an iPhone for your awesome phone no matter how awesome it is right away.

534
00:28:00,802 --> 00:28:04,474
Speaker 0: Well, it is true that Apple has not checked off every box on the list.

535
00:28:04,775 --> 00:28:06,823
Speaker 0: Like, you know, remember they didn't check off copy paste.

536
00:28:06,863 --> 00:28:08,790
Speaker 0: They didn't even have apps to begin with, right?

537
00:28:09,431 --> 00:28:11,739
Speaker 0: And that is why they are beatable.

538
00:28:11,779 --> 00:28:14,730
Speaker 0: Because if you do check off more boxes than they do, you can win.

539
00:28:15,172 --> 00:28:15,574
Speaker 0: It's hard to say.

540
00:28:15,594 --> 00:28:18,470
Speaker 0: If they actually checked off the whole thing, they would be invincible, right?

541
00:28:18,790 --> 00:28:22,023
Speaker 1: Their brand name is really... Don't underestimate the power of that brand.

542
00:28:22,043 --> 00:28:23,730
Speaker 1: It would take a lot to bring that brand down.

543
00:28:24,110 --> 00:28:27,550
Speaker 0: Because part of the check... If you had everything on the checklist, that would be enough.

544
00:28:27,670 --> 00:28:34,110
Speaker 1: So part of your checklist, I thought you don't agree with like coercing people through persuasive like argument and advertising.

545
00:28:34,650 --> 00:28:35,634
Speaker 1: You're going to alter people's...

546
00:28:35,654 --> 00:28:40,857
Speaker 0: You wouldn't need to because part, you know, it would have to be designed really well and it would be amazing in its own right.

547
00:28:40,897 --> 00:28:44,209
Speaker 1: But isn't designing really well just a form of coercion just like politeness is?

548
00:28:44,671 --> 00:28:45,653
Speaker 1: No.

549
00:28:45,773 --> 00:28:50,705
Speaker 0: In a device, in a thing that you are carrying with you, it's not about, you know, the design of coercing you.

550
00:28:53,050 --> 00:28:55,030
Speaker 1: What if you have to trick them into buying it with advertising?

551
00:28:55,610 --> 00:28:59,301
Speaker 0: You don't need to trick anyone into buying it because it's industrial design.

552
00:28:59,381 --> 00:29:02,470
Speaker 0: It's obviously superior and a better experience.

553
00:29:02,770 --> 00:29:07,790
Speaker 1: I point out obviously superior and better experience have often not meant much in the consumer world.

554
00:29:08,271 --> 00:29:08,632
Speaker 0: Really?

555
00:29:09,013 --> 00:29:10,016
Speaker 0: When, when have they not?

556
00:29:10,497 --> 00:29:15,130
Speaker 0: When is the, when is the obviously superior thing not been the consumer product of choice?

557
00:29:15,450 --> 00:29:15,671
Speaker 0: Betamax?

558
00:29:15,691 --> 00:29:16,936
Speaker 0: In the technology world.

559
00:29:17,157 --> 00:29:17,519
Speaker 1: Betamax?

560
00:29:17,539 --> 00:29:19,046
Speaker 0: But it was not obviously superior.

561
00:29:19,066 --> 00:29:20,070
Speaker 0: It had less running time.

562
00:29:20,150 --> 00:29:23,321
Speaker 0: And people don't give a shit about quality of the video or the audio.

563
00:29:23,522 --> 00:29:25,910
Speaker 0: They cared about the running time and the price much more.

564
00:29:26,270 --> 00:29:27,638
Speaker 0: So that is why VHS won.

565
00:29:27,699 --> 00:29:29,630
Speaker 0: It was obviously superior to the consumer.

566
00:29:29,850 --> 00:29:35,490
Speaker 1: Well, see, the thing is you keep saying obviously superior to the consumer, but different consumers do have different utility.

567
00:29:35,811 --> 00:29:38,376
Speaker 0: The, the, the, the consumer as a whole, right?

568
00:29:38,416 --> 00:29:45,170
Speaker 0: The, the, the mass populous, the, the, you know, price and running time were far greater utility overall.

569
00:29:45,310 --> 00:29:50,810
Speaker 0: I'm sure there are many people who needed more quality video editing people and such, and those people bought Betamax, you know?

570
00:29:51,230 --> 00:29:53,670
Speaker 0: But the vast majority of people, obviously.

571
00:29:53,830 --> 00:29:55,390
Speaker 1: Plus what about Farmville over Counter-Strike?

572
00:29:56,170 --> 00:29:58,510
Speaker 0: That obviously most more people want Farmville.

573
00:29:58,931 --> 00:30:03,865
Speaker 1: So would you say therefore that from a game design perspective, Farmville is better?

574
00:30:04,166 --> 00:30:05,530
Speaker 1: From a marketing perspective it is.

575
00:30:05,530 --> 00:30:08,310
Speaker 0: No, from an industrial design perspective, Farmville is better.

576
00:30:08,330 --> 00:30:09,850
Speaker 1: From a money making for me perspective.

577
00:30:10,290 --> 00:30:10,377
Speaker 0: Yes.

578
00:30:18,330 --> 00:30:20,013
Speaker 1: So things of the day.

579
00:30:20,895 --> 00:30:25,564
Speaker 1: There was a prank at RIT that was oft talked about, but never enacted.

580
00:30:25,604 --> 00:30:28,470
Speaker 1: I don't know if it would actually work on anyone, but we called it truck.

581
00:30:29,351 --> 00:30:30,776
Speaker 1: I'm sure many of you have heard of this before.

582
00:30:30,836 --> 00:30:34,970
Speaker 1: The idea was that you find someone who's asleep, possibly they've passed out drunk or something, so they're really asleep.

583
00:30:34,970 --> 00:30:37,630
Speaker 0: Is this like the one where they do the fake buried alive thing?

584
00:30:38,771 --> 00:30:39,974
Speaker 1: Ah, the fake buried alive.

585
00:30:39,994 --> 00:30:40,997
Speaker 1: That was great.

586
00:30:42,100 --> 00:30:43,504
Speaker 1: I think you're referring to that video in college here.

587
00:30:43,524 --> 00:30:45,830
Speaker 1: Remember when the guy finally got out, he punched everybody in the face?

588
00:30:46,090 --> 00:30:46,177
Speaker 0: Yeah.

589
00:30:47,030 --> 00:30:54,730
Speaker 1: So truck, what you do is you wait for someone to fall asleep, you know, laying on their back, and you take two flashlights and you hold them far away from their face and far apart.

590
00:30:55,291 --> 00:31:07,170
Speaker 1: And you have two other people standing in either ear just slowly mumbling truck, truck, truck, truck, truck, truck, getting slowly louder as you bring the flashlights closer and closer together.

591
00:31:07,571 --> 00:31:10,543
Speaker 0: Don't you start the flashlights close together and bring them farther apart?

592
00:31:10,563 --> 00:31:12,009
Speaker 0: Because that's what it looks like when a truck is...

593
00:31:12,190 --> 00:31:13,558
Speaker 1: I think you start far away from their face.

594
00:31:13,598 --> 00:31:15,470
Speaker 1: You start right in their eyes, it's going to blind them.

595
00:31:15,830 --> 00:31:21,065
Speaker 0: No, you have the flashlights close together and far away, and you spread them out as you get closer.

596
00:31:21,125 --> 00:31:22,930
Speaker 0: That's what it looks like when a truck is heading towards you.

597
00:31:22,930 --> 00:31:24,822
Speaker 1: See, we never actually tried to do it.

598
00:31:24,863 --> 00:31:25,929
Speaker 1: We only talked about it.

599
00:31:26,490 --> 00:31:27,942
Speaker 0: I think a truck is coming at you.

600
00:31:28,103 --> 00:31:28,930
Speaker 0: Two distant lights.

601
00:31:29,090 --> 00:31:35,450
Speaker 1: But the point is, we have the lights getting closer, truck, truck, truck, truck, truck, and then you scream truck and hit them in the face.

602
00:31:35,850 --> 00:31:36,850
Speaker 0: With like a pillow or something.

603
00:31:36,970 --> 00:31:39,838
Speaker 1: And, you know, we always suspected that people would freak out.

604
00:31:40,360 --> 00:31:43,670
Speaker 1: And we never did it, and the few times we tried it didn't work because it probably wouldn't work.

605
00:31:44,310 --> 00:31:50,530
Speaker 1: But what does work, as this video will show, is waking someone up and yelling instructions at them.

606
00:31:51,171 --> 00:31:57,850
Speaker 1: Because if you wake someone up urgently enough, and you give them instructions while they're still confused, they will probably just do whatever you tell them to do.

607
00:31:58,070 --> 00:31:59,689
Speaker 1: Yeah, you just shake someone and go, "Oh my god, oh my god, oh my

608
00:31:59,709 --> 00:31:59,770
Speaker 1: god!".

609
00:31:59,951 --> 00:32:02,408
Speaker 1: Like, "Oh my god, bite this banana, now take this skin in the closet, we're all going to

610
00:32:02,448 --> 00:32:02,669
Speaker 1: die!".

611
00:32:03,232 --> 00:32:04,115
Speaker 1: And it's great.

612
00:32:04,135 --> 00:32:05,801
Speaker 1: This video's worth watching.

613
00:32:06,544 --> 00:32:08,070
Speaker 1: I suggest you try this on your friends.

614
00:32:08,790 --> 00:32:12,610
Speaker 0: Alright, so I had an issue technologically this morning with SSH, right?

615
00:32:12,630 --> 00:32:13,490
Speaker 1: It happens to everyone.

616
00:32:13,530 --> 00:32:22,683
Speaker 0: Yes, I had SSH on one computer and on another computer and I had him configured identically and everything identical and I Accessation to one with the public key.

617
00:32:22,784 --> 00:32:23,247
Speaker 0: It was all good.

618
00:32:24,052 --> 00:32:28,670
Speaker 0: I station the other one with the public key rejected and I'm like, what the fuck retired.

619
00:32:28,931 --> 00:32:33,646
Speaker 0: So I do SSH like for ultra verbose to see all the text doesn't tell me shit.

620
00:32:34,188 --> 00:32:34,790
Speaker 0: I'm like, what the fuck?

621
00:32:34,890 --> 00:32:36,057
Speaker 1: Well, you put like two V's in there.

622
00:32:36,077 --> 00:32:37,344
Speaker 0: I have a three V's in there.

623
00:32:37,364 --> 00:32:38,450
Speaker 1: I didn't do anything.

624
00:32:38,631 --> 00:32:43,850
Speaker 0: You can always get more verbose Until you see every single deep level of debugging.

625
00:32:44,191 --> 00:32:45,657
Speaker 1: I've always been more of a like.

626
00:32:45,737 --> 00:32:48,930
Speaker 1: I'm the kind of person who will like tail varlog messages while I'm doing stuff.

627
00:32:49,031 --> 00:32:50,889
Speaker 1: I don't tend to use verbose outputs from the.

628
00:32:51,111 --> 00:32:59,802
Speaker 0: I was doing it on the client As this age whatever I was doing but with a - V in there to see some weird point about the kind of work I do.

629
00:33:00,991 --> 00:33:05,765
Speaker 1: I'm almost never not like everything I. everything that ever matters to me is server-side.

630
00:33:06,206 --> 00:33:12,610
Speaker 0: like if I have a client-side problem I'm like what I was just trying to get the information right about why I was being rejected.

631
00:33:12,811 --> 00:33:19,130
Speaker 0: So I was like, I'm typing this command and I'm like, well, obviously the server should be telling the client why it was rejected.

632
00:33:19,130 --> 00:33:20,735
Speaker 0: Yeah so if I do the right.

633
00:33:20,755 --> 00:33:24,093
Speaker 0: but I didn't see anything and I couldn't for the life of me figure out what the hell it was.

634
00:33:24,113 --> 00:33:38,040
Speaker 0: and then eventually right, I You know I went to server fault and I got the answer which was Go look in this log file on the server and you'll find the real answer which is like varlog Auth or varlog security something like that.

635
00:33:38,120 --> 00:33:40,950
Speaker 0: and I found the answer which is like permissions on this folder.

636
00:33:41,191 --> 00:33:44,390
Speaker 0: I check permissions on like all the SSH configs and all those folders.

637
00:33:44,932 --> 00:33:50,170
Speaker 0: I didn't check permissions on my actual home folder which apparently matter which is I don't know what the fuck is up with that anyway.

638
00:33:50,933 --> 00:34:02,970
Speaker 0: But the thing the reason I was messing with SSH is because of these three links that are my thing of the day collectively right, so one of them is SSH automatic reverse tunnels for workflow simplification.

639
00:34:03,732 --> 00:34:09,370
Speaker 0: So what this does is, you know, you can do SSH tunnel right connect the port on this machine to this port on this machine.

640
00:34:09,710 --> 00:34:20,379
Speaker 0: So you can make it so that you could telnet to localhost on Port 30 and that'll actually take you to port 80 Google.com or some other machine.

641
00:34:20,418 --> 00:34:25,056
Speaker 0: you can SSH to and but it will be over SSH So it'll be all encrypted right.

642
00:34:25,659 --> 00:34:28,370
Speaker 0: what this does is you SSH a new machine.

643
00:34:29,072 --> 00:34:31,120
Speaker 0: You put some stuff in your SSH config file.

644
00:34:31,481 --> 00:34:33,850
Speaker 0: it automatically makes a tunnel going the other direction.

645
00:34:34,452 --> 00:34:44,280
Speaker 0: so then what you can do for example is you could you know, I could SSH the other machine and then SCP something to local and then it'll come back.

646
00:34:44,340 --> 00:34:49,839
Speaker 0: so I can SSH I can copy files back over the line while I'm on the Other machine, you know, you could also.

647
00:34:49,859 --> 00:34:50,623
Speaker 0: it's a useful thing.

648
00:34:50,643 --> 00:34:51,588
Speaker 0: So this is a very useful trick.

649
00:34:52,554 --> 00:34:56,618
Speaker 0: useful trick number two Transparent multi hop SSH, right?

650
00:34:57,139 --> 00:35:02,136
Speaker 0: So you SSH into one machine and then you have agent forwarding all configured Which I'm not even gonna talk about.

651
00:35:02,176 --> 00:35:08,230
Speaker 0: you can go find out about agent forwarding on your own and then you SSH from there To another machine to another machine to another machine, right?

652
00:35:08,930 --> 00:35:20,284
Speaker 0: But let's say your network is configured in such a way Right to where you've got a whole bunch of computers and only one of them has a path out to the world Right, so you have to SSH to that one first and then to a second one.

653
00:35:20,324 --> 00:35:21,549
Speaker 0: that's like hidden behind it.

654
00:35:21,950 --> 00:35:25,586
Speaker 0: So you have to SSH two times every time you want to talk to this inside machine.

655
00:35:25,686 --> 00:35:29,870
Speaker 0: even with agent forwarding You won't have to type in a password, but you'll still have to SSH two times.

656
00:35:30,633 --> 00:35:34,470
Speaker 0: You can configure SSH in such a way using this transparent multi hop.

657
00:35:34,831 --> 00:35:41,490
Speaker 0: So where you SSH directly to the inside machine and it'll automatically go hop hop into it right through the entryway.

658
00:35:42,439 --> 00:35:46,288
Speaker 0: And here is the third SSH thing is this thing called BC VI.

659
00:35:46,670 --> 00:35:56,590
Speaker 0: I haven't really looked at this too carefully to figure out exactly how it works But basically it's back channel VI and what it allows you to do is you SSH to a remote machine.

660
00:35:57,131 --> 00:35:59,537
Speaker 0: You have no X forwarding none whatsoever.

661
00:35:59,617 --> 00:36:04,730
Speaker 0: No X right, but you want to use your nice G VIM your GUI VIM on your local machine.

662
00:36:04,811 --> 00:36:06,243
Speaker 0: You don't want to use VIM in the terminal.

663
00:36:06,263 --> 00:36:07,030
Speaker 0: That's not as good.

664
00:36:07,931 --> 00:36:20,050
Speaker 0: It'll actually you know over the forwarding back channel Whatever open up the remote file in the local Graphical VIM and then you can save it and it'll save to the other where it's supposed to save and that's pretty awesome.

665
00:36:20,110 --> 00:36:31,808
Speaker 0: That's like I always like end up like installing X on machines that don't need X just so I can use Or I'll use like I'll mount remote file systems like using G VFS or some ridiculous thing like that.

666
00:36:31,828 --> 00:36:36,330
Speaker 1: Yeah Yeah, see if we finally added we have an X win 32 like that thing running.

667
00:36:36,631 --> 00:36:44,998
Speaker 1: So yeah, because it has X way it has this live functionality, which is you have permanent persistent sessions Mmm, so I can basically disconnect.

668
00:36:45,019 --> 00:36:50,503
Speaker 1: so I have like an FVWM custom with like, you know I'm a cessation like hundreds of boxes and all this stuff going on.

669
00:36:50,824 --> 00:36:51,146
Speaker 1: some of it.

670
00:36:51,166 --> 00:36:52,170
Speaker 1: I've like active processes.

671
00:36:52,190 --> 00:36:54,037
Speaker 0: I don't know how people like work like.

672
00:36:54,098 --> 00:37:00,725
Speaker 0: I see other people working and they have windows like a hundred tabs and they get a Hundred windows open and I'm like, I just close everything immediately.

673
00:37:00,926 --> 00:37:02,559
Speaker 0: I have like one the tab I'm working on.

674
00:37:02,580 --> 00:37:03,310
Speaker 0: if I'm not working on it, right?

675
00:37:04,171 --> 00:37:12,150
Speaker 1: I leave terminals open in different colored background FVWM desktops to different environments in case I have to respond to something quickly.

676
00:37:12,230 --> 00:37:18,893
Speaker 1: Those are the you know, I go to the red desktop, which is where I have like shells to all the key production Ready to go then.

677
00:37:19,455 --> 00:37:22,082
Speaker 1: I escalate permissions to the user that can actually do shit.

678
00:37:22,443 --> 00:37:23,567
Speaker 1: I don't stay escalated.

679
00:37:23,587 --> 00:37:27,936
Speaker 1: So I don't accidentally, you know It's like other people's workflows.

680
00:37:27,976 --> 00:37:31,830
Speaker 0: It seems like most people I've seen, you know, watch compute have a lot of stuff open.

681
00:37:32,071 --> 00:37:34,600
Speaker 1: Oh, it's not even that I think the average person.

682
00:37:34,660 --> 00:37:37,550
Speaker 1: they just leave all these fucking windows open in the background all over the place.

683
00:37:37,951 --> 00:37:41,466
Speaker 0: Yeah, and I feel like I'm the only one who ever like I just constantly closing things.

684
00:37:41,486 --> 00:37:42,470
Speaker 0: I'm like close close close close.

685
00:37:42,932 --> 00:37:44,158
Speaker 1: I try to close everything.

686
00:37:44,198 --> 00:37:46,710
Speaker 1: I'm not using or expecting to use in the next few minutes.

687
00:37:46,851 --> 00:37:49,808
Speaker 0: I close like even things I'm gonna use in the next few minutes and I open them again.

688
00:37:49,969 --> 00:37:52,119
Speaker 0: Yeah That's you know, that's the way it is.

689
00:37:52,220 --> 00:37:56,207
Speaker 0: Anyway, so there's three awesome SSH tricks for you.

690
00:37:57,971 --> 00:38:06,403
Speaker 1: So we this was kind of a crowd people but I think it'll work especially since I'm watching Ghost in the Shell standalone complex again And goddamn is that not a great show?

691
00:38:06,443 --> 00:38:09,057
Speaker 1: even if you're not an anime fan You don't listen to the Wednesday shows.

692
00:38:09,579 --> 00:38:17,514
Speaker 1: if you care about science and technology and sci-fi You'll probably like Ghost in the Shell because it's basically straight up about Cyberization and what it means.

693
00:38:17,534 --> 00:38:21,127
Speaker 0: I'm pretty sure like a lot of non anime peoples have seen Ghost in the Shell.

694
00:38:21,167 --> 00:38:22,030
Speaker 0: Yeah, it's like Akira.

695
00:38:22,090 --> 00:38:25,990
Speaker 0: It's like all kinds of people have seen it, but they probably don't realize it's not restricted to anime.

696
00:38:26,050 --> 00:38:28,990
Speaker 0: There's a fact I would say it's more popular among non anime fan.

697
00:38:29,050 --> 00:38:30,014
Speaker 1: They probably don't know.

698
00:38:30,095 --> 00:38:33,570
Speaker 1: over that there are two 26 episode TV shows a motion to the movie.

699
00:38:33,570 --> 00:38:33,932
Speaker 1: I don't know.

700
00:38:33,972 --> 00:38:39,305
Speaker 0: some of the people who watch the Adult Swim might have known because I think one of them or both of them Was on the Adult Swim at some point.

701
00:38:39,386 --> 00:38:39,730
Speaker 0: I don't know.

702
00:38:39,831 --> 00:38:49,510
Speaker 0: I do think that yes they are far less well known than the movie and there are many people who perhaps saw the movie back in the 90s and Did not know that there was more that had come out afterwards.

703
00:38:49,791 --> 00:38:55,510
Speaker 1: But the show the main like push of the show and the movies is the ad, you know, what is consciousness?

704
00:38:55,730 --> 00:38:56,275
Speaker 1: what are we?

705
00:38:56,335 --> 00:39:00,438
Speaker 1: that sort of thing and It really raises the issue of cyberization.

706
00:39:00,498 --> 00:39:08,144
Speaker 1: Like what will society be like when we can just start replacing Entire human bodies with robots that effectively are immortal?

707
00:39:08,505 --> 00:39:14,084
Speaker 1: and the only thing we can't really Make immortal yet is an actual physical brain.

708
00:39:14,104 --> 00:39:26,070
Speaker 0: I Think one thing will definitely be problematic right is that you know right now we have a you know issues of power and electricity All right, and you know humans are powered by food.

709
00:39:26,692 --> 00:39:30,306
Speaker 0: So we if we get into some kind of world we stop making food.

710
00:39:30,366 --> 00:39:31,370
Speaker 0: that saves us a bunch.

711
00:39:31,975 --> 00:39:33,510
Speaker 0: But then we have to make more electricity.

712
00:39:33,651 --> 00:39:35,363
Speaker 0: Otherwise you'll die when you run out of batteries.

713
00:39:35,604 --> 00:39:42,250
Speaker 1: or at the same time I'm we already made a robot that kills flies eats them and then uses that to make more electricity to kill more flies.

714
00:39:42,431 --> 00:39:46,790
Speaker 0: But is that you know, is that gonna be enough to power a whole human being forever?

715
00:39:47,390 --> 00:39:51,504
Speaker 0: Well the whole you know the idea of you know in Galaxy Express 3 9, you know, what did the?

716
00:39:51,564 --> 00:39:53,130
Speaker 0: the robot people eight people?

717
00:39:54,613 --> 00:40:08,670
Speaker 1: Well, I think that the issue there is that you know The thing that's holding back technology at least technology that individual humans will use More so than anything now is the fact that we cannot make safe batteries that hold a lot of power.

718
00:40:09,413 --> 00:40:10,718
Speaker 0: Oh, that's a completely separate issue.

719
00:40:10,758 --> 00:40:12,565
Speaker 1: Yeah, it's the same thing for cyberization.

720
00:40:12,605 --> 00:40:14,010
Speaker 1: Like that's a hurdle that would love to be overcome.

721
00:40:14,070 --> 00:40:15,034
Speaker 1: How do you power this stuff?

722
00:40:15,415 --> 00:40:19,490
Speaker 1: I mean, we can't even make a laptop that lasts 12 hours and does anything useful.

723
00:40:19,854 --> 00:40:21,110
Speaker 1: How are we gonna make a person?

724
00:40:22,691 --> 00:40:23,755
Speaker 0: We're not right.

725
00:40:23,795 --> 00:40:28,489
Speaker 0: we're the best that we're gonna get I think without advances in power that are crazy.

726
00:40:28,509 --> 00:40:33,463
Speaker 0: is You know some just some robot parts, you know, we already got some of this going on.

727
00:40:33,503 --> 00:40:37,014
Speaker 1: people biomechanical systems might work Especially if we have this kind of like.

728
00:40:37,295 --> 00:40:42,570
Speaker 1: we make some sort of pseudo biological muscle tissue that we control with technology.

729
00:40:42,650 --> 00:40:45,320
Speaker 0: I mean people already have pacemakers right plenty of them.

730
00:40:45,642 --> 00:40:49,360
Speaker 0: that some guy just went home with a Artificial heart for the first time.

731
00:40:49,421 --> 00:40:50,750
Speaker 0: other people have hard artificial heart.

732
00:40:50,750 --> 00:40:51,777
Speaker 0: I mean, you know, whatever took one home.

733
00:40:51,797 --> 00:40:55,461
Speaker 1: a lot of people when they talk about cyber ization They either think you know goes to the shell right away.

734
00:40:55,482 --> 00:41:02,806
Speaker 1: They don't realize that Humans, I've been cyber izing themselves like for serious where there have been cyborgs since we had the first prosthetic.

735
00:41:02,906 --> 00:41:04,250
Speaker 0: our crutches are what?

736
00:41:04,350 --> 00:41:04,752
Speaker 0: You know what?

737
00:41:04,833 --> 00:41:06,401
Speaker 0: It's quite, you know, that doesn't count.

738
00:41:06,421 --> 00:41:08,010
Speaker 0: They're not integrated though.

739
00:41:08,110 --> 00:41:11,279
Speaker 1: Maybe it maybe cyborg requires integrated first hook.

740
00:41:11,580 --> 00:41:12,924
Speaker 0: a peg leg doesn't count.

741
00:41:13,124 --> 00:41:14,307
Speaker 1: Yeah, those might count.

742
00:41:14,548 --> 00:41:15,210
Speaker 0: Why doesn't that count?

743
00:41:15,210 --> 00:41:15,934
Speaker 1: Those are prosthetics.

744
00:41:16,034 --> 00:41:16,295
Speaker 1: Yeah.

745
00:41:17,220 --> 00:41:19,170
Speaker 0: Oh, why does it have to be attached to you to count though?

746
00:41:19,470 --> 00:41:22,370
Speaker 1: Well integrated prosthetics are very different from detached prosthetics.

747
00:41:22,712 --> 00:41:24,238
Speaker 1: I mean the love I crutches.

748
00:41:24,279 --> 00:41:26,830
Speaker 1: and by that logic a pencil is a prosthetic because I can write with it.

749
00:41:26,850 --> 00:41:27,291
Speaker 1: I couldn't do that.

750
00:41:27,332 --> 00:41:27,693
Speaker 1: Normally.

751
00:41:27,713 --> 00:41:33,230
Speaker 0: Well, I mean neurologically it actually is a prosthetic right your your brain treats when you use tools.

752
00:41:33,290 --> 00:41:36,070
Speaker 0: Your brain treats the tools as a part of your own body.

753
00:41:36,732 --> 00:41:37,033
Speaker 0: You know.

754
00:41:37,113 --> 00:41:41,046
Speaker 0: so in that way, I guess it is sort of a prosthetic to use any tool.

755
00:41:41,126 --> 00:41:45,178
Speaker 0: physical wise You know a keyboard is an extension of my body when I am typing.

756
00:41:45,519 --> 00:41:53,481
Speaker 1: well by definition a prosthesis is literally just Something that replaces a body part that is now missing.

757
00:41:53,902 --> 00:41:55,547
Speaker 0: Mmm, but I mean we're not talking about.

758
00:41:55,567 --> 00:41:56,550
Speaker 0: we're not talking about.

759
00:41:56,972 --> 00:41:59,021
Speaker 0: We're not talking about just prosthesis though.

760
00:41:59,041 --> 00:42:01,010
Speaker 0: We're talking about any cyber ization.

761
00:42:01,231 --> 00:42:06,830
Speaker 1: Well, that's gonna be the big push is that right now and I think this is kind of the most there is non prosthetic cyber ization.

762
00:42:07,013 --> 00:42:19,741
Speaker 1: the most interesting Aspect of cyber ization is that there we're going to get to the point where people will do it voluntarily not to overcome a failure Of their body but to enhance an already perfectly functioning body.

763
00:42:20,182 --> 00:42:22,890
Speaker 1: that's going to be I think the biggest tipping point in human history.

764
00:42:24,234 --> 00:42:25,851
Speaker 0: Some people are already sort of doing well We have.

765
00:42:25,891 --> 00:42:30,809
Speaker 1: that was surgery now with baseball players the biggest example that one surgery that makes them basically a better thrower.

766
00:42:31,191 --> 00:42:32,054
Speaker 1: That originally was used to.

767
00:42:32,335 --> 00:42:37,434
Speaker 1: I forget what problem it treated but it was found that by doing it You basically make them pitch faster.

768
00:42:37,796 --> 00:42:37,996
Speaker 0: Yeah.

769
00:42:38,016 --> 00:42:40,022
Speaker 0: Well, I mean look people are taking steroids, right?

770
00:42:40,042 --> 00:42:42,530
Speaker 0: What happens when we have like nanobots steroids?

771
00:42:42,850 --> 00:42:47,250
Speaker 0: Yep, or steroids that goes around your muscles and it cleans up all the lactic acid faster.

772
00:42:47,653 --> 00:42:55,150
Speaker 0: So, you know, it doesn't hurt when you exert yourself and then you can be like, ah What if we made steroids that have zero negative side effects?

773
00:42:55,812 --> 00:43:00,450
Speaker 0: All they do is increase your little robots that go around making muscles and eating fats.

774
00:43:01,956 --> 00:43:02,830
Speaker 1: So what do we do at that point?

775
00:43:02,951 --> 00:43:04,538
Speaker 1: We've already talked but I want to get into the argument.

776
00:43:04,619 --> 00:43:06,850
Speaker 0: everyone will turn into like like ultra buff.

777
00:43:07,452 --> 00:43:13,630
Speaker 1: I don't want to get into sports because that argument we've had on this show more than once as to what that'll mean for sports.

778
00:43:14,192 --> 00:43:16,141
Speaker 1: Mm-hmm, I guess I don't know what.

779
00:43:16,483 --> 00:43:17,749
Speaker 1: at what point would you go for it?

780
00:43:18,430 --> 00:43:19,112
Speaker 0: I'll go for it.

781
00:43:19,494 --> 00:43:20,176
Speaker 0: Mostly Mike.

782
00:43:20,196 --> 00:43:23,990
Speaker 0: The only reason I wouldn't go for it, which is a pretty big reason right is the safety issue.

783
00:43:24,091 --> 00:43:24,292
Speaker 0: Right.

784
00:43:24,333 --> 00:43:28,013
Speaker 0: It's like okay, so I'll install this Bionic eye.

785
00:43:28,655 --> 00:43:32,770
Speaker 0: Oh fuck, you know, there was a camera in my bionic eye and someone else is watching.

786
00:43:33,112 --> 00:43:34,198
Speaker 0: Great, that was awesome.

787
00:43:34,218 --> 00:43:35,486
Speaker 1: That was a plot in Ghost in the Shell.

788
00:43:37,333 --> 00:43:39,950
Speaker 0: Pretty much everything you're gonna come up with is gonna be a plot in something, right?

789
00:43:40,410 --> 00:43:41,835
Speaker 1: Most of them will be in Ghost in the Shell.

790
00:43:42,116 --> 00:43:44,343
Speaker 0: Or, you know, surrogates or something like that, right?

791
00:43:44,905 --> 00:43:46,410
Speaker 0: Oh, I'll replace my arm.

792
00:43:46,570 --> 00:43:50,503
Speaker 0: Oh shit, my arm got hacked and I did something I didn't want to do.

793
00:43:50,523 --> 00:43:54,618
Speaker 1: I don't think surrogates brought up any issue That wasn't already brought up by Ghost in the Shell.

794
00:43:54,638 --> 00:43:57,550
Speaker 0: I don't know, it had the stay at home.

795
00:43:58,291 --> 00:44:03,170
Speaker 0: Well, so it's Ghost in the Shell with remote agents and Ghost in the Shell to man-machine interface.

796
00:44:03,190 --> 00:44:04,539
Speaker 0: Yeah, I don't know what came first.

797
00:44:04,559 --> 00:44:05,042
Speaker 1: And in the TV show.

798
00:44:05,908 --> 00:44:08,576
Speaker 0: I guess Anyway, didn't you know but it didn't have the.

799
00:44:08,696 --> 00:44:16,970
Speaker 0: I think what surrogates has is the is the social issue of what if everyone's staying At home and a robot is going out right as opposed to it goes in the shell.

800
00:44:17,030 --> 00:44:20,030
Speaker 0: It's like some people go out some surrogates go out.

801
00:44:20,151 --> 00:44:21,199
Speaker 1: Well, that's the interesting thing.

802
00:44:21,279 --> 00:44:23,714
Speaker 1: A lot of people they watch Ghost in the Shell They think of it as like.

803
00:44:23,995 --> 00:44:27,810
Speaker 1: so that is a future world where we've reached this level where people are cyberizing.

804
00:44:27,890 --> 00:44:34,630
Speaker 1: But really it's looking at this is a world where the technology is new and only the richest people are doing this.

805
00:44:34,630 --> 00:44:35,253
Speaker 0: I mean think about this.

806
00:44:35,294 --> 00:44:38,370
Speaker 0: Let's say I'm still using keyboards, right and I got cyber hands.

807
00:44:38,892 --> 00:44:40,919
Speaker 0: Someone could figure out what password I'm typing.

808
00:44:40,939 --> 00:44:44,010
Speaker 0: if they could hack my hand because they see me typing, right?

809
00:44:44,110 --> 00:44:45,870
Speaker 0: They could see the signals go into the fingers.

810
00:44:46,212 --> 00:44:49,870
Speaker 0: Just it would be like they have a key logger in my hand instead of a key logger in the keyboard.

811
00:44:49,970 --> 00:44:51,376
Speaker 1: Well, that's one of the big one.

812
00:44:51,396 --> 00:44:56,657
Speaker 1: of the biggest issues too is what if we get to the point to where you can hack A person the same way that you hack a computer.

813
00:44:56,697 --> 00:44:57,580
Speaker 0: That's why you got it.

814
00:44:57,700 --> 00:45:00,710
Speaker 0: I gotta be you know, here's here's my concern.

815
00:45:01,352 --> 00:45:09,644
Speaker 1: I do not believe that in the United States the legal system we have in place and the government systems We have in place could ever deal with that situation.

816
00:45:09,785 --> 00:45:18,630
Speaker 0: No, they couldn't human hack Yeah, basically become anarchy if it could never be addressed by because people who are cyberized and we're like Smart hackers would secure themselves.

817
00:45:18,811 --> 00:45:22,723
Speaker 0: They couldn't be hacked and they would hack everyone else with absolutely.

818
00:45:22,803 --> 00:45:24,830
Speaker 0: it would basically be like gods walking around.

819
00:45:24,870 --> 00:45:26,115
Speaker 0: They could just control people.

820
00:45:26,175 --> 00:45:31,856
Speaker 0: and you know, I mean we're already approaching a point where we can read people's, you know Actual biological brains.

821
00:45:31,876 --> 00:45:34,085
Speaker 0: if people have digital brains.

822
00:45:34,105 --> 00:45:35,470
Speaker 0: Oh, they're so fucked.

823
00:45:36,274 --> 00:45:36,998
Speaker 0: They're so fucked.

824
00:45:37,139 --> 00:45:37,763
Speaker 0: Oh my god.

825
00:45:37,843 --> 00:45:38,065
Speaker 0: Yep.

826
00:45:38,769 --> 00:45:40,075
Speaker 1: So What?

827
00:45:40,095 --> 00:45:43,330
Speaker 1: so let's say there weren't any safety issues like say let's say it's like ghost in the shell.

828
00:45:43,451 --> 00:45:44,941
Speaker 1: So we're at the ghost in the shell world, right?

829
00:45:44,961 --> 00:45:46,190
Speaker 0: There's so many safety issues.

830
00:45:46,313 --> 00:45:53,650
Speaker 0: Oh my gosh the majors is gonna come and use my body to kill some guy and then I'll have to get it to pay a ton of money to get a new cyber body.

831
00:45:53,650 --> 00:45:56,363
Speaker 1: But you saw that only happens to you know people.

832
00:45:56,403 --> 00:45:57,830
Speaker 1: she's after she's not after us.

833
00:45:57,990 --> 00:46:00,970
Speaker 0: There's plenty of security issues going on in the ghost in the shell world.

834
00:46:00,990 --> 00:46:02,138
Speaker 0: That's pretty much all it's about.

835
00:46:02,158 --> 00:46:03,910
Speaker 0: is everyone's getting you know hacked?

836
00:46:04,050 --> 00:46:04,894
Speaker 1: I think everyone is in.

837
00:46:04,914 --> 00:46:06,019
Speaker 1: the majority of people aren't.

838
00:46:06,240 --> 00:46:07,425
Speaker 0: but you mean even look at like go.

839
00:46:07,465 --> 00:46:13,417
Speaker 0: and even the first ghost in the Shell manga right there was that police officer with like a cyber arm and the software in it Wasn't even hacked.

840
00:46:13,437 --> 00:46:14,870
Speaker 0: It was just like buggy software in it.

841
00:46:14,870 --> 00:46:19,550
Speaker 0: So her arm wasn't working properly and then the major hacked her and was like, ah, this arm is all fucked up.

842
00:46:19,651 --> 00:46:20,849
Speaker 0: Oh, it's got the shitty software in it.

843
00:46:20,950 --> 00:46:21,674
Speaker 1: I've done that before.

844
00:46:21,734 --> 00:46:27,230
Speaker 1: remember we were at RIT hanging out in Luke's apartment with Dave and Luke was not gone And we went to use his computer and it was locked.

845
00:46:27,290 --> 00:46:31,730
Speaker 1: So I broke into it because we wanted to look on the internet and then we saw that he didn't have video drivers installed.

846
00:46:31,951 --> 00:46:35,183
Speaker 1: He was running like 320 by 200 and I fixed it.

847
00:46:35,203 --> 00:46:35,846
Speaker 1: Yep.

848
00:46:35,966 --> 00:46:36,669
Speaker 1: It was the same thing.

849
00:46:37,370 --> 00:46:37,993
Speaker 0: Yeah, right.

850
00:46:38,033 --> 00:46:46,190
Speaker 0: That's exact, you know, so but the thing is that's a bigger deal when it's your arm And you know right now, you know, it doesn't happen too often.

851
00:46:46,551 --> 00:46:52,018
Speaker 0: But imagine if you get in a situation where it's like fuck I can't get the better driver and now your arm is all fucked Up.

852
00:46:52,059 --> 00:46:55,825
Speaker 0: Yeah, I mean What is usually something that's gonna be so awesome.

853
00:46:55,905 --> 00:46:56,187
Speaker 0: Oh, man.

854
00:46:56,207 --> 00:46:57,814
Speaker 0: I'm so strong I got some grip.

855
00:46:57,855 --> 00:47:12,090
Speaker 0: I can punch real hard and type read crazy fast and You know play crocodile perfectly and throw real whatever but I'll fuck myself for is all fucked up And I can't fix it because of some reason now becomes an inconvenience.

856
00:47:12,351 --> 00:47:13,335
Speaker 0: What you know, it's not.

857
00:47:13,435 --> 00:47:17,310
Speaker 0: it's no longer a boon but a bane and I don't want to ever have a bane.

858
00:47:17,410 --> 00:47:18,838
Speaker 0: It's got to be 100% boon or at least 99% boon.

859
00:47:18,858 --> 00:47:19,481
Speaker 0: All right.

860
00:47:21,430 --> 00:47:24,748
Speaker 1: So what about the situation of consciousness like say you're getting near the end of your life?

861
00:47:25,370 --> 00:47:29,967
Speaker 1: Okay, physically your body is degrading your you know, you don't have more than five or six years to go.

862
00:47:29,987 --> 00:47:32,077
Speaker 1: Okay What about when we?

863
00:47:32,097 --> 00:47:34,430
Speaker 1: I don't think this will happen in our lifetime necessarily.

864
00:47:34,570 --> 00:47:38,843
Speaker 1: But what about the kind of analog transfer of consciousness or at least of memories?

865
00:47:39,024 --> 00:47:41,854
Speaker 0: Well if it's possible, right, you know It's a basic thing.

866
00:47:41,894 --> 00:47:53,277
Speaker 1: I would argue that it is definitely possible to make an Effectively perfect copy of an individual person's complete set of experiences memories because they're just stored in Physical goop in our heads.

867
00:47:53,859 --> 00:47:54,641
Speaker 0: It's all there.

868
00:47:55,023 --> 00:47:56,046
Speaker 0: Yeah, but it's a lot.

869
00:47:56,086 --> 00:47:57,010
Speaker 0: See what is a lot?

870
00:47:57,230 --> 00:47:58,053
Speaker 0: What could you store it in?

871
00:47:58,094 --> 00:47:59,640
Speaker 0: that wasn't an actual brain?

872
00:48:00,122 --> 00:48:02,933
Speaker 1: Well, let's say we could make something All right.

873
00:48:03,515 --> 00:48:10,960
Speaker 0: So if your body is sucking real bad, I mean, it's better to have right eat Even if someone hacks your you know, get a new body.

874
00:48:10,980 --> 00:48:13,650
Speaker 0: Yeah, and someone hacks it and it sucks.

875
00:48:14,152 --> 00:48:15,837
Speaker 0: It's better than nobody right?

876
00:48:15,857 --> 00:48:25,407
Speaker 0: So if I'm in a dying old geezer can't walk situation I'll take the hackable legs before I get you know, rather than the brain specifically.

877
00:48:25,447 --> 00:48:32,450
Speaker 1: I mean think about this Alright, so say we can make us remember our better to have be alive and you know, continue your consciousness.

878
00:48:32,571 --> 00:48:34,800
Speaker 1: Well, no, that's the interesting thing to be dead.

879
00:48:34,880 --> 00:48:37,150
Speaker 1: You keep saying be alive and continue my consciousness.

880
00:48:37,271 --> 00:48:40,990
Speaker 1: So, let me let me lay out a bunch of groundwork for this and then let's see how you answer now.

881
00:48:41,674 --> 00:48:44,430
Speaker 1: So, all right, your consciousness is not a unified thing.

882
00:48:44,972 --> 00:49:01,647
Speaker 1: We know that our human consciousness is all these disparate parts kind of talking to each other in Real time but not synchronized like one side of your brain Cannot know the state of any other part of the brain While that other side knows the state of it because there's propagation delays through the speed of light.

883
00:49:01,667 --> 00:49:02,189
Speaker 1: Mm-hmm.

884
00:49:02,691 --> 00:49:05,686
Speaker 1: So the state our consciousness is not a unit.

885
00:49:05,746 --> 00:49:06,490
Speaker 1: It's not a thing.

886
00:49:06,590 --> 00:49:09,659
Speaker 1: It's this kind of analog overall structure.

887
00:49:09,980 --> 00:49:13,329
Speaker 1: That is the like the temporal sum of all the interactions going on at once.

888
00:49:13,751 --> 00:49:16,790
Speaker 1: Yeah, so there is no like unit of consciousness.

889
00:49:16,971 --> 00:49:21,805
Speaker 1: It's just all these kind of independent systems hitting each other with huge delays between them.

890
00:49:21,845 --> 00:49:24,032
Speaker 1: Sometimes variable delays between them Mm-hmm.

891
00:49:24,373 --> 00:49:28,470
Speaker 1: So and it's a single thread your consciousness is the thread that is continuing.

892
00:49:29,253 --> 00:49:36,499
Speaker 1: So if you move your consciousness to another structure So the and the analogy of your consciousness is 100%.

893
00:49:36,499 --> 00:49:40,569
Speaker 1: every individual memory, you know every experience everything that is you.

894
00:49:41,472 --> 00:49:49,780
Speaker 1: But that thread very likely could not ever itself be transferred because it exists in this physical medium.

895
00:49:50,161 --> 00:49:55,096
Speaker 1: that can only exist on that physical medium if you Recreate it perfectly somewhere else.

896
00:49:55,377 --> 00:49:59,129
Speaker 1: You're recreating its structure with a different underlying physical structure.

897
00:50:00,469 --> 00:50:02,036
Speaker 0: Uh What you could do, right?

898
00:50:02,056 --> 00:50:08,018
Speaker 1: Let's say you have so even if you have a person say you make an organic brain That is 100% identical to the brain.

899
00:50:08,058 --> 00:50:12,190
Speaker 1: You're transferring out of right what everything all the memories can be transferred.

900
00:50:12,492 --> 00:50:21,570
Speaker 1: But basically if you turn on that new brain It would pick up thinking consciously the exact moment where your previous brain left off.

901
00:50:21,931 --> 00:50:24,942
Speaker 1: But it wouldn't stop your previous brain from thinking it would just be a new you.

902
00:50:25,263 --> 00:50:27,049
Speaker 1: that's completely indistinguishable from you.

903
00:50:27,952 --> 00:50:34,730
Speaker 1: Your threat of consciousness by the nature of physics very likely would not be able to inhabit any new big thing.

904
00:50:34,791 --> 00:50:41,994
Speaker 1: we could ever build a Perfect copy if you could but you would still die That's true.

905
00:50:42,035 --> 00:50:52,680
Speaker 0: I was gonna say like what if I went into like a real-time FMRI and in real time they built a brain that was identical to my brain Based on the FMRI, right?

906
00:50:52,700 --> 00:50:55,469
Speaker 0: So now you got two identical brains like but that one's not you.

907
00:50:56,092 --> 00:50:59,269
Speaker 0: But I would sit there looking at one that was me, but it wouldn't be me.

908
00:50:59,430 --> 00:51:00,435
Speaker 1: But here's the scary thing.

909
00:51:00,757 --> 00:51:03,209
Speaker 1: It would be indistinct indistinguishable from you.

910
00:51:03,652 --> 00:51:06,026
Speaker 0: But what would I still but the thing is right?

911
00:51:06,066 --> 00:51:09,650
Speaker 0: So, okay So now I they make it across the room from me, right?

912
00:51:09,810 --> 00:51:10,133
Speaker 0: There's two.

913
00:51:10,173 --> 00:51:12,669
Speaker 0: this is two rooms next to each other in the in the brain lab.

914
00:51:12,810 --> 00:51:16,690
Speaker 0: Yeah, and they make the copy of me in room number two, and I'm in room number one, right?

915
00:51:17,930 --> 00:51:21,783
Speaker 0: Already as you know, we're in sync while they're making it and then we're done.

916
00:51:22,004 --> 00:51:22,365
Speaker 0: So it goes.

917
00:51:22,406 --> 00:51:28,029
Speaker 0: ha already Scott one and Scott two are in separate environments, so they diverge immediately.

918
00:51:28,351 --> 00:51:32,405
Speaker 0: However, so we're not so I can think and talk to Scott too and it's not.

919
00:51:32,445 --> 00:51:35,338
Speaker 0: it's gonna have a conversation with me It's not gonna be in sync with me.

920
00:51:35,720 --> 00:51:40,599
Speaker 0: It's not gonna be like there's one Consciousness that is controlling both of these bodies.

921
00:51:40,640 --> 00:51:42,608
Speaker 1: That's the interesting and also scary part.

922
00:51:42,668 --> 00:51:45,189
Speaker 1: because alright So you have this copy of you it?

923
00:51:45,310 --> 00:51:54,161
Speaker 0: Picks up and until it realizes basically it'll look around the room and realize and of course if it has a younger invent You know better body than the old dying Scott has right?

924
00:51:54,402 --> 00:51:56,830
Speaker 0: It's definitely gonna have go on a different path.

925
00:51:56,930 --> 00:51:58,436
Speaker 0: Oh, yeah, then, you know the car.

926
00:51:58,476 --> 00:52:00,784
Speaker 0: they start at the exact same instant.

927
00:52:01,045 --> 00:52:01,747
Speaker 0: They would diverge.

928
00:52:01,928 --> 00:52:03,374
Speaker 1: but here's the thing Great, so they diverge.

929
00:52:03,414 --> 00:52:09,802
Speaker 1: All right, it immediately knows I am the copy in the better body Mm-hmm, but otherwise it has every memory that you had.

930
00:52:09,862 --> 00:52:10,364
Speaker 1: it is 100% you.

931
00:52:10,444 --> 00:52:13,455
Speaker 0: it is this though But it's not me because I'm still thinking.

932
00:52:13,756 --> 00:52:21,110
Speaker 1: yeah, so it is as though that moment you went into this new body But simultaneously your old body is still going the thread.

933
00:52:21,271 --> 00:52:24,688
Speaker 1: So you get a new cyborg body and you copy make a copy of your brain into it.

934
00:52:25,432 --> 00:52:30,350
Speaker 1: The you that got copied to the new one is still gonna die in the old body.

935
00:52:30,390 --> 00:52:34,290
Speaker 1: It can never be moved to another body because it is the sum of its physical parts.

936
00:52:34,551 --> 00:52:36,578
Speaker 1: The analogy can't ever know itself.

937
00:52:36,619 --> 00:52:39,650
Speaker 1: The analogy can't be transferred but consciousness is the analogy.

938
00:52:40,454 --> 00:52:43,710
Speaker 1: So it is possible with you know, the idea of cyber ization.

939
00:52:43,790 --> 00:52:54,510
Speaker 0: In fact, it is likely very likely that the process of making you're saying that once the old, you know I make a copy of myself and when the the old Scott dies the the new Scott will also die.

940
00:52:54,611 --> 00:52:58,148
Speaker 1: No, no, no, no that so you you've got your continuous threat of consciousness, right?

941
00:52:58,329 --> 00:53:00,960
Speaker 1: Yeah So right now you're thinking stuff and you have all your memories.

942
00:53:01,282 --> 00:53:02,748
Speaker 1: and so you're thinking I am here.

943
00:53:02,768 --> 00:53:03,270
Speaker 1: I am here.

944
00:53:03,330 --> 00:53:04,294
Speaker 1: You look at yourself.

945
00:53:04,675 --> 00:53:07,546
Speaker 1: Mm-hmm that thread of you you will let you know.

946
00:53:07,566 --> 00:53:08,590
Speaker 1: you get a new cyber body.

947
00:53:08,892 --> 00:53:10,581
Speaker 1: You won't wake up in the new cyber body.

948
00:53:10,822 --> 00:53:16,150
Speaker 1: you go to sleep you wake up You're in your old shitty body and there's a new Scott over there who leaves an awesome life and then you die.

949
00:53:16,311 --> 00:53:17,058
Speaker 0: Yeah, we already said that.

950
00:53:17,118 --> 00:53:17,320
Speaker 0: Yeah.

951
00:53:17,441 --> 00:53:22,608
Speaker 1: Yeah So what if you're near the end of your life and that is the only way to transfer?

952
00:53:22,628 --> 00:53:23,090
Speaker 1: do you do it?

953
00:53:23,250 --> 00:53:27,570
Speaker 1: Do you want to make a new copy of yourself that you will never experience to go off and do you forever?

954
00:53:28,190 --> 00:53:35,664
Speaker 0: Well, you know, it depends right like if I'm Being super productive like let's say I'm like Norman bollock or something, right then.

955
00:53:35,724 --> 00:53:42,940
Speaker 0: Yeah, because okay sure It's not gonna get me the huge benefit of still being alive You know, I mean you're already gonna die, right?

956
00:53:42,981 --> 00:53:46,714
Speaker 0: Yeah, so I mean, you know It's not really.

957
00:53:46,875 --> 00:53:50,995
Speaker 0: if you can't avoid the death right in any way All you can do is make a new you.

958
00:53:51,357 --> 00:53:52,521
Speaker 0: then oh, you know do it.

959
00:53:52,682 --> 00:53:55,936
Speaker 0: if and if extending your life would benefit You know the world.

960
00:53:56,077 --> 00:54:02,329
Speaker 0: but here's the icing thing, you know, if you're gonna die Anyway, what difference does it make it as a copy of yours the one you could make to use?

961
00:54:03,911 --> 00:54:05,921
Speaker 0: Maybe is that is there gonna be a benefit to?

962
00:54:05,961 --> 00:54:06,403
Speaker 0: to me's?

963
00:54:06,504 --> 00:54:07,710
Speaker 1: of course, you know ghost in the shell.

964
00:54:07,770 --> 00:54:11,410
Speaker 1: The only way they get around that is they say well, there's a ghost that we can't figure out how to transfer.

965
00:54:11,531 --> 00:54:15,310
Speaker 0: I mean, it depends on the people right whether how many it'll be worth it, right?

966
00:54:15,470 --> 00:54:17,217
Speaker 0: Let's say you're the best pitcher in baseball.

967
00:54:17,699 --> 00:54:19,144
Speaker 0: You should make five use.

968
00:54:19,325 --> 00:54:20,509
Speaker 0: you can pitch every day.

969
00:54:21,231 --> 00:54:22,494
Speaker 0: Maybe you should make ten use.

970
00:54:22,535 --> 00:54:33,264
Speaker 0: you can pitch in relief of yourself and you would win right and then make nine fielders Right of the same guy all hit home runs every time.

971
00:54:33,424 --> 00:54:39,562
Speaker 0: six babe Ruth's lose Lose right, so, you know, it depends right?

972
00:54:39,602 --> 00:54:46,222
Speaker 1: Whereas if you're say for example I Hitler no, we don't want to make more Hitler's.

973
00:54:46,442 --> 00:54:48,389
Speaker 0: you're a great military general, right?

974
00:54:48,650 --> 00:54:51,650
Speaker 0: You know a lot of military strategies and tactics, right?

975
00:54:51,991 --> 00:54:56,310
Speaker 0: You only want one of you but you want the ability to make a backup if you die, right?

976
00:54:56,410 --> 00:55:03,880
Speaker 0: But if there's two of you That's problematic because two of you are not gonna come up with something that one of you wouldn't have come up with But two of you that other guy.

977
00:55:03,920 --> 00:55:05,810
Speaker 0: he could become a traitor and now you're fighting against you.

978
00:55:05,910 --> 00:55:06,434
Speaker 0: You don't want that.

979
00:55:06,696 --> 00:55:08,670
Speaker 1: now you could be a really good fucking movie.

980
00:55:09,332 --> 00:55:11,619
Speaker 1: So the Russians steal the clone of MacArthur.

981
00:55:11,659 --> 00:55:16,194
Speaker 1: that's leading our army and they wake him up and then they teach him Look how much better our system is.

982
00:55:16,395 --> 00:55:18,702
Speaker 1: There's MacArthur us versus red MacArthur.

983
00:55:18,722 --> 00:55:21,150
Speaker 0: Yeah, that's you know, you get a super man.

984
00:55:21,210 --> 00:55:22,317
Speaker 1: It's an awesome fucking movie.

985
00:55:22,559 --> 00:55:24,290
Speaker 0: Yeah, but you don't want that to be real life, right?

986
00:55:24,330 --> 00:55:29,230
Speaker 0: You know, you just like you know, if you make five baseball pictures that are all you you don't want one traded.

987
00:55:30,473 --> 00:55:32,328
Speaker 1: You're forgetting what if you do the dr.

988
00:55:32,830 --> 00:55:33,331
Speaker 1: McNinja?

989
00:55:33,711 --> 00:55:44,460
Speaker 1: so you make ten of your of you the great general and you send them to fight in ten different fronts in American wars on the quote-unquote, but dr.

990
00:55:44,480 --> 00:55:46,006
Speaker 0: McNinja was able to reabsorb them.

991
00:55:46,046 --> 00:55:47,050
Speaker 0: We didn't say that you could know.

992
00:55:47,211 --> 00:55:48,305
Speaker 1: So you can't reabsorb them, right?

993
00:55:48,487 --> 00:55:48,690
Speaker 1: All right.

994
00:55:48,891 --> 00:55:52,770
Speaker 1: Well, maybe we could by having by combined giving them all the combined.

995
00:55:52,891 --> 00:55:59,350
Speaker 0: Oh, you get a whole bunch of people that are already awesome And they all get different training like each guy learns a different martial art.

996
00:55:59,631 --> 00:56:10,610
Speaker 0: Then they all come back and they either sit on the Council of elders and debate among each other or they all teach each other Also now all of them that the thing is right if you can transfer your stuff, right?

997
00:56:10,830 --> 00:56:16,310
Speaker 0: Why would you bother making ten guys to each learn one martial art then come back and all teach each other?

998
00:56:16,710 --> 00:56:18,760
Speaker 0: The ten March the nine martial arts that they didn't.

999
00:56:18,820 --> 00:56:19,383
Speaker 1: here's the other thing.

1000
00:56:19,444 --> 00:56:26,198
Speaker 0: you could in the same amount of time You could have one guy just go and learn ten martial arts Dies he can make another copy to continue on.

1001
00:56:26,258 --> 00:56:27,262
Speaker 1: would you want to?

1002
00:56:27,302 --> 00:56:28,125
Speaker 0: wouldn't take any less.

1003
00:56:28,165 --> 00:56:29,490
Speaker 1: would you want to take the full?

1004
00:56:29,650 --> 00:56:36,452
Speaker 1: Experiences of another person's life because you get everything you have all the skills they had but their consciousness You know.

1005
00:56:36,473 --> 00:56:47,019
Speaker 1: the new thread of their conscious that starts up in your brain Would theoretically unless you did something very clever be equal in strength to you And in fact you could not differentiate you from it.

1006
00:56:47,080 --> 00:56:47,882
Speaker 1: there just be one.

1007
00:56:48,223 --> 00:56:50,730
Speaker 1: would you do that if you were combining with someone awesome?

1008
00:56:51,871 --> 00:56:54,279
Speaker 0: Wait, so it's like I just added Einstein to myself.

1009
00:56:54,299 --> 00:56:58,638
Speaker 1: Yeah, there's two brains in my brain, but there is no you and Einstein There's just the you.

1010
00:56:58,678 --> 00:57:00,675
Speaker 1: that is the combat combination I mean goes to the shell.

1011
00:57:00,695 --> 00:57:01,299
Speaker 1: got into that one.

1012
00:57:01,380 --> 00:57:05,926
Speaker 0: Oh, right, right if I could do that It depends on who is but then it was never the one who was merged.

1013
00:57:06,007 --> 00:57:09,530
Speaker 1: all of humanity Have a perfect like to remind then go off into space.

1014
00:57:09,671 --> 00:57:12,203
Speaker 0: No, you can't merge all humanity because you get a bunch of shit in there.

1015
00:57:12,243 --> 00:57:13,590
Speaker 0: You gotta have a good filter on that shit.

1016
00:57:13,630 --> 00:57:14,476
Speaker 0: Yeah, yeah.

1017
00:57:14,496 --> 00:57:16,650
Speaker 1: Yeah, so all I'm leading up to with this is alright.

1018
00:57:16,771 --> 00:57:19,444
Speaker 1: So we we know, you know, you make a new coffee view.

1019
00:57:19,464 --> 00:57:20,590
Speaker 1: the threat of consciousness dies.

1020
00:57:20,892 --> 00:57:23,690
Speaker 1: Here's a practical consideration because we keep going into the theoretical.

1021
00:57:24,331 --> 00:57:32,150
Speaker 1: What if so you now are at your peak mental capacity because you know, we're at the age where we're before the metal decline.

1022
00:57:32,291 --> 00:57:32,572
Speaker 1: I don't know.

1023
00:57:32,633 --> 00:57:33,055
Speaker 0: I hope not.

1024
00:57:33,678 --> 00:57:34,000
Speaker 1: So we're.

1025
00:57:34,061 --> 00:57:35,710
Speaker 1: eventually we're gonna get to a point like, you know, ten.

1026
00:57:35,750 --> 00:57:37,097
Speaker 0: I hope I got more room to grow.

1027
00:57:37,439 --> 00:57:45,090
Speaker 1: So let's say you're at the point you're 40 and Scientists told us now that your brain from this point forward is going to degrade.

1028
00:57:45,991 --> 00:57:48,059
Speaker 1: You won't be as clever and quick.

1029
00:57:48,340 --> 00:57:50,950
Speaker 1: you'll never know then why they're quick than you are now, is that true?

1030
00:57:51,530 --> 00:57:52,097
Speaker 1: Let's say it is.

1031
00:57:52,664 --> 00:57:56,910
Speaker 0: if it's true Then we got to change the Constitution because we can't be allowing anyone over 40 to be president.

1032
00:57:57,291 --> 00:57:59,319
Speaker 0: But they have a declining mental state.

1033
00:57:59,379 --> 00:58:06,102
Speaker 1: thing is well, we do human brains decline But we make up for that with the benefit of the memories.

1034
00:58:06,142 --> 00:58:08,770
Speaker 1: We have the additional experience, but then is it really?

1035
00:58:08,770 --> 00:58:10,497
Speaker 1: 60 year old right?

1036
00:58:10,517 --> 00:58:11,403
Speaker 0: was that a decline?

1037
00:58:11,463 --> 00:58:12,510
Speaker 0: if you know more well.

1038
00:58:12,631 --> 00:58:16,591
Speaker 1: But the thing is the physical part does decline but you have the memories Uh-huh.

1039
00:58:16,772 --> 00:58:18,477
Speaker 1: All right, so so that so we know this.

1040
00:58:18,859 --> 00:58:26,402
Speaker 1: so if we somehow could prevent the physical decline and keep the memories We would just get better over time forever like the elves and burning wheel do.

1041
00:58:26,422 --> 00:58:27,144
Speaker 1: Mm-hmm.

1042
00:58:27,725 --> 00:58:28,648
Speaker 1: All right, so we've got.

1043
00:58:28,668 --> 00:58:29,310
Speaker 1: we got all that.

1044
00:58:30,192 --> 00:58:32,829
Speaker 1: You know, this is why the 60 year old racquetball player can beat me.

1045
00:58:33,532 --> 00:58:37,567
Speaker 1: I'm faster than him, but he knows a hell of a lot more about racquetball than I do.

1046
00:58:37,587 --> 00:58:40,449
Speaker 0: Uh-huh I didn't realize racquetball was a knowledge game.

1047
00:58:40,831 --> 00:58:42,378
Speaker 1: It is more so than a physical game.

1048
00:58:42,518 --> 00:58:47,889
Speaker 1: much older men who are very frail can kick the shit out of me Uh-huh, but don't they have to hit the ball real hard?

1049
00:58:48,414 --> 00:58:54,950
Speaker 1: No Cuz they hit it with such precision that it goes into the corner or he sees where I'm standing and knows how fast I am.

1050
00:58:55,392 --> 00:59:03,362
Speaker 1: So he hits it perfectly to where I have to run Super fast and just tires me out and tires me out and then he just beats me Thinking everyone just sit around racquetball.

1051
00:59:03,382 --> 00:59:03,866
Speaker 0: This is a way.

1052
00:59:03,906 --> 00:59:04,450
Speaker 0: big tension.

1053
00:59:04,511 --> 00:59:06,750
Speaker 0: Yeah, you get two people in this little box hitting the ball.

1054
00:59:06,770 --> 00:59:11,250
Speaker 0: All right, so if but why don't you just like how do you not bump into the other guy?

1055
00:59:11,410 --> 00:59:14,410
Speaker 0: I would just stand where you needed to go and you bump into me.

1056
00:59:14,470 --> 00:59:15,314
Speaker 1: You're in my way.

1057
00:59:15,334 --> 00:59:18,529
Speaker 1: I can just fucking hit you and then I get a point cuz you were in my way.

1058
00:59:19,591 --> 00:59:24,530
Speaker 0: Oh, so why don't I just barrel into the other guy on purpose every time instead of actually trying to hit the ball?

1059
00:59:25,410 --> 00:59:27,219
Speaker 1: Uh, you could that's that's a valid stretch.

1060
00:59:27,239 --> 00:59:29,490
Speaker 1: You might get a foul depends on what kind of you know style you're playing.

1061
00:59:29,711 --> 00:59:37,525
Speaker 0: But if I if the guy who gets bumped is the loser I would just play bump every time I like push rim.

1062
00:59:37,646 --> 00:59:38,790
Speaker 1: It's more complex than that.

1063
00:59:38,890 --> 00:59:41,219
Speaker 1: So it's my but the balls coming at me and I got to hit it.

1064
00:59:41,640 --> 00:59:45,409
Speaker 1: if you're in my way and I hit you It's your fault.

1065
00:59:45,832 --> 00:59:49,690
Speaker 0: So but if I if you run up if you're like hiding in the corner, I come over and kick you in the shin.

1066
00:59:49,771 --> 00:59:53,967
Speaker 0: No, no, no, no No, no when the ball was going to the opposite corner.

1067
00:59:54,168 --> 00:59:55,736
Speaker 0: now good And so how do we?

1068
00:59:55,756 --> 00:59:58,750
Speaker 0: we need a referee to determine if I were you were actually in my way or not?

1069
00:59:59,231 --> 01:00:00,801
Speaker 1: So having played racquetball a lot.

1070
01:00:01,223 --> 01:00:02,209
Speaker 1: It's always obvious.

1071
01:00:03,953 --> 01:00:04,799
Speaker 1: But all that aside.

1072
01:00:04,819 --> 01:00:06,330
Speaker 1: All right, so we've laid all this ground rules.

1073
01:00:06,390 --> 01:00:19,208
Speaker 1: So practical question you're 40. science tells us your that you your physical brain is going to get slower over time But your memories and knowledge are going to kind of balance that for another 20 or 30 years, but the decline is real.

1074
01:00:19,228 --> 01:00:27,670
Speaker 1: Mm-hmm Let's say we can cyber eyes you we can make a copy of your brain into a cyber brain that will not degrade.

1075
01:00:28,312 --> 01:00:33,790
Speaker 1: So your peak physical brain will go on forever never getting worse and your memories will just increase.

1076
01:00:34,031 --> 01:00:38,570
Speaker 1: All right, but to do this we have to destroy your current physical brain and kill you.

1077
01:00:39,871 --> 01:00:45,347
Speaker 1: I would you would you kill yourself to create a copy of you that you will never experience.

1078
01:00:45,708 --> 01:00:50,470
Speaker 1: that is indistinguishable from you to all for every intent and purpose.

1079
01:00:50,731 --> 01:01:01,139
Speaker 1: the only distinguishing factor between the two is that one it is immortal and will be forever and awesome and - Your solipsistic mind will never experience that.

1080
01:01:01,159 --> 01:01:02,324
Speaker 0: All right, so here's the.

1081
01:01:02,404 --> 01:01:03,750
Speaker 0: the answer is kind of obvious, right?

1082
01:01:03,870 --> 01:01:09,309
Speaker 0: You already said that you you know, you're you're still gonna be good for so many years, you know with your memory.

1083
01:01:09,530 --> 01:01:10,213
Speaker 0: But the physical decline.

1084
01:01:10,233 --> 01:01:12,906
Speaker 1: so say you're saying you wait till you're 60 to do it.

1085
01:01:13,810 --> 01:01:17,623
Speaker 1: There's the decline of 60 gets transferred over because you're that's your.

1086
01:01:18,245 --> 01:01:19,108
Speaker 0: why does it have to?

1087
01:01:19,128 --> 01:01:19,449
Speaker 0: that's BS.

1088
01:01:19,752 --> 01:01:20,408
Speaker 0: That's how it is.

1089
01:01:22,310 --> 01:01:23,921
Speaker 0: We're gonna make shit up.

1090
01:01:23,981 --> 01:01:26,556
Speaker 0: let's make shit up to be awesome We're you.

1091
01:01:26,576 --> 01:01:29,030
Speaker 0: the cyber body is always a peak physical.

1092
01:01:29,472 --> 01:01:30,295
Speaker 1: Think about it the weight.

1093
01:01:30,395 --> 01:01:31,299
Speaker 0: so you want to wait to.

1094
01:01:31,319 --> 01:01:33,026
Speaker 0: you have all the memories of being way old.

1095
01:01:33,187 --> 01:01:38,810
Speaker 1: if you want to copy The state of your brain you're gonna copy the broken shit along with the working shit.

1096
01:01:38,890 --> 01:01:47,330
Speaker 1: But if we know that much about brains we can fix the brain - can you though because the the brokenness of the brain is In is an inherent part of your consciousness.

1097
01:01:48,056 --> 01:01:49,529
Speaker 0: Now we can fix the brain.

1098
01:01:50,091 --> 01:01:56,530
Speaker 1: If you want to skip that part, let's go to another question, let's say to do cyber ization.

1099
01:01:56,972 --> 01:01:58,377
Speaker 1: There's like a final deadline.

1100
01:01:58,417 --> 01:02:01,609
Speaker 1: Like if you get past a certain point of degradation, they can't fix it.

1101
01:02:02,231 --> 01:02:10,860
Speaker 1: Mmm, so like if you're 80 and you're a senile you can't undo that when you're making the copy of you You'd make a copy of you that will stay at that level.

1102
01:02:10,920 --> 01:02:13,950
Speaker 0: all these unnecessary complications realistic complications.

1103
01:02:14,131 --> 01:02:15,900
Speaker 0: I don't think it's a realistic complication.

1104
01:02:16,100 --> 01:02:18,050
Speaker 1: Well, they are for physics and everything.

1105
01:02:18,191 --> 01:02:19,515
Speaker 1: All right, so let's say that's not the case.

1106
01:02:19,576 --> 01:02:23,530
Speaker 1: We can still kind of fix it, but we can't fix death.

1107
01:02:24,490 --> 01:02:29,685
Speaker 1: Alright at what point would you accept killing yourself to make a copy of you?

1108
01:02:29,726 --> 01:02:30,367
Speaker 1: That is immortal.

1109
01:02:30,468 --> 01:02:32,254
Speaker 0: what I'm gonna die Anyway, all right.

1110
01:02:32,274 --> 01:02:34,302
Speaker 1: So what if you can't be a hundred percent sure?

1111
01:02:34,804 --> 01:02:38,179
Speaker 0: I'm in severe pain All right, I'm suffering.

1112
01:02:38,200 --> 01:02:40,169
Speaker 0: I can't really do all that much.

1113
01:02:40,832 --> 01:02:49,586
Speaker 0: You know, maybe it's like a Stephen Hawking situation, you know, I'm not too old, but I'm not really, you know I'd be much better off with a brand new body or I'm 90.

1114
01:02:49,586 --> 01:02:54,663
Speaker 0: I'm in a hospital bed I can talk but really, you know, I got some cancer some shit.

1115
01:02:54,703 --> 01:02:56,288
Speaker 0: I can't you know do anything.

1116
01:02:56,488 --> 01:02:57,070
Speaker 0: It's it hurts.

1117
01:02:57,170 --> 01:02:58,295
Speaker 0: They got me on some drugs.

1118
01:02:58,315 --> 01:03:00,021
Speaker 0: It's not looking too good.

1119
01:03:00,041 --> 01:03:01,547
Speaker 0: No, you know, that's a good.

1120
01:03:01,567 --> 01:03:02,290
Speaker 0: that's a good point.

1121
01:03:02,511 --> 01:03:03,297
Speaker 1: So here's the thing.

1122
01:03:03,377 --> 01:03:09,030
Speaker 0: the the in a perfect world All my everyone I know and care about is dead because I outlived them all those bastards.

1123
01:03:09,492 --> 01:03:11,720
Speaker 1: Perfect world question is not nearly as interesting.

1124
01:03:11,740 --> 01:03:12,865
Speaker 1: So I want to go back to the other question.

1125
01:03:12,885 --> 01:03:13,869
Speaker 1: Let's say it sucks.

1126
01:03:14,371 --> 01:03:15,957
Speaker 0: All right, I'll go with your hypothetical.

1127
01:03:16,258 --> 01:03:17,101
Speaker 1: I think it's more interesting.

1128
01:03:17,141 --> 01:03:24,695
Speaker 1: Let's say right now this world it sucks We know for a fact the day you turn 40 it Your brain is gonna start declining.

1129
01:03:24,976 --> 01:03:29,090
Speaker 1: and when you cyber eyes it kills you because they take your brain apart to do it.

1130
01:03:29,271 --> 01:03:29,957
Speaker 1: so it kills you.

1131
01:03:29,978 --> 01:03:37,510
Speaker 1: all right and The state of your brain whatever physical decline you've had of the physical brain cannot be repaired in the new brain.

1132
01:03:37,510 --> 01:03:40,020
Speaker 1: It's just a copy that will never degrade further.

1133
01:03:40,040 --> 01:03:42,630
Speaker 1: All right, at what point do you cyber eyes?

1134
01:03:42,890 --> 01:03:45,079
Speaker 0: It depends on me right like like.

1135
01:03:45,099 --> 01:03:46,405
Speaker 0: I'm asking what am I what?

1136
01:03:46,465 --> 01:03:47,670
Speaker 0: No, it depends on me, right?

1137
01:03:47,731 --> 01:03:48,649
Speaker 0: Like what are my skills?

1138
01:03:48,850 --> 01:04:00,910
Speaker 0: Like am I gonna be able to accomplish some awesomeness way by adding some memories despite the degradation of the brain, you know Is the rest of my life looking pretty good, you know, is it looking pretty shitty?

1139
01:04:01,111 --> 01:04:05,103
Speaker 0: Is it looking like I'm gonna you know make you know, it's like okay.

1140
01:04:05,163 --> 01:04:07,190
Speaker 0: I can do some pretty awesome with the rest of my life.

1141
01:04:07,271 --> 01:04:08,348
Speaker 0: I'll stick with it, you know.

1142
01:04:10,053 --> 01:04:12,950
Speaker 0: Or you know, it looks like I'm not gonna be able to do any awesome.

1143
01:04:13,251 --> 01:04:16,104
Speaker 0: But a cyber me can do so much awesome.

1144
01:04:16,325 --> 01:04:17,329
Speaker 0: It can change the whole world.

1145
01:04:17,891 --> 01:04:18,854
Speaker 1: So shit.

1146
01:04:19,175 --> 01:04:19,636
Speaker 1: Oh shit.

1147
01:04:19,757 --> 01:04:20,920
Speaker 1: Why so I'm looking online.

1148
01:04:21,321 --> 01:04:34,530
Speaker 1: I'll sacrifice myself for the greater good of the cyber, whatever brain power peaks at about 22 and Maintains steadily until about 27 and then declines irrevocably as bullshit.

1149
01:04:34,751 --> 01:04:37,223
Speaker 1: That's that's the reality of the world according to all these articles.

1150
01:04:37,263 --> 01:04:38,147
Speaker 1: I've been pulling up with.

1151
01:04:38,167 --> 01:04:38,730
Speaker 0: that's not true.

1152
01:04:39,053 --> 01:04:39,921
Speaker 0: That's what everyone says.

1153
01:04:39,941 --> 01:04:40,587
Speaker 0: No, it's not true.

1154
01:04:40,708 --> 01:04:51,910
Speaker 1: So Assuming that all these these like theses and all this research I'm looking at right now is true We're at the point at which we will never have the physical brain power again that we have right now.

1155
01:04:52,611 --> 01:04:52,811
Speaker 1: Mm-hmm.

1156
01:04:53,153 --> 01:04:53,715
Speaker 1: What are we gonna do?

1157
01:04:53,735 --> 01:04:59,502
Speaker 1: Because I think if we had cyberization right now, I Think within about five or six years.

1158
01:04:59,522 --> 01:05:08,308
Speaker 0: I would probably Kill myself to do that and to make a copy Right, like if like if you if you do that, it's like what do I care?

1159
01:05:08,328 --> 01:05:10,934
Speaker 0: Yes Scary part.

1160
01:05:11,215 --> 01:05:12,399
Speaker 0: that's like oh, hey rim.

1161
01:05:12,420 --> 01:05:13,142
Speaker 0: Let's do the podcast.

1162
01:05:13,203 --> 01:05:17,225
Speaker 0: All right, you're rim - the To me, it doesn't matter for me.

1163
01:05:17,245 --> 01:05:17,910
Speaker 0: This is the coolest

1164
01:05:18,951 --> 01:05:36,630
Speaker 1: Me about cyberization is the idea that the only difference between you that gets destroyed and perfect copy of you That is immortal The only person the only thing that could ever tell there ever was a difference at all is the solipsistic Consciousness that only ever existed inside of the old you in the first place.

1165
01:05:36,690 --> 01:05:39,846
Speaker 1: We just no other thing in the world could ever tell there was a difference.

1166
01:05:40,067 --> 01:05:47,130
Speaker 1: the thing is Sacrifice your own solipsistic existence for immortality for another you and for everyone who ever knew you.

1167
01:05:47,933 --> 01:05:49,841
Speaker 1: Isn't that the most selfless thing you could ever do?

1168
01:05:49,861 --> 01:05:54,560
Speaker 1: No, that's not the most selfless thing you'd ever do And saving babies from Hitler at the same.

1169
01:05:54,741 --> 01:06:01,726
Speaker 0: the most the most selfless thing you'd ever do is actually Completely get yourself killed, you know to save Or the universe?

1170
01:06:01,969 --> 01:06:03,356
Speaker 1: no No What about you?

1171
01:06:03,397 --> 01:06:05,207
Speaker 1: make you kill yourself at 22 your?

1172
01:06:05,970 --> 01:06:09,721
Speaker 1: Absolute peak to make a hundred copies of you that will live forever.

1173
01:06:10,062 --> 01:06:12,990
Speaker 1: and they keep coming back and getting internal memory for that.

1174
01:06:13,050 --> 01:06:13,996
Speaker 0: it's just the mechanism.

1175
01:06:14,257 --> 01:06:23,524
Speaker 1: and have the elf situation where the the Accumulated grief of all the painful things you've seen as you vow to spend all of eternity Enduring all hardship to make other people's.

1176
01:06:23,604 --> 01:06:24,370
Speaker 1: I think it's not understand.

1177
01:06:24,470 --> 01:06:28,528
Speaker 0: Why can't we just come up with a way to transfer the solipsistic you right to where Scott?

1178
01:06:28,548 --> 01:06:29,030
Speaker 1: but think about it?

1179
01:06:29,110 --> 01:06:37,261
Speaker 1: The solipsistic you is an asynchronous like basically Analogy that can't actually see itself.

1180
01:06:37,682 --> 01:06:39,910
Speaker 0: Well, it's a. it's a bunch of electrical signals, right?

1181
01:06:40,290 --> 01:06:44,504
Speaker 0: So you can have like a magnet and you could basically move all those electrons.

1182
01:06:44,584 --> 01:06:45,888
Speaker 0: It's got what electron.

1183
01:06:45,929 --> 01:06:48,910
Speaker 1: I mean Your consciousness is not any state.

1184
01:06:49,091 --> 01:06:52,370
Speaker 1: It's just like a process that kind of starts and stops.

1185
01:06:52,450 --> 01:07:03,070
Speaker 1: I mean it is very likely that any time you want anesthesia or you go to sleep even that you die Just the same as if you killed destroyed your brain and made a new copy.

1186
01:07:03,271 --> 01:07:08,417
Speaker 0: so maybe we should do it that way where you take a nap and Then they move it while you're sleeping.

1187
01:07:08,618 --> 01:07:10,486
Speaker 0: Yeah, and then you wake up and then you wouldn't even know.

1188
01:07:10,567 --> 01:07:11,330
Speaker 0: so it'd be totally cool.

1189
01:07:11,491 --> 01:07:13,300
Speaker 0: Yeah, but it doesn't matter.

1190
01:07:13,702 --> 01:07:14,788
Speaker 1: say we say we determine.

1191
01:07:14,808 --> 01:07:15,190
Speaker 1: that's true.

1192
01:07:15,290 --> 01:07:15,832
Speaker 1: Say we you know.

1193
01:07:15,872 --> 01:07:19,665
Speaker 1: say we do understand everything that cognition is and everything the consciousness is.

1194
01:07:20,087 --> 01:07:24,181
Speaker 1: and we determine that yes Every time you lose consciousness You'd might as well be dead and a new.

1195
01:07:24,201 --> 01:07:25,510
Speaker 1: you starts up with the same memories.

1196
01:07:25,851 --> 01:07:28,681
Speaker 1: Whatever the process that you know starts that waking up happens.

1197
01:07:28,922 --> 01:07:32,742
Speaker 1: Mm-hmm then yeah, you might as well cyberize right away Cuz you died anyway every fucking night.

1198
01:07:32,944 --> 01:07:37,990
Speaker 0: Yeah, or What if instead you use make sure you put the new one in my bed?

1199
01:07:38,070 --> 01:07:39,130
Speaker 0: So I wake up in the right place?

1200
01:07:39,170 --> 01:07:42,529
Speaker 1: This raises a really really interesting question that we're not gonna have time to talk about.

1201
01:07:43,333 --> 01:07:47,610
Speaker 1: So what if we you know that that military drug that lets you basically stay up for like a week straight.

1202
01:07:48,220 --> 01:07:51,950
Speaker 1: All right What if we come up with a safe way to do that forever?

1203
01:07:52,430 --> 01:07:58,129
Speaker 1: So you basically your threat you could have a. your personal physical threat of consciousness just never stops ever again.

1204
01:07:58,952 --> 01:08:05,659
Speaker 1: Okay, as opposed to currently now where it starts and stops all the time But the only stateful part is the memory set.

1205
01:08:06,020 --> 01:08:09,169
Speaker 1: what if the consciousness itself were able to be made stateful?

1206
01:08:09,832 --> 01:08:14,110
Speaker 1: Currently that drives us insane, but that might just be due to physical problems that we don't understand yet.

1207
01:08:14,471 --> 01:08:20,430
Speaker 0: So we overcome all of those problems, right and then I could just be up all the time and never sleep.

1208
01:08:21,350 --> 01:08:21,792
Speaker 0: Why not?

1209
01:08:21,872 --> 01:08:22,274
Speaker 1: Why would you?

1210
01:08:22,295 --> 01:08:23,741
Speaker 0: why would you want to sleep if you didn't have?

1211
01:08:23,801 --> 01:08:29,196
Speaker 0: I mean you could maybe like relax your muscles Cuz they would still hurt I guess Depending right.

1212
01:08:29,236 --> 01:08:32,609
Speaker 0: but and you could like take rest lay down and rest or whatever.

1213
01:08:33,390 --> 01:08:35,921
Speaker 0: But you wouldn't be tired and need to sleep.

1214
01:08:36,103 --> 01:08:36,604
Speaker 0: Why would you?

1215
01:08:36,625 --> 01:08:37,790
Speaker 0: you could just keep doing stuff?

1216
01:08:37,870 --> 01:08:44,189
Speaker 0: You can see already all night and work all day Raises the right now if you try to do that with this horrible side effects.

1217
01:08:44,611 --> 01:08:46,237
Speaker 1: So this raises the fundamental.

1218
01:08:46,377 --> 01:08:49,428
Speaker 1: I guess all debates about cyberization are really just practical debates.

1219
01:08:49,468 --> 01:08:50,069
Speaker 1: It's no different.

1220
01:08:50,130 --> 01:08:52,990
Speaker 0: What is the risk/reward ratio considering these consequences?

1221
01:08:53,170 --> 01:08:56,448
Speaker 1: It's no different than our debate about cell phones in the first half of this show.

1222
01:08:57,032 --> 01:08:57,997
Speaker 1: Do I do this or this?

1223
01:08:58,037 --> 01:09:03,029
Speaker 1: Well, this has this benefit this risk, you know, Apple whatever Do you want the Apple body or the HTC body?

1224
01:09:04,470 --> 01:09:04,993
Speaker 0: It's all.

1225
01:09:05,013 --> 01:09:05,194
Speaker 0: yeah.

1226
01:09:05,234 --> 01:09:08,026
Speaker 0: think about if you hey, all right, so if you had to get a body today, right?

1227
01:09:09,069 --> 01:09:09,894
Speaker 0: Which one would you get?

1228
01:09:10,014 --> 01:09:13,290
Speaker 0: Oh my fucking god if your body had to be a smartphone.

1229
01:09:13,371 --> 01:09:14,859
Speaker 1: So the Apple one's totally pretty.

1230
01:09:14,941 --> 01:09:16,328
Speaker 1: but Steve Jobs watches you pee.

1231
01:09:17,334 --> 01:09:18,207
Speaker 0: It doesn't watch you pee.

1232
01:09:19,551 --> 01:09:22,541
Speaker 0: He just tells you which you know, he doesn't allow 20 inch penises.

1233
01:09:22,581 --> 01:09:24,607
Speaker 0: He only allows the 10 and 15 inch penises.

1234
01:09:24,707 --> 01:09:28,309
Speaker 0: Yeah And they the 15 inch one cost five bucks more.

1235
01:09:29,551 --> 01:09:31,596
Speaker 1: But the blue one, I don't know.

1236
01:09:31,676 --> 01:09:33,523
Speaker 0: I'd pay five bucks or 15 inch pain.

1237
01:09:34,185 --> 01:09:37,002
Speaker 1: so but here's the thing You know, this raises really.

1238
01:09:37,042 --> 01:09:45,850
Speaker 1: the final Cyberization is no different than any other debate Except it raises a lot of vitriol and a lot of people and people try to treat it as a different debate.

1239
01:09:46,152 --> 01:09:47,438
Speaker 1: From which cell phone do I do?

1240
01:09:47,738 --> 01:09:48,564
Speaker 1: and I don't think it is.

1241
01:09:48,584 --> 01:09:59,390
Speaker 1: I think the only reason Anyway treated differently from any other debate in the world Is that human people who aren't able to deal with transhumanism freak the fuck out.

1242
01:09:59,812 --> 01:10:06,223
Speaker 0: those are people who have some sort of spiritual or you know Idea of souls or something like that is.

1243
01:10:06,263 --> 01:10:12,967
Speaker 0: the only reason you would have a problem with it Is if you believed in anything like that that somehow Consciousness was special or non or not.

1244
01:10:13,027 --> 01:10:27,490
Speaker 1: just add We're somehow real if we decide to make it special or we find some reason that like we find something, you know There's this actual there's this magnetic field and that's your soul.

1245
01:10:27,550 --> 01:10:32,270
Speaker 0: Well, we can use a magnet to move the magnetic field to a different brain and you can maintain your consciousness.

1246
01:10:33,775 --> 01:10:48,850
Speaker 1: the only people for whom this is really controversial on some deep level are the people who Decided that being human, you know the same humans effectively that existed for you How different from being a raccoon and that why don't we just make something better?

1247
01:10:49,211 --> 01:10:59,070
Speaker 1: I think the future of human evolution is that our Information world the world we interact with information wise is already evolving faster than physical things could ever keep up.

1248
01:10:59,351 --> 01:11:01,518
Speaker 1: We will never evolve faster than our own information.

1249
01:11:02,099 --> 01:11:05,450
Speaker 1: So why don't we become the information itself?

1250
01:11:05,610 --> 01:11:11,169
Speaker 1: Why doesn't that analogy one level higher than us be the legacy of humankind and humankind will eventually disappear.

1251
01:11:12,313 --> 01:11:14,240
Speaker 0: Yeah, that's that's the last question.

1252
01:11:14,381 --> 01:11:16,247
Speaker 1: Yeah, and that's what I'm going for.

1253
01:11:16,388 --> 01:11:16,890
Speaker 1: I'll give up.

1254
01:11:17,251 --> 01:11:23,150
Speaker 1: I am so totally willing to give up my humanity for pretty much an iPhone at this point.

1255
01:11:24,192 --> 01:11:26,505
Speaker 0: But what I was but you're still but you're just arguing.

1256
01:11:26,525 --> 01:11:29,904
Speaker 0: you don't want the iPhone But you'll just buy a pre with money, but you owe.

1257
01:11:29,924 --> 01:11:30,808
Speaker 0: you won't buy an iPhone.

1258
01:11:31,210 --> 01:11:33,258
Speaker 0: You can buy an iPhone for money, but you're only good.

1259
01:11:33,278 --> 01:11:36,249
Speaker 0: You only give up your humanity, but I can't be an iPhone.

1260
01:11:38,138 --> 01:11:38,547
Speaker 0: Yeah, you can.

1261
01:11:40,355 --> 01:11:41,847
Speaker 1: I think we've exhausted this for now.

1262
01:11:48,273 --> 01:11:50,810
Speaker 1: This has been geek nights with rim and Scott special.

1263
01:11:50,870 --> 01:11:55,610
Speaker 1: Thanks to DJ pretzel for the opening music cat leave for web design and Brando K for the logos.

1264
01:11:55,911 --> 01:11:57,898
Speaker 0: Be sure to visit our website at front row.

1265
01:11:57,938 --> 01:12:00,930
Speaker 0: crew calm for show notes discussion news and more.

1266
01:12:01,210 --> 01:12:08,610
Speaker 1: Remember geek nights is not one but four different shows sci-tech Mondays gaming Tuesdays and make comic Wednesdays and indiscriminate Thursdays.

1267
01:12:08,971 --> 01:12:12,145
Speaker 0: Geek nights is distributed under a Creative Commons attribution 3.0 license.

1268
01:12:13,391 --> 01:12:18,146
Speaker 0: Geek nights is recorded live with no studio and no audience, but unlike those other late shows.

1269
01:12:18,286 --> 01:12:19,570
Speaker 0: It's actually recorded at night.

