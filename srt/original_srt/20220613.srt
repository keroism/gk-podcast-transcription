1
00:00:08,700 --> 00:00:09,988
Speaker 1: It's Monday, June 13th, 2022.

2
00:00:09,988 --> 00:00:10,209
Speaker 1: I'm Rym.

3
00:00:13,526 --> 00:00:14,060
Speaker 0: I'm Scott.

4
00:00:14,340 --> 00:00:15,605
Speaker 1: And this is Geek Nights!

5
00:00:15,645 --> 00:00:21,458
Speaker 1: Tonight, AI and machine learning are a hot topic for a bunch of reasons, so we're gonna talk a little bit about that.

6
00:00:23,044 --> 00:00:24,480
Speaker 1: And know your chatbot is not sentient.

7
00:00:26,140 --> 00:00:27,178
Speaker 0: Let's do this.

8
00:00:29,262 --> 00:00:40,860
Speaker 1: This was one of the longer hiatus we've ever had in Geek Nights, and it was only because the NHL couldn't do as a solid and scheduled the Stanley Cup playoffs opposite Geek Nights.

9
00:00:41,040 --> 00:00:41,799
Speaker 0: They had to schedule them.

10
00:00:42,061 --> 00:00:42,503
Speaker 0: Hold on.

11
00:00:42,563 --> 00:00:46,140
Speaker 0: So first of all, they scheduled it the best they've ever scheduled it.

12
00:00:46,264 --> 00:00:46,598
Speaker 0: That is true.

13
00:00:47,360 --> 00:00:48,242
Speaker 0: Every other night, right?

14
00:00:48,543 --> 00:00:54,780
Speaker 0: The problem was that A, we talked about this before.

15
00:00:54,880 --> 00:00:58,260
Speaker 0: We had tickets to every home game, which we've never had in our lives.

16
00:00:58,901 --> 00:01:02,580
Speaker 0: B, we kept fucking winning, which means we kept playing.

17
00:01:02,701 --> 00:01:07,140
Speaker 0: And C, two series go to seven and another one go to six.

18
00:01:07,261 --> 00:01:08,499
Speaker 0: So it's like we didn't have a break.

19
00:01:08,660 --> 00:01:12,480
Speaker 0: If they would have swept someone, we could have had a Geek Nights time in there.

20
00:01:13,020 --> 00:01:18,599
Speaker 1: I mean, what I expected was the Rangers to crush the pens, get crushed by Carolina, and that's it.

21
00:01:19,021 --> 00:01:22,839
Speaker 1: And then instead, I got game seven, game seven, and then a 4-2 series.

22
00:01:24,261 --> 00:01:24,462
Speaker 0: Right.

23
00:01:24,502 --> 00:01:27,760
Speaker 0: And then on top of that, other life shit was going on.

24
00:01:27,900 --> 00:01:31,240
Speaker 0: I think I discussed this on all the other platforms.

25
00:01:31,421 --> 00:01:35,300
Speaker 0: So basically, I unexpectedly got a new job.

26
00:01:35,500 --> 00:01:36,859
Speaker 0: I was not looking for a new job.

27
00:01:37,041 --> 00:01:38,660
Speaker 0: I was very happy with the job I had.

28
00:01:38,980 --> 00:01:44,240
Speaker 0: If you want to work at the place I was working, there's openings and it's a good place to work.

29
00:01:44,640 --> 00:01:51,600
Speaker 0: If the new job is a trap, which it very well could be, I would go back there, no question.

30
00:01:53,560 --> 00:02:04,860
Speaker 0: But yeah, so, you know, recruiters, you know, they come to me and it's like, you know, I'm very sort of, you know, professional, but also somewhat aggressive with them.

31
00:02:05,040 --> 00:02:05,281
Speaker 0: Right.

32
00:02:05,382 --> 00:02:11,760
Speaker 0: In terms of like, you know, I'm like, look, you know, if your job is immoral, then I'm just I'm like, no, I won't work there.

33
00:02:11,960 --> 00:02:12,121
Speaker 0: Bye.

34
00:02:12,341 --> 00:02:12,582
Speaker 0: Right.

35
00:02:13,124 --> 00:02:18,588
Speaker 0: If the job is morally acceptable, I'm like, all right, well, is it better be better than the job I have?

36
00:02:18,628 --> 00:02:20,120
Speaker 0: The job I got is real good.

37
00:02:20,160 --> 00:02:21,259
Speaker 1: And there's two things I'll take.

38
00:02:21,460 --> 00:02:24,132
Speaker 1: Like, I would take better because it's engaging work.

39
00:02:24,313 --> 00:02:25,720
Speaker 1: better because it's better for the world.

40
00:02:25,921 --> 00:02:29,600
Speaker 1: Like it's activism or it pays so much more that I'll just take the money.

41
00:02:30,241 --> 00:02:30,341
Speaker 0: Right.

42
00:02:30,361 --> 00:02:35,560
Speaker 0: Well, I mean, some months ago, someone did make me an offer for more engaging, more.

43
00:02:35,580 --> 00:02:38,900
Speaker 0: at least the product was, I guess, somewhat more interesting than music publishing.

44
00:02:39,280 --> 00:02:40,349
Speaker 0: Right.

45
00:02:40,430 --> 00:02:41,599
Speaker 0: But, you know, it was it was cool.

46
00:02:42,381 --> 00:02:47,600
Speaker 0: And, you know, but when it came time to actually make me an offer, it was similar monies.

47
00:02:48,242 --> 00:02:52,280
Speaker 0: And I was like, you know, this is not better than the job I have.

48
00:02:52,461 --> 00:02:57,280
Speaker 0: And I demanded either 20 percent more monies or four days a week.

49
00:02:57,401 --> 00:02:57,602
Speaker 0: Right.

50
00:02:57,622 --> 00:03:00,180
Speaker 0: Which would be 20 percent less time, I said, one or the other.

51
00:03:00,340 --> 00:03:02,280
Speaker 0: And they were like, too rich for our blood.

52
00:03:02,682 --> 00:03:02,967
Speaker 0: Thanks.

53
00:03:03,008 --> 00:03:03,558
Speaker 0: Have a nice day.

54
00:03:03,660 --> 00:03:04,367
Speaker 0: And I'm like, all right.

55
00:03:04,731 --> 00:03:05,580
Speaker 0: I still got a great job.

56
00:03:05,640 --> 00:03:06,062
Speaker 0: I don't care.

57
00:03:06,123 --> 00:03:06,304
Speaker 0: Yeah.

58
00:03:06,344 --> 00:03:10,045
Speaker 1: And also, I respect a recruiter who's up front and is like, oh, OK, never mind.

59
00:03:10,186 --> 00:03:11,900
Speaker 0: Like, well, that wasn't the recruiter.

60
00:03:11,980 --> 00:03:12,764
Speaker 0: That was actually.

61
00:03:12,865 --> 00:03:15,759
Speaker 0: I went through a whole interview process at the end.

62
00:03:16,622 --> 00:03:18,920
Speaker 0: And the recruiters always inflate.

63
00:03:19,021 --> 00:03:27,405
Speaker 0: They like to tell you it's going to be a lot of money because then they convince you to go through the interview process and then hoping that you'll feel bad and be bad at negotiating.

64
00:03:27,727 --> 00:03:29,880
Speaker 1: And I've always taken the exact opposite.

65
00:03:30,040 --> 00:03:35,560
Speaker 1: My strategy in interviewing my entire life is even if the salary is low, if I like the job, I would go for it anyway.

66
00:03:36,061 --> 00:03:37,860
Speaker 1: Make that like make sure I crush the interview.

67
00:03:37,980 --> 00:03:38,643
Speaker 1: So they love me.

68
00:03:38,663 --> 00:03:41,696
Speaker 0: And I mean, I try to walk away a lot.

69
00:03:43,462 --> 00:03:45,799
Speaker 0: I definitely crush the interviews and they love me.

70
00:03:46,360 --> 00:03:47,223
Speaker 0: Right.

71
00:03:47,624 --> 00:03:57,360
Speaker 0: It was just that, you know, while I would have taken that job in a heart attack, if I had no job, it wasn't significantly better than the job I had.

72
00:03:57,600 --> 00:03:58,163
Speaker 0: So why change?

73
00:03:58,203 --> 00:04:01,720
Speaker 1: Well, that's rule number two of my life is no lateral moves ever.

74
00:04:02,941 --> 00:04:03,042
Speaker 0: Right.

75
00:04:03,062 --> 00:04:07,382
Speaker 0: So anyway, so another place comes along where I'm going to be working starting next month.

76
00:04:08,286 --> 00:04:12,867
Speaker 0: And, you know, it's like it's got positives, it's got negatives, but it's not evil.

77
00:04:13,130 --> 00:04:13,879
Speaker 0: The people are nice.

78
00:04:14,303 --> 00:04:16,480
Speaker 0: You know, I'm going through the interviews there.

79
00:04:17,122 --> 00:04:17,930
Speaker 0: Same deal, right?

80
00:04:17,990 --> 00:04:18,959
Speaker 0: I crush the interviews.

81
00:04:19,141 --> 00:04:19,868
Speaker 0: They're in love with me.

82
00:04:19,908 --> 00:04:21,240
Speaker 0: They want to hire me so badly.

83
00:04:21,742 --> 00:04:22,063
Speaker 0: Right.

84
00:04:22,585 --> 00:04:25,334
Speaker 0: You know, and I'm like, you know, I get to.

85
00:04:25,374 --> 00:04:27,608
Speaker 0: finally I get to the negotiating part at the end.

86
00:04:28,156 --> 00:04:28,400
Speaker 0: Right.

87
00:04:28,861 --> 00:04:32,099
Speaker 0: And I'm like, you know, I'd really like to work four days a week.

88
00:04:32,541 --> 00:04:33,765
Speaker 0: Right.

89
00:04:33,785 --> 00:04:37,970
Speaker 0: And I'm like, I know you're going to say no, but that's where I got to start my ask.

90
00:04:38,215 --> 00:04:38,460
Speaker 0: Right.

91
00:04:40,161 --> 00:04:48,500
Speaker 0: And somehow I don't know how, but the offer ended up being both four days a week and 20 percent more.

92
00:04:48,641 --> 00:04:56,420
Speaker 1: So all I got to say is if this works out, like if they're hiring and they need a product manager, just, you know, we'll see what happens.

93
00:04:56,541 --> 00:04:56,723
Speaker 0: Right.

94
00:04:56,905 --> 00:04:58,380
Speaker 0: You know, this could be a trap.

95
00:04:58,500 --> 00:04:59,700
Speaker 0: This could be too good to be true.

96
00:04:59,820 --> 00:05:01,720
Speaker 0: This could be who knows what's going to happen.

97
00:05:01,920 --> 00:05:03,617
Speaker 0: But I certainly say no.

98
00:05:04,100 --> 00:05:04,727
Speaker 0: It's like I didn't.

99
00:05:04,909 --> 00:05:05,799
Speaker 0: I didn't want to go.

100
00:05:06,141 --> 00:05:09,560
Speaker 0: I was like I didn't want to leave the job I had, but I had kind of no choice.

101
00:05:09,620 --> 00:05:10,310
Speaker 1: You know what it's like?

102
00:05:10,350 --> 00:05:11,060
Speaker 1: It's like a bidding.

103
00:05:11,200 --> 00:05:17,840
Speaker 1: It's like in a bidding game or even like a train game or something where I put for some sort of like ridiculous lowball offer like in sieve.

104
00:05:17,980 --> 00:05:21,720
Speaker 1: I'm like, hey, Scott, you gave me three luxury resources for like one nighter.

105
00:05:22,582 --> 00:05:24,179
Speaker 1: And then the other side says, OK.

106
00:05:24,442 --> 00:05:25,174
Speaker 1: And I'm like, wait, what?

107
00:05:25,357 --> 00:05:25,520
Speaker 1: What?

108
00:05:25,880 --> 00:05:26,939
Speaker 1: I didn't expect you to say yes.

109
00:05:27,660 --> 00:05:28,547
Speaker 1: Nobody says yes to that.

110
00:05:28,588 --> 00:05:29,515
Speaker 1: What's what's going on?

111
00:05:30,822 --> 00:05:31,507
Speaker 0: I needed that night.

112
00:05:31,548 --> 00:05:33,300
Speaker 0: I was the one nighter I needed to make a gun.

113
00:05:33,440 --> 00:05:34,142
Speaker 0: Boom.

114
00:05:35,627 --> 00:05:39,960
Speaker 0: Anyway, so, yeah, that's that took up some time and distracted from geek nights.

115
00:05:40,040 --> 00:05:44,100
Speaker 0: But now now we're in we're in the opposite situation.

116
00:05:44,281 --> 00:05:45,107
Speaker 0: Right.

117
00:05:45,530 --> 00:05:46,979
Speaker 0: Whereas I don't have to work for a whole month.

118
00:05:47,320 --> 00:05:48,002
Speaker 0: It's summertime.

119
00:05:48,023 --> 00:05:53,743
Speaker 0: I worked on the website most of the day to day doing lots of stuff, the geek nights website.

120
00:05:54,205 --> 00:05:55,390
Speaker 0: And then what?

121
00:05:55,490 --> 00:05:58,402
Speaker 0: even when I start working next month, it won't be on Friday.

122
00:05:58,422 --> 00:06:06,200
Speaker 0: I know there's no geek nights on Friday, but that still is going to give me time eventually to do some kind of content or something.

123
00:06:07,161 --> 00:06:11,798
Speaker 0: At least some of those Fridays once in a while will be spent doing something.

124
00:06:11,818 --> 00:06:14,220
Speaker 0: geek nights Internet content production related.

125
00:06:14,741 --> 00:06:16,028
Speaker 0: Right.

126
00:06:16,049 --> 00:06:17,940
Speaker 0: So that's all I'm willing to say.

127
00:06:18,500 --> 00:06:20,138
Speaker 0: I'm not willing to talk any more than that.

128
00:06:22,422 --> 00:06:25,916
Speaker 1: So yes, the hockey will keep watching because honestly, this is going to be.

129
00:06:26,298 --> 00:06:27,511
Speaker 0: it's only a few games left.

130
00:06:27,796 --> 00:06:27,999
Speaker 1: Yep.

131
00:06:29,681 --> 00:06:33,640
Speaker 1: Also, I am very confident Colorado is going to win.

132
00:06:34,221 --> 00:06:37,580
Speaker 0: But I said that early in the regular season.

133
00:06:37,860 --> 00:06:37,940
Speaker 1: Yeah.

134
00:06:37,960 --> 00:06:43,969
Speaker 1: And when we say the whole thing, whoever came out of the Eastern Conference is just going to be sacrificed before the Nidhogg.

135
00:06:44,010 --> 00:06:45,840
Speaker 1: that is the Colorado avalanche.

136
00:06:46,401 --> 00:06:47,340
Speaker 0: That's what's going to happen.

137
00:06:47,840 --> 00:06:52,524
Speaker 1: We were just fighting over who gets to have the honor of being crushed by them in the finals.

138
00:06:53,332 --> 00:06:54,080
Speaker 0: Well, we'll see.

139
00:06:54,600 --> 00:07:00,480
Speaker 1: But the one thing so I Emily and I ended up being at the last home game because that's like how our tickets broke down.

140
00:07:00,600 --> 00:07:00,901
Speaker 0: We were at.

141
00:07:01,162 --> 00:07:05,762
Speaker 0: we were at the last game at Madison Square Garden, but the game was being played in Florida.

142
00:07:05,983 --> 00:07:10,902
Speaker 1: But there is a technology angle here because something happened that I this happens.

143
00:07:11,384 --> 00:07:20,880
Speaker 1: It always happens in the playoffs, but maybe because I haven't paid attention to the playoffs or gone to games in a long time, but also it feels like it was more than like the last time I went to playoffs games.

144
00:07:21,401 --> 00:07:31,540
Speaker 1: The production value of the technology, the lighting and the sound and the stuff happening in the arena leveled up every round of the playoffs.

145
00:07:32,082 --> 00:07:34,579
Speaker 1: And every time it leveled up more than I expected it to.

146
00:07:34,840 --> 00:07:39,240
Speaker 1: So in the last round, the last game we went to, they had smoke, they had lasers.

147
00:07:39,341 --> 00:07:40,431
Speaker 1: So not just I got the lasers.

148
00:07:41,821 --> 00:07:47,360
Speaker 1: They had lasers that are if they hit a person in the eye would be fucking dangerous.

149
00:07:48,122 --> 00:07:48,713
Speaker 1: They were true.

150
00:07:48,795 --> 00:07:49,020
Speaker 1: Yes.

151
00:07:49,625 --> 00:07:50,400
Speaker 1: So bright.

152
00:07:51,040 --> 00:07:55,199
Speaker 0: This is a laser that was like going across MSG from one end to the other.

153
00:07:55,600 --> 00:07:55,700
Speaker 1: Right.

154
00:07:55,720 --> 00:08:04,340
Speaker 1: And culminating to like a spot, maybe an inch, like inch and a half in diameter that was setting the building on fire.

155
00:08:04,481 --> 00:08:07,400
Speaker 0: But it's like they were like that hit you in the eye.

156
00:08:07,541 --> 00:08:08,940
Speaker 1: I think you would be insta blinded.

157
00:08:09,323 --> 00:08:11,060
Speaker 1: And it looks like they were hard.

158
00:08:11,340 --> 00:08:20,882
Speaker 0: And also they were bright enough to be visible just in the ambient air slightly, which well, they also combine them with the smoke machine that come out from under the scoreboard.

159
00:08:20,922 --> 00:08:26,479
Speaker 1: But some of them were occasionally touching briefly the bottom of the American flag that's hanging up there.

160
00:08:27,062 --> 00:08:29,337
Speaker 1: And it was like a sun every time it touched it.

161
00:08:30,320 --> 00:08:41,979
Speaker 1: So what's interesting is if you look at how they work, how they work this out, they were all aimed and I'm pretty there had better have been hardware controls, ensuring that they couldn't aim anywhere else.

162
00:08:42,421 --> 00:08:49,200
Speaker 1: They were flying around, but they were only aiming at the boundaries between the levels in the arena, like the little wall.

163
00:08:49,560 --> 00:08:52,900
Speaker 0: There was some that were aimed at the bottom of the boards on the ice.

164
00:08:53,061 --> 00:09:00,999
Speaker 0: And there were some that were aimed on like, you know, the flat where the score, the horizontal scoreboards are in between the one hundred two hundred level.

165
00:09:01,644 --> 00:09:03,320
Speaker 0: But yeah, they're basically not hitting anywhere.

166
00:09:03,421 --> 00:09:07,000
Speaker 0: There were seats and anywhere there could be a human body.

167
00:09:08,567 --> 00:09:08,729
Speaker 1: Right.

168
00:09:08,749 --> 00:09:10,119
Speaker 0: We were flying or something.

169
00:09:10,381 --> 00:09:10,642
Speaker 1: Yeah.

170
00:09:10,702 --> 00:09:29,740
Speaker 1: Which is just a fascinating use of technology, because if there weren't hardware lockouts, if they were relying solely on the programing of the controllers for those laser lights as someone who has done formal risk analysis professionally for more than a decade, that I cannot imagine what the risk assessment was of that situation.

171
00:09:29,940 --> 00:09:32,899
Speaker 0: I mean, they could have been doing it with software, just the software really good.

172
00:09:33,283 --> 00:09:35,159
Speaker 0: Yeah, I just know how that shit works.

173
00:09:35,982 --> 00:09:41,060
Speaker 1: I, I deeply distrust software interlocks on dangerous hardware.

174
00:09:41,900 --> 00:09:45,900
Speaker 0: Yeah, but anyway, they had a lot of cool special effects going on.

175
00:09:47,481 --> 00:09:51,980
Speaker 1: So in some actual tech news, let's talk about Web Push for Safari.

176
00:09:52,820 --> 00:09:56,560
Speaker 0: Well, so Apple's WWDC happened, right?

177
00:09:56,720 --> 00:10:04,240
Speaker 0: This is where at the time of year where they have a big developer conference for Apple developers and they announce all their new Apple shit.

178
00:10:04,880 --> 00:10:10,480
Speaker 0: And usually it's all like what's going to be updated in iOS, what's going to be updated in Mac OS, all that kind of stuff.

179
00:10:10,740 --> 00:10:11,442
Speaker 0: Right.

180
00:10:11,462 --> 00:10:17,565
Speaker 0: And the big update they announced was this sort of something stage center stage something.

181
00:10:17,585 --> 00:10:19,300
Speaker 0: I forget what it's called.

182
00:10:19,460 --> 00:10:22,540
Speaker 0: Apple stage something main stage.

183
00:10:23,702 --> 00:10:25,800
Speaker 0: No, not that stage manager.

184
00:10:25,981 --> 00:10:26,402
Speaker 0: There it is.

185
00:10:26,482 --> 00:10:40,378
Speaker 0: Apple stage manager, which is basically, you know, we talk about a lot of how Apple will take something that someone else has already invented and sort of just polish it and make a new UI for it and act like they invented it.

186
00:10:41,480 --> 00:10:45,900
Speaker 0: Like when they came out with spaces or virtual desktops.

187
00:10:46,200 --> 00:10:46,301
Speaker 0: Right.

188
00:10:46,743 --> 00:10:51,680
Speaker 0: And it's like virtual desktops have been on Unix and, you know, X11 Windows systems for decades.

189
00:10:52,002 --> 00:10:52,203
Speaker 0: Yeah.

190
00:10:52,223 --> 00:11:01,000
Speaker 1: And they acted like they invented it like it was even funnier because by the time that appeared was around the time when I had two giant monitors and didn't really use virtual desktops anymore.

191
00:11:01,861 --> 00:11:08,920
Speaker 0: But anyway, but they stage manager, which they just announced, is basically like it's effectively virtual desktops.

192
00:11:09,000 --> 00:11:09,241
Speaker 0: Right.

193
00:11:09,522 --> 00:11:14,380
Speaker 0: You group your windows by task and then you can sort of, you know, bring out.

194
00:11:14,640 --> 00:11:16,240
Speaker 0: That's how I use virtual desktops anyway.

195
00:11:16,381 --> 00:11:16,542
Speaker 0: Right.

196
00:11:16,603 --> 00:11:18,700
Speaker 0: It's like I'll have a virtual desktop for each task.

197
00:11:19,282 --> 00:11:24,159
Speaker 0: And then you just, you know, you swap out all your windows for a whole different set of windows to work on something else.

198
00:11:24,260 --> 00:11:24,442
Speaker 0: Right.

199
00:11:24,462 --> 00:11:29,600
Speaker 0: So it's like on the other virtual desktop that I'm not looking at right now, there's all the stuff to work on the geek nights website.

200
00:11:29,742 --> 00:11:29,885
Speaker 0: Right.

201
00:11:29,905 --> 00:11:30,598
Speaker 0: But all that's hidden.

202
00:11:31,022 --> 00:11:34,440
Speaker 0: Because now I'm working on my desktop where I can do a geek nights episode.

203
00:11:35,342 --> 00:11:35,503
Speaker 0: Right.

204
00:11:35,523 --> 00:11:42,079
Speaker 0: So the stage manager is a new UI to effectively, you know, group applications by task and switch between tasks.

205
00:11:42,864 --> 00:11:44,300
Speaker 0: And it's actually really nice looking.

206
00:11:45,102 --> 00:11:45,885
Speaker 0: I kind of like it.

207
00:11:46,025 --> 00:11:49,880
Speaker 0: I wish my computer did that, but not enough to go get a Mac.

208
00:11:51,541 --> 00:12:01,900
Speaker 0: But they are bringing it to iPad, which is nice, but they're not bringing it to iPads that don't have M1 processors, which I think is actually a big problem.

209
00:12:02,101 --> 00:12:07,380
Speaker 0: And that, you know, Apple has always been known for the one thing they do.

210
00:12:07,520 --> 00:12:14,280
Speaker 0: I think the best thing Apple does is that they support really old hardware with new OS updates for a long time.

211
00:12:14,720 --> 00:12:14,921
Speaker 0: Yeah.

212
00:12:15,322 --> 00:12:25,618
Speaker 0: My iPhone 7 was which, you know, I guess I could have waited another year or two before updating it, you know, but I think I guess it was seven years before of lifetime.

213
00:12:26,141 --> 00:12:35,141
Speaker 0: If you bought an iPhone 7, the day it came out, you could have used it for seven years before it stopped getting iOS updates and you could have even kept using it longer than that.

214
00:12:35,161 --> 00:12:41,800
Speaker 0: You just eventually might run into situations where, you know, updates to iOS apps don't work because your iOS is too old.

215
00:12:41,940 --> 00:12:42,582
Speaker 0: Right now.

216
00:12:43,384 --> 00:12:49,720
Speaker 0: So, yeah, I only kept my iPhone 7 for five years and my Apple Watch 3.

217
00:12:49,720 --> 00:12:50,227
Speaker 0: I bought that.

218
00:12:50,268 --> 00:12:51,079
Speaker 0: I'm wearing it right now.

219
00:12:51,180 --> 00:12:55,080
Speaker 0: It's still good, but it looks like later this year it's not going to be good anymore.

220
00:12:55,562 --> 00:13:01,059
Speaker 0: They're going to have an iOS update that, you know, a watch OS update that finally puts the third watch.

221
00:13:01,900 --> 00:13:02,202
Speaker 0: Right.

222
00:13:02,242 --> 00:13:05,838
Speaker 0: Meanwhile, they're already on like Watch 8 or something or I forget what number they're on.

223
00:13:06,281 --> 00:13:09,620
Speaker 1: Yeah, I have the most recent one that's actually out and it's fine.

224
00:13:10,560 --> 00:13:12,620
Speaker 0: Yeah, I mean, it'll last you many years.

225
00:13:12,761 --> 00:13:12,921
Speaker 0: Right.

226
00:13:12,942 --> 00:13:16,882
Speaker 0: Whereas Android devices, they get maybe a handful of years of updates and then that's it.

227
00:13:17,063 --> 00:13:20,660
Speaker 1: In two years, I went through two and a half Android watches.

228
00:13:21,842 --> 00:13:27,879
Speaker 0: Yeah, I mean, I used to update my Apple phone, my iPhone every two years because the contract would be every two years.

229
00:13:28,562 --> 00:13:36,400
Speaker 0: But and because the hardware updates were so significant, like what's iPhone 3G compared to iPhone 4, compared to iPhone 5, compared to six, compared to seven.

230
00:13:36,884 --> 00:13:37,066
Speaker 0: Right.

231
00:13:37,106 --> 00:13:38,360
Speaker 0: It's like those are big leaps.

232
00:13:39,022 --> 00:13:42,460
Speaker 0: But then after seven, like the leaps weren't there anymore.

233
00:13:42,500 --> 00:13:48,574
Speaker 0: You know, I went from seven to twelve mini, whatever the first mini one, whatever number that was.

234
00:13:48,716 --> 00:13:48,920
Speaker 0: Right.

235
00:13:49,442 --> 00:13:50,124
Speaker 0: So anyway.

236
00:13:51,208 --> 00:13:52,312
Speaker 0: So, yeah, that's the.

237
00:13:52,833 --> 00:13:54,800
Speaker 0: you know, they got their iOS updates and whatnot.

238
00:13:54,880 --> 00:13:55,041
Speaker 0: Right.

239
00:13:55,081 --> 00:14:02,380
Speaker 0: And the big one feature that they announced that really intrigued me was the sort of live notifications.

240
00:14:02,742 --> 00:14:03,045
Speaker 0: Right.

241
00:14:03,630 --> 00:14:04,597
Speaker 0: I forget what they called them.

242
00:14:06,021 --> 00:14:12,814
Speaker 0: But, you know, there are certain situations where let's say you order something like food to be delivered.

243
00:14:13,037 --> 00:14:13,240
Speaker 0: Right.

244
00:14:14,041 --> 00:14:18,562
Speaker 0: Right now, if you turn on all the notifications for your food delivery app of choice.

245
00:14:18,582 --> 00:14:19,265
Speaker 0: Right.

246
00:14:19,386 --> 00:14:19,727
Speaker 0: What they'll?

247
00:14:19,768 --> 00:14:19,989
Speaker 0: they'll?

248
00:14:20,190 --> 00:14:22,603
Speaker 0: they'll message you and be like, we got your order.

249
00:14:22,846 --> 00:14:24,160
Speaker 0: Then they'll give you another notification.

250
00:14:24,541 --> 00:14:26,800
Speaker 0: The restaurant started working on another notification.

251
00:14:27,122 --> 00:14:28,620
Speaker 0: They finished working on another one.

252
00:14:28,700 --> 00:14:29,550
Speaker 0: The delivery is coming.

253
00:14:29,895 --> 00:14:30,340
Speaker 0: Another one.

254
00:14:30,460 --> 00:14:31,368
Speaker 0: The delivery is close.

255
00:14:31,772 --> 00:14:32,619
Speaker 0: The delivery is here.

256
00:14:33,020 --> 00:14:35,900
Speaker 0: You like a pile of notifications for this one thing.

257
00:14:36,341 --> 00:14:36,623
Speaker 0: Right.

258
00:14:36,925 --> 00:14:39,100
Speaker 0: It's like your phone is going ding, ding, ding, ding.

259
00:14:39,241 --> 00:14:39,422
Speaker 0: Right.

260
00:14:39,906 --> 00:14:41,840
Speaker 0: And that's not a great user experience.

261
00:14:41,840 --> 00:14:42,706
Speaker 0: It's kind of a mess.

262
00:14:42,746 --> 00:14:44,720
Speaker 0: And you have to clear out a pile of notifications.

263
00:14:45,481 --> 00:14:45,661
Speaker 0: Right.

264
00:14:46,163 --> 00:14:50,176
Speaker 0: Really, that whole process should be one notification.

265
00:14:50,537 --> 00:14:56,780
Speaker 0: that sort of the notification itself live updates to tell you the current status when you look at it.

266
00:14:56,840 --> 00:14:58,620
Speaker 0: And that's they added that it's basically.

267
00:14:59,161 --> 00:15:17,100
Speaker 0: So when you if you were to now order food on a new, you know, once the new iOS comes out and the apps update themselves, you would get a note, you would order some food, a notification would come up and that notification would just stay there and always be displaying the current status of your food order.

268
00:15:17,740 --> 00:15:21,782
Speaker 0: And when it was done, that thing would go away and it would sort of be on your home screen.

269
00:15:21,802 --> 00:15:22,043
Speaker 0: Right.

270
00:15:22,103 --> 00:15:27,560
Speaker 0: And you could do the same thing for other similar like time gated events, like a sporting event.

271
00:15:27,720 --> 00:15:29,480
Speaker 0: It's like, OK, the sporting event started.

272
00:15:29,621 --> 00:15:33,960
Speaker 0: Now there's a thing on your on your lock screen showing you the live score.

273
00:15:34,443 --> 00:15:36,360
Speaker 0: And then when the game's over, it's gone.

274
00:15:36,561 --> 00:15:36,742
Speaker 0: Right.

275
00:15:36,762 --> 00:15:44,020
Speaker 0: Just one notification, one thing that knows when to come and when to go away and update itself rather than having multiple notifications.

276
00:15:44,360 --> 00:15:46,980
Speaker 0: So that's going to be a good thing.

277
00:15:47,140 --> 00:15:47,522
Speaker 0: I like that.

278
00:15:48,428 --> 00:15:50,500
Speaker 0: So the thing rim talked about the Web push.

279
00:15:50,820 --> 00:15:51,041
Speaker 0: Right.

280
00:15:51,081 --> 00:15:56,199
Speaker 0: So speaking of notifications, as long as they've had notifications on iOS.

281
00:15:57,061 --> 00:15:57,322
Speaker 0: Right.

282
00:15:57,462 --> 00:16:04,900
Speaker 0: And the way it's worked is that, you know, you had to get an app in the App Store, get an Apple developer account.

283
00:16:05,363 --> 00:16:05,626
Speaker 0: Right.

284
00:16:05,687 --> 00:16:06,820
Speaker 0: Get your app approved.

285
00:16:07,222 --> 00:16:08,819
Speaker 0: Get someone to install your app.

286
00:16:09,241 --> 00:16:15,240
Speaker 0: Get someone to give your app notification permissions, and then the app can notify them within certain bounds.

287
00:16:15,820 --> 00:16:16,082
Speaker 0: Right.

288
00:16:16,183 --> 00:16:18,520
Speaker 0: A behavior that Apple deemed acceptable.

289
00:16:19,482 --> 00:16:25,400
Speaker 0: And then, you know, it's like, you know, in order to not ruin the battery life and annoy the users.

290
00:16:25,704 --> 00:16:25,866
Speaker 0: Right.

291
00:16:25,907 --> 00:16:26,880
Speaker 0: With lots of notifications.

292
00:16:28,522 --> 00:16:38,840
Speaker 0: So that's the way it's been, you might notice if you're anyone who uses a normal computer still that, like even today, desktop Web browsers do notifications.

293
00:16:39,060 --> 00:16:44,980
Speaker 0: Like you can go to a website and it'll pop up an annoying message like, hey, you want notifications from this website?

294
00:16:45,240 --> 00:16:45,320
Speaker 0: Huh?

295
00:16:45,421 --> 00:16:45,601
Speaker 0: Huh?

296
00:16:45,662 --> 00:16:45,883
Speaker 0: Huh?

297
00:16:46,384 --> 00:16:55,060
Speaker 0: And if you say yes, it's like your Web browser is basically going to be keeping open like a little thing in the background, like a little service in the background on your computer.

298
00:16:55,821 --> 00:17:09,959
Speaker 0: And the website, just any old website can send notifications and they'll show up like in Windows or Mac, like as OS level notifications, you know, and that's, you know, it's nice when you want to use it.

299
00:17:10,099 --> 00:17:10,281
Speaker 1: Yeah.

300
00:17:10,321 --> 00:17:13,059
Speaker 1: Ninety nine percent of the time you don't want to use it like that.

301
00:17:13,300 --> 00:17:13,541
Speaker 0: Right.

302
00:17:13,885 --> 00:17:15,500
Speaker 0: When you want to use it, it's nice.

303
00:17:15,721 --> 00:17:24,160
Speaker 0: Like, you know, let's say at the game for the website for train games, you can close your tab for train games and still get a notification in OS level.

304
00:17:24,281 --> 00:17:25,880
Speaker 0: That's like, hey, it's your turn in the train game.

305
00:17:26,001 --> 00:17:26,182
Speaker 0: Right.

306
00:17:26,202 --> 00:17:26,685
Speaker 0: That's nice.

307
00:17:27,167 --> 00:17:29,020
Speaker 0: Or Gmail, like, hey, you got an email.

308
00:17:29,142 --> 00:17:30,199
Speaker 0: That's an important email.

309
00:17:30,842 --> 00:17:35,960
Speaker 0: OS level notification, even though your browser might be closed or, you know, whatever.

310
00:17:36,380 --> 00:17:38,457
Speaker 0: So it can be nice when you want to do it.

311
00:17:39,800 --> 00:17:48,339
Speaker 0: But what Apple is going to do is they're going to enable Web push right on Mac OS and iOS via Safari.

312
00:17:48,540 --> 00:18:02,800
Speaker 0: So this means that somebody with a Web page and no iOS app, no Mac app whatsoever can now send you notifications on iOS just from a website.

313
00:18:04,280 --> 00:18:13,421
Speaker 0: And so there's a video here that's actually targeted towards Web developers who want to implement this feature, but it's also useful because it tells users how the feature is going to work.

314
00:18:14,284 --> 00:18:29,209
Speaker 0: And the key here is that a they are not going to allow you to pop up a thing like, hey, you want notifications, the user must click or do some action in order to start the.

315
00:18:29,370 --> 00:18:30,717
Speaker 0: I want that is key.

316
00:18:31,441 --> 00:18:42,981
Speaker 1: That is very similar to how MailChimp dominated all the mailing services in the early days when they force either second verification or active subscription as opposed to opt out.

317
00:18:43,604 --> 00:18:43,824
Speaker 0: Yep.

318
00:18:43,885 --> 00:18:52,100
Speaker 0: So it's the example they give is like a website that has a bell icon and they're like, you can't ask the user for notification permission until they click the bell icon.

319
00:18:52,702 --> 00:18:55,560
Speaker 0: Then they can say, yes, I want notifications from this website.

320
00:18:57,041 --> 00:19:14,020
Speaker 0: And the second thing is that it is technically possible to do this the Web push and send messages to someone's Web browser and have JavaScript take action in response to those messages without displaying a notification to the user.

321
00:19:14,080 --> 00:19:18,580
Speaker 0: You could just sort of, you know, get the user's computer to do processing or whatever.

322
00:19:19,142 --> 00:19:24,181
Speaker 0: And Apple's like, yeah, you can, you know, a you can hurt someone's battery life like that.

323
00:19:24,743 --> 00:19:29,000
Speaker 0: Be you're using someone's CPU, possibly their phone CPU.

324
00:19:29,181 --> 00:19:33,940
Speaker 0: Hey, phone Bitcoin for me without them realizing you're doing it right.

325
00:19:34,120 --> 00:19:42,823
Speaker 0: So Apple has a system in place where if you don't display a notification to the user quickly enough.

326
00:19:43,526 --> 00:19:49,359
Speaker 0: after you send them a Web push and you do that like three times or something, then that's it.

327
00:19:49,480 --> 00:19:51,100
Speaker 0: They take away your Web push permission.

328
00:19:51,504 --> 00:19:52,900
Speaker 0: It's like you're done, right?

329
00:19:52,941 --> 00:19:53,980
Speaker 0: You can't do it anymore.

330
00:19:54,121 --> 00:19:55,480
Speaker 0: Your whole domain is just blocked.

331
00:19:56,021 --> 00:20:03,720
Speaker 1: I noticed this side thing in the article you link to around service worker APIs to that what it is service work that that seems to be exactly what this is.

332
00:20:03,800 --> 00:20:04,899
Speaker 1: This looks pretty good.

333
00:20:05,181 --> 00:20:15,040
Speaker 0: A service worker API is basically the JavaScript that's sitting on your computer that will leap into action once a notification comes in from the Internet.

334
00:20:15,160 --> 00:20:15,361
Speaker 0: Right.

335
00:20:15,381 --> 00:20:30,406
Speaker 0: So it's like, you know, it's like you leave instructions on the person's phone or on their computer of like what to do if they receive a message from Web site X. And then so when Web site X sends a message, the Safari browser says, oh, where did this message come from?

336
00:20:30,708 --> 00:20:33,022
Speaker 0: It came from Web site X. Send it.

337
00:20:33,103 --> 00:20:34,799
Speaker 0: Do I have a service worker for Web site X?

338
00:20:34,980 --> 00:20:35,322
Speaker 0: I do.

339
00:20:35,644 --> 00:20:38,060
Speaker 0: And it sends the message to the service worker for Web site.

340
00:20:38,181 --> 00:20:43,240
Speaker 1: What are the examples they give on like what is what it's supposed to do, what they expect you to use this for is actually a really good one.

341
00:20:43,621 --> 00:20:50,780
Speaker 1: I'm just going to quote the Web site receiving centralized updates to expensive to calculate data such as geolocation or gyroscope.

342
00:20:51,102 --> 00:20:53,300
Speaker 1: So multiple pages can make use of one set of data.

343
00:20:54,121 --> 00:20:58,040
Speaker 0: Yeah, you could go to a Web site, click on like do something, go away.

344
00:20:58,541 --> 00:21:03,001
Speaker 0: And then once the thing is done, you'll get a notification that's like, hey, thing is done.

345
00:21:03,122 --> 00:21:08,473
Speaker 0: You click on it and that will take you to the Web page with the results of the thing that is now done.

346
00:21:08,493 --> 00:21:08,919
Speaker 0: Right.

347
00:21:09,721 --> 00:21:11,900
Speaker 0: So anyway, yeah, they're now good.

348
00:21:11,980 --> 00:21:24,640
Speaker 0: You're now going to be able to send notifications to users of iOS devices without having to make an iOS app and get in the app store or give Apple money or make an Apple developer account or anything.

349
00:21:24,780 --> 00:21:31,820
Speaker 0: And that is kind of a big deal because, for example, front row crew dot com could make it so that you get a notification on your phone.

350
00:21:31,980 --> 00:21:37,302
Speaker 0: But it's a new geek nights episode, even though that we don't have an official iPhone app whatsoever.

351
00:21:38,446 --> 00:21:43,943
Speaker 0: So and the notifications are much more useful on the phone than they are the desktop.

352
00:21:44,204 --> 00:21:47,820
Speaker 0: So I foresee do and a lot of people only have phones.

353
00:21:48,502 --> 00:22:00,780
Speaker 0: So I think what you're going to see is a lot more Web sites implementing notifications, implementing them in a way that is acceptable to Apple and won't annoy you by asking you to accept notifications just every just from visiting the Web.

354
00:22:01,160 --> 00:22:07,600
Speaker 1: This is where Apple often serves as a thought leader because they will implement and enforce design patterns that are pleasant to the user.

355
00:22:08,102 --> 00:22:15,940
Speaker 1: And then the rest of the ecosystem is usually forced to basically follow Apple's rules because people get used to like that's old flash.

356
00:22:16,184 --> 00:22:17,160
Speaker 0: That's how they killed flash.

357
00:22:17,280 --> 00:22:26,669
Speaker 1: I think that's a big part of why so many people perceive Apple products to be more pleasant to use, because there's usually at least one design pattern where Apple has said, nope, that sucks.

358
00:22:26,730 --> 00:22:27,579
Speaker 1: We're just going to ban it.

359
00:22:28,381 --> 00:22:34,262
Speaker 0: I mean, listen, you know, if I wasn't somebody who played games like, you know, with Steam.

360
00:22:34,583 --> 00:22:34,824
Speaker 0: Right.

361
00:22:35,025 --> 00:22:38,640
Speaker 0: And be developed software that was like Linux software.

362
00:22:39,443 --> 00:22:39,686
Speaker 0: Right.

363
00:22:40,012 --> 00:22:40,438
Speaker 0: I'll get a Mac.

364
00:22:40,882 --> 00:22:41,063
Speaker 0: Right.

365
00:22:41,124 --> 00:22:42,819
Speaker 0: There's no reason for me to not have a Mac.

366
00:22:43,382 --> 00:22:43,727
Speaker 0: Right.

367
00:22:43,808 --> 00:22:44,700
Speaker 0: I've had an iPhone.

368
00:22:45,161 --> 00:22:48,380
Speaker 0: There'll be no reason to not have a Mac as like those are the two things.

369
00:22:48,560 --> 00:22:50,359
Speaker 0: And Mac will basically never do those things.

370
00:22:51,581 --> 00:22:57,320
Speaker 0: So, yeah, playing games and editing like extreme video with the real Mac can do that.

371
00:22:57,340 --> 00:22:58,820
Speaker 0: You can run all the Adobe software on there.

372
00:22:58,900 --> 00:22:59,323
Speaker 0: No problem.

373
00:22:59,343 --> 00:22:59,605
Speaker 1: Oh, yeah.

374
00:22:59,625 --> 00:23:01,700
Speaker 1: What you run into is you can't get you can't get a good GPU.

375
00:23:03,803 --> 00:23:08,593
Speaker 0: Well, I mean, if you're, you know, the GPU, if you get the Mac Studio, it's going to be for video editing.

376
00:23:08,634 --> 00:23:09,140
Speaker 0: You'll be fine.

377
00:23:09,720 --> 00:23:09,821
Speaker 0: Right.

378
00:23:10,948 --> 00:23:12,620
Speaker 1: So it's in some other news.

379
00:23:12,941 --> 00:23:13,999
Speaker 1: This is actually a big deal.

380
00:23:14,681 --> 00:23:15,264
Speaker 1: New York.

381
00:23:15,766 --> 00:23:18,360
Speaker 1: I think I think all this left is the governor has to sign this.

382
00:23:18,541 --> 00:23:25,570
Speaker 1: And we have a pretty OK governor right now in considering the span of what it is better than the last governor.

383
00:23:25,590 --> 00:23:26,539
Speaker 0: But that's not saying much.

384
00:23:26,940 --> 00:23:28,312
Speaker 1: We're going to primary around.

385
00:23:28,332 --> 00:23:29,220
Speaker 0: It could be a lot worse.

386
00:23:29,360 --> 00:23:30,219
Speaker 0: It could be a lot better.

387
00:23:30,360 --> 00:23:30,761
Speaker 0: Yeah.

388
00:23:31,082 --> 00:23:38,423
Speaker 1: But we have our state legislature has passed a right to repair law in New York for electronics.

389
00:23:38,523 --> 00:23:42,340
Speaker 1: And because New York is so big, this could have national ramifications.

390
00:23:43,600 --> 00:23:51,325
Speaker 1: Basically, this law, unfortunately, it doesn't actually cover agricultural equipment or cars or most hardware.

391
00:23:51,425 --> 00:23:56,547
Speaker 1: really only covers things like cell phones and like digital technology.

392
00:23:56,608 --> 00:23:57,759
Speaker 1: The definition is a little bit weird.

393
00:23:58,481 --> 00:24:01,180
Speaker 0: It's limited, but it's, you know, it's a start to nothing.

394
00:24:01,720 --> 00:24:07,320
Speaker 1: Kind of like that week sauce gun bill that looks like it might actually pass the Senate because they got 10 Republicans on board.

395
00:24:07,861 --> 00:24:08,609
Speaker 1: Yes, it's not.

396
00:24:08,670 --> 00:24:09,539
Speaker 0: But it's not nothing.

397
00:24:09,861 --> 00:24:12,120
Speaker 1: Yeah, not nothing is the best you can do sometimes.

398
00:24:12,980 --> 00:24:28,600
Speaker 1: So the bill better than nothing requires digital electronics manufacturers to make repair instructions and parts available, not just to technicians who are independent, but also directly to consumers.

399
00:24:29,221 --> 00:24:29,361
Speaker 0: Right.

400
00:24:29,381 --> 00:24:35,124
Speaker 0: Well, I mean, Apple did make, you know, all this repair stuff available to everybody.

401
00:24:35,205 --> 00:24:37,559
Speaker 0: There are some articles out there of people who did it.

402
00:24:38,061 --> 00:24:46,863
Speaker 0: And the thing is, it's like, yeah, they'll send you all this ridiculous equipment to like fix your iPhone and you have to send it back within a limited time frame so you don't get charged for it.

403
00:24:47,225 --> 00:24:50,679
Speaker 0: Thousands of dollars of expensive equipment and instructions like.

404
00:24:50,720 --> 00:24:51,808
Speaker 0: you can do it yourself.

405
00:24:51,949 --> 00:24:53,078
Speaker 0: You can succeed at it.

406
00:24:53,982 --> 00:24:57,020
Speaker 0: But it's you know, it's still not for the faint of heart, right?

407
00:24:57,121 --> 00:24:57,705
Speaker 0: It's really.

408
00:24:58,169 --> 00:24:58,955
Speaker 0: it's like they do this.

409
00:24:59,800 --> 00:25:08,280
Speaker 0: They would satisfy the law and that, yes, they'll send it to just any old person, but it's really designed for somebody who is a professional.

410
00:25:08,702 --> 00:25:11,260
Speaker 0: You know, it's very much like they're running a repair shop.

411
00:25:11,400 --> 00:25:11,661
Speaker 0: Right.

412
00:25:11,681 --> 00:25:19,240
Speaker 1: Well, it's very much analogous to car repair and how you don't have to take your car to the dealership you brought it from to get it repaired.

413
00:25:19,562 --> 00:25:20,700
Speaker 1: Go to any fucking mechanic.

414
00:25:21,022 --> 00:25:21,979
Speaker 1: Some mechanics are good.

415
00:25:22,340 --> 00:25:23,219
Speaker 1: Some mechanics are bad.

416
00:25:23,521 --> 00:25:24,639
Speaker 1: Some mechanics are cheap.

417
00:25:25,043 --> 00:25:26,800
Speaker 0: Some people can do it themselves.

418
00:25:27,540 --> 00:25:27,721
Speaker 1: Exactly.

419
00:25:28,022 --> 00:25:31,580
Speaker 1: But you can do that because of laws and regulations.

420
00:25:32,340 --> 00:25:39,419
Speaker 1: And now we're applying the same principle, at least to phones and laptops and a lot of digital or digital adjacent goods.

421
00:25:40,241 --> 00:25:43,460
Speaker 0: It's all my thing of the day from I think the last time we did shows, right?

422
00:25:43,580 --> 00:25:44,800
Speaker 0: Stas takes fix on YouTube.

423
00:25:44,960 --> 00:25:46,240
Speaker 0: I'm still loving that guy's channel.

424
00:25:46,460 --> 00:25:53,160
Speaker 0: He's just, you know, this guy who admits he doesn't know much about electronics, but somehow he orders some broken shit on eBay.

425
00:25:53,563 --> 00:25:56,220
Speaker 0: He fixes it and then he's real excited when it works.

426
00:25:56,461 --> 00:25:59,099
Speaker 0: He's like, yeah, it's like it's really fun.

427
00:25:59,461 --> 00:26:02,460
Speaker 0: It's like, but how did this get all about fixing stuff these days?

428
00:26:02,540 --> 00:26:03,739
Speaker 1: Like how did this come to pass?

429
00:26:03,860 --> 00:26:10,620
Speaker 1: Because with with any time a law passes anywhere in the US, the real question is, how did that good law make it through our government?

430
00:26:10,680 --> 00:26:11,584
Speaker 1: Like how did that happen?

431
00:26:12,045 --> 00:26:17,020
Speaker 1: Because if there's a pattern to that, you could make more good things happen theoretically.

432
00:26:17,120 --> 00:26:26,620
Speaker 1: So one, it happened in an extremely blue state like this is a state that is completely controlled by the Democrats to it's only within one state.

433
00:26:26,740 --> 00:26:28,400
Speaker 1: It just happens to be a very powerful state.

434
00:26:28,781 --> 00:26:30,267
Speaker 1: See also how California.

435
00:26:30,749 --> 00:26:34,488
Speaker 1: lot of companies follow California rules because they don't want to have trouble with.

436
00:26:34,508 --> 00:26:36,260
Speaker 1: the California market is too big to ignore.

437
00:26:36,781 --> 00:26:40,832
Speaker 1: And three, a number of companies.

438
00:26:40,872 --> 00:26:42,877
Speaker 0: don't put gay people in their movies.

439
00:26:42,918 --> 00:26:44,148
Speaker 0: that China market too big.

440
00:26:44,168 --> 00:26:44,579
Speaker 0: Exactly.

441
00:26:46,281 --> 00:26:56,940
Speaker 1: So there were a number of public groups, advocacy groups that were aggressively fighting for this in New York state for at least five years now.

442
00:26:57,020 --> 00:27:07,060
Speaker 1: This is the culmination of years and years of constant letter writing, bothering random state senators like harassing government officials, donating money.

443
00:27:07,300 --> 00:27:15,320
Speaker 1: This is this was a lot of work to get this bill passed from a lot of very, very dedicated activists in this very tiny space.

444
00:27:15,801 --> 00:27:25,820
Speaker 1: So my advice to you is if you care about something about on this level, go harass like the state legislature in an important blue state.

445
00:27:26,060 --> 00:27:29,420
Speaker 1: That is probably the best for a long period of time.

446
00:27:29,601 --> 00:27:30,960
Speaker 0: Yeah, a lot of help, right?

447
00:27:31,221 --> 00:27:32,359
Speaker 0: That's how you can make it happen.

448
00:27:32,560 --> 00:27:42,559
Speaker 1: You've got to be annoying to the point to where a state government in the United States will would rather do what you want to make you go away than continue to listen to you.

449
00:27:45,162 --> 00:27:50,280
Speaker 1: It's a strategy in some other news, that's a good segue into the main bit.

450
00:27:51,120 --> 00:27:58,460
Speaker 1: We can't ignore Dolly because it is the new wordle in terms of completely filling my Twitter stream with.

451
00:27:58,760 --> 00:28:00,332
Speaker 0: Yeah, it's this little A.I.

452
00:28:01,540 --> 00:28:10,120
Speaker 0: ish right machine learning image generating model app that is available to the general public on some sort of throttled basis.

453
00:28:10,340 --> 00:28:14,522
Speaker 0: You can't, you know, just use it because it takes quite a bit of computing power to make it go.

454
00:28:14,683 --> 00:28:14,924
Speaker 1: Yep.

455
00:28:15,185 --> 00:28:18,943
Speaker 1: And what we're playing with is the mini version, not the full on version, of course.

456
00:28:19,386 --> 00:28:21,380
Speaker 0: And it's free to try to use, though.

457
00:28:21,400 --> 00:28:34,480
Speaker 0: You just got to get through the throttling and yeah, you can type in any old text and it will search its database of images and it will try to combine those images into new images that match your description.

458
00:28:34,621 --> 00:28:40,860
Speaker 0: So you could write like, you know, rim jumping up and down and it will try to make pictures of rim jumping up and down.

459
00:28:40,980 --> 00:28:41,201
Speaker 0: Right.

460
00:28:41,241 --> 00:28:41,563
Speaker 0: For what?

461
00:28:41,603 --> 00:28:52,920
Speaker 0: However, it understands those words and you can have a lot of fun creating a lot of really interesting images by typing lots of pop culture combinations into this thing.

462
00:28:53,220 --> 00:28:57,702
Speaker 1: I just got it to make Shrek riding a bicycle through Penn Station and it pretty much nailed it.

463
00:28:58,405 --> 00:29:04,720
Speaker 0: Yeah, it's you know, you got to sort of learn you're in like what kind of things it does a good job with and what kinds of things it doesn't do.

464
00:29:04,940 --> 00:29:07,599
Speaker 1: I found and if you read their website for the full A.I.

465
00:29:07,860 --> 00:29:13,322
Speaker 1: model, it's interesting because it really focuses on natural language and detailed descriptions.

466
00:29:13,422 --> 00:29:20,304
Speaker 1: So you'd like an example of a good search would be a storefront that has the word open A.I.

467
00:29:20,445 --> 00:29:31,800
Speaker 1: written on it or an armchair in the shape of an avocado, the exact same cat on the top as a sketch on the bottom, like things like that.

468
00:29:32,121 --> 00:29:34,678
Speaker 1: The more specific you are, the better it is.

469
00:29:35,221 --> 00:29:39,120
Speaker 1: And it basically it seems like it's very similar.

470
00:29:39,340 --> 00:29:41,620
Speaker 1: Actually, now that I'm scrolling down to the website, they even call it out.

471
00:29:41,680 --> 00:29:42,611
Speaker 1: They say it's similar.

472
00:29:42,651 --> 00:29:43,339
Speaker 1: So it's not just me.

473
00:29:44,020 --> 00:29:55,243
Speaker 1: This is basically doing the same kind of thing that GPT-3 does, which is a natural language neural network where you can tell it things like write an article about this new cell phone.

474
00:29:55,304 --> 00:29:58,036
Speaker 1: It'll write a kind of shitty article about this new cell phone.

475
00:29:58,076 --> 00:30:01,160
Speaker 1: that's good enough, like it's enough to start with.

476
00:30:01,240 --> 00:30:02,256
Speaker 1: It gives you a starting point.

477
00:30:04,360 --> 00:30:09,020
Speaker 0: These it's a lot of fun and there's a lot of good memes people have generated with it.

478
00:30:09,501 --> 00:30:09,742
Speaker 1: Yep.

479
00:30:10,244 --> 00:30:14,179
Speaker 1: And we're going to edge into a little bit of people are talking about this.

480
00:30:16,321 --> 00:30:20,940
Speaker 1: I've seen people having such ridiculous takes as this will destroy jobs.

481
00:30:21,720 --> 00:30:22,462
Speaker 1: This A.I.

482
00:30:22,502 --> 00:30:25,612
Speaker 1: will take away my source of like it's people are.

483
00:30:26,034 --> 00:30:28,906
Speaker 1: no people are overreacting to the wrong.

484
00:30:29,128 --> 00:30:30,679
Speaker 0: Those are tongue in cheek.

485
00:30:31,444 --> 00:30:32,412
Speaker 0: This will take my job.

486
00:30:32,453 --> 00:30:33,240
Speaker 0: I don't think it's serious.

487
00:30:33,560 --> 00:30:44,920
Speaker 1: I've seen a few serious people, but it's more serious in the same way as the people who would complain that digital animation destroyed like my ability to make money painting cells.

488
00:30:47,321 --> 00:30:53,940
Speaker 1: Anyway, anyway, things of the day.

489
00:30:54,161 --> 00:31:03,098
Speaker 1: So I'm on a big Star Trek kick lately and my cup overflow it because there's a lot of new Star Trek, also a lot of old Star Trek that I never fully watched.

490
00:31:03,118 --> 00:31:09,280
Speaker 1: that I'm digging through currently near the end of a rewatch slash first watch of Deep Space Nine.

491
00:31:09,861 --> 00:31:22,862
Speaker 1: And my thing of the day is, if you're familiar in the original Star Trek or TOS, as nerds will call it, the original series, there was a parallel sort of sister show that was official.

492
00:31:22,962 --> 00:31:28,006
Speaker 1: It's canonical, but it was animated, filmation animation, like really limited animation.

493
00:31:28,026 --> 00:31:29,539
Speaker 1: It's a full on Star Trek.

494
00:31:30,202 --> 00:31:32,657
Speaker 1: You've seen it out there, but I've seen it.

495
00:31:33,341 --> 00:31:34,380
Speaker 0: But, you know, I don't a lot.

496
00:31:34,520 --> 00:31:36,760
Speaker 0: I know that many people like watch the whole thing.

497
00:31:36,861 --> 00:31:38,579
Speaker 0: I've definitely seen that it exists.

498
00:31:38,680 --> 00:31:46,560
Speaker 1: But what's notable about it, other than the fact that it is fully canonical, is that it has a distinctive style.

499
00:31:47,400 --> 00:31:53,700
Speaker 1: It is like it's hard to describe the style in full, but it uses a lot of like weird, almost Dutch angles.

500
00:31:54,302 --> 00:32:02,580
Speaker 1: It has a lot of close ups of a face just reacting to something and not saying anything while music plays for like five or six seconds at a time.

501
00:32:03,001 --> 00:32:10,439
Speaker 1: Clearly, it's a lot of the things they were doing to reduce how much animation they had to make in a Star Trek to crank this show out.

502
00:32:10,903 --> 00:32:12,600
Speaker 1: But it has a very distinctive style.

503
00:32:13,081 --> 00:32:14,278
Speaker 0: It was very super frenzy.

504
00:32:14,760 --> 00:32:16,039
Speaker 0: Yeah, I felt so.

505
00:32:17,180 --> 00:32:30,700
Speaker 1: This video is from some fans of this Grazelle Automations, and they have made a five and a half minute long video that is fully animated.

506
00:32:31,141 --> 00:32:38,480
Speaker 1: It is a basically an episode of Star Trek Voyager condensed into an animated in this style.

507
00:32:39,460 --> 00:32:48,860
Speaker 1: And what makes this sublime is they didn't just take the audio from this episode of Star Trek Voyager and just like animate it in the animation style.

508
00:32:49,421 --> 00:32:50,435
Speaker 1: They reworked it.

509
00:32:51,161 --> 00:32:55,340
Speaker 1: It has a soundtrack like the original animation instead of like Voyager.

510
00:32:55,843 --> 00:32:57,719
Speaker 1: They added all the weird pregnant pauses.

511
00:32:58,222 --> 00:32:59,720
Speaker 1: They moved the dialogue around.

512
00:33:00,501 --> 00:33:06,000
Speaker 1: It mimics completely everything that made that original animated show distinctive.

513
00:33:06,462 --> 00:33:10,359
Speaker 1: It's kind of amazing how like they hit it out of the park.

514
00:33:12,043 --> 00:33:13,780
Speaker 0: I mean, I do appreciate that, right?

515
00:33:13,960 --> 00:33:18,338
Speaker 0: I talk a lot about how like, hey, you know, why not make something new in the old style, right?

516
00:33:18,480 --> 00:33:24,559
Speaker 0: Why not just make, you know, a noir movie with some black and white film as it, you know, as if it was nineteen.

517
00:33:25,100 --> 00:33:27,260
Speaker 1: Scott, you ever see a movie called Brick?

518
00:33:29,061 --> 00:33:30,165
Speaker 0: No, really, it's a.

519
00:33:30,266 --> 00:33:37,619
Speaker 1: it's a straight up film noir and it's about a brick of cocaine, but it's set in like a dumpy high school, like earnestly.

520
00:33:38,120 --> 00:33:41,280
Speaker 1: And it's this film is I can't believe you've never seen it.

521
00:33:41,360 --> 00:33:43,440
Speaker 0: I'm sure there are people who do it, but it's not common.

522
00:33:43,760 --> 00:33:44,439
Speaker 1: Oh, yeah, it's not common.

523
00:33:45,382 --> 00:33:50,600
Speaker 0: Yeah, you know, old movies, they always open with like, you know, credits and like a classical music playing.

524
00:33:50,900 --> 00:33:55,658
Speaker 0: And it's like, you know, it's like what if you went to the movie theater today and a movie started like that?

525
00:33:55,960 --> 00:33:56,243
Speaker 0: It's like.

526
00:33:56,729 --> 00:33:57,357
Speaker 0: no one does that.

527
00:33:58,340 --> 00:33:58,978
Speaker 1: Yeah, it is rare.

528
00:33:59,380 --> 00:34:07,900
Speaker 1: Case in point, Hateful Eight, like that movie, that's what it was like to go to an old timey four and a half hour long movie with an intermission a little bit.

529
00:34:09,167 --> 00:34:10,199
Speaker 0: Anyway, so the thing of the day.

530
00:34:10,400 --> 00:34:12,020
Speaker 0: So here's his book that's coming out.

531
00:34:12,199 --> 00:34:17,340
Speaker 0: The book is not actually coming out until September, September this year.

532
00:34:17,620 --> 00:34:17,860
Speaker 0: Right.

533
00:34:18,001 --> 00:34:33,940
Speaker 0: But there is currently a free chapter one preview and you can get an early access e-book for thirty two dollars and you can preorder the print book and that will also include the early access e-book for forty dollars.

534
00:34:34,260 --> 00:34:39,560
Speaker 0: And the book is actually you might think, ah, it's an electronics book because it's called Open Circuits.

535
00:34:39,641 --> 00:34:41,980
Speaker 0: But actually this is a photography book.

536
00:34:42,181 --> 00:34:42,442
Speaker 0: Right.

537
00:34:43,103 --> 00:34:47,280
Speaker 0: It's a. it's a book where basically they take electronic components.

538
00:34:47,842 --> 00:34:48,063
Speaker 0: Right.

539
00:34:48,105 --> 00:34:49,940
Speaker 0: The free chapter is passive components.

540
00:34:50,081 --> 00:34:54,500
Speaker 0: But then the other chapters, you have semiconductors, electromechanics, cables and connectors, et cetera.

541
00:34:54,620 --> 00:34:54,821
Speaker 0: Right.

542
00:34:55,643 --> 00:35:08,780
Speaker 0: And I each chapter is basically photographs of not just the components because like, you know, what a resistor looks like, but mostly cross sections of the components.

543
00:35:09,281 --> 00:35:09,482
Speaker 0: Right.

544
00:35:09,562 --> 00:35:18,000
Speaker 0: It's like you might know that like, yeah, a capacitor is like, you know, you like you know, in the theory and principle of what's in there.

545
00:35:18,121 --> 00:35:18,343
Speaker 0: Right.

546
00:35:18,383 --> 00:35:20,120
Speaker 0: There's maybe some oil in there or something.

547
00:35:20,421 --> 00:35:20,703
Speaker 0: Right.

548
00:35:21,125 --> 00:35:24,300
Speaker 0: But like if if I said, all right.

549
00:35:24,340 --> 00:35:27,480
Speaker 0: I'm about to cut open a capacitor right in half and show you the inside.

550
00:35:27,602 --> 00:35:28,233
Speaker 0: It's discharge.

551
00:35:28,274 --> 00:35:28,640
Speaker 0: Don't worry.

552
00:35:29,140 --> 00:35:29,302
Speaker 0: Right.

553
00:35:29,524 --> 00:35:31,078
Speaker 0: Would you know what you're about to see?

554
00:35:32,220 --> 00:35:32,724
Speaker 1: I would.

555
00:35:33,813 --> 00:35:34,680
Speaker 1: But most people wouldn't.

556
00:35:34,820 --> 00:35:37,552
Speaker 1: And I wouldn't for most electronic components, just I happen to.

557
00:35:38,074 --> 00:35:41,399
Speaker 1: I used to desolder and resolder capacitors as a job.

558
00:35:41,682 --> 00:35:43,280
Speaker 0: But did you cut a capacitor in half?

559
00:35:43,901 --> 00:35:53,319
Speaker 1: Yes, actually, because I had to prove that capacitors I was taking out of computer motherboards were defective because of the electrolyte solution inside of them.

560
00:35:54,381 --> 00:35:59,500
Speaker 0: Anyway, so the point is doing that at IBM, it's a book of photography and it's all.

561
00:35:59,520 --> 00:36:04,240
Speaker 0: the photographs are basically the insides and cross sections of electronic components.

562
00:36:04,400 --> 00:36:06,360
Speaker 0: You can see what the hell is actually in their school.

563
00:36:06,661 --> 00:36:08,740
Speaker 0: This is pretty fascinating.

564
00:36:09,322 --> 00:36:12,600
Speaker 0: I recommend you download the free first chapter.

565
00:36:12,841 --> 00:36:19,640
Speaker 0: And if you like what you see, well, there's the beef, not free options that will, you know, you can preorder for September.

566
00:36:19,801 --> 00:36:22,676
Speaker 0: Oh, and look, it says use coupon code preorder to get 25 percent off.

567
00:36:23,885 --> 00:36:24,880
Speaker 0: I never understood that.

568
00:36:25,020 --> 00:36:26,860
Speaker 0: It's like I'm on the purchasing page.

569
00:36:27,301 --> 00:36:29,400
Speaker 0: Why are you writing the coupon code right there?

570
00:36:29,520 --> 00:36:32,036
Speaker 0: Why not just lower the price by 25 percent?

571
00:36:32,980 --> 00:36:36,180
Speaker 1: I can explain that with modern pricing theory.

572
00:36:36,900 --> 00:36:37,544
Speaker 1: There is a very.

573
00:36:37,624 --> 00:36:40,724
Speaker 1: I talked to us in Geek Nights a while ago, but this is a direct example of it.

574
00:36:41,088 --> 00:36:42,220
Speaker 1: This is a simple example.

575
00:36:42,341 --> 00:36:46,080
Speaker 1: But the idea is it's the same reason coupons exist in the modern world.

576
00:36:46,780 --> 00:36:47,164
Speaker 1: You do that.

577
00:36:47,224 --> 00:36:48,960
Speaker 0: Coupon is right on the page.

578
00:36:49,140 --> 00:36:50,286
Speaker 1: Here's what you're up.

579
00:36:50,306 --> 00:36:53,020
Speaker 1: So despite it being right on the page.

580
00:36:53,481 --> 00:37:09,880
Speaker 1: The data on how many people copy paste it right there when they buy it versus just don't bother is extremely useful data, because terrifyingly, that is how you generate lists of people who care more about time than money or more about money than time.

581
00:37:10,060 --> 00:37:12,940
Speaker 0: People who know how to read generate a list of people.

582
00:37:13,160 --> 00:37:15,080
Speaker 1: Not all I'm going to I'll put it diplomatically.

583
00:37:15,680 --> 00:37:29,915
Speaker 1: You can infer a lot about the future behavior of someone on an e-commerce site based on whether or not they will copy paste the word preorder and all caps from one part of a page into another part of a page to get 25 percent off.

584
00:37:30,920 --> 00:37:33,199
Speaker 1: You can also infer a lot about someone who will not do that.

585
00:37:33,820 --> 00:37:34,726
Speaker 1: I'll just leave it at that.

586
00:37:35,088 --> 00:37:37,080
Speaker 1: That is something they teach product managers.

587
00:37:38,921 --> 00:37:44,566
Speaker 0: Anyway, you took this one pricing like like training like one time and like you just keep going on and on.

588
00:37:44,748 --> 00:37:46,120
Speaker 1: I've taken away more than one training.

589
00:37:46,300 --> 00:37:48,559
Speaker 1: I've been this is something I've been studying for the last five years.

590
00:37:48,701 --> 00:37:50,077
Speaker 0: But anyway, uh huh.

591
00:37:50,580 --> 00:37:50,661
Speaker 0: Yeah.

592
00:37:50,721 --> 00:37:50,984
Speaker 0: OK.

593
00:37:51,004 --> 00:37:52,398
Speaker 0: You went to a training five years ago.

594
00:37:55,201 --> 00:38:01,959
Speaker 1: But anyway, in the bad moment, the Geek Guys Book Club book, Nine Fox Gambit, the next Thursday episode, we are doing the show.

595
00:38:02,402 --> 00:38:05,580
Speaker 1: If you start reading it now, you can easily finish by next Thursday.

596
00:38:05,640 --> 00:38:08,640
Speaker 1: The book is not long, nor is it hard to read.

597
00:38:09,441 --> 00:38:19,199
Speaker 1: And if you bounce off of the first two paragraphs, that's a bunch of proper nouns and cylindrical heresy, you'll you'll figure out what's going on real quick.

598
00:38:21,682 --> 00:38:22,365
Speaker 1: And I'll leave it at that.

599
00:38:22,525 --> 00:38:26,360
Speaker 1: I already started the second book because that's how I roll.

600
00:38:27,180 --> 00:38:28,239
Speaker 0: I really met a moment.

601
00:38:28,441 --> 00:38:30,080
Speaker 0: Also, I guess I'm working on the website.

602
00:38:31,022 --> 00:38:32,440
Speaker 0: It's coming along very nicely.

603
00:38:32,600 --> 00:38:34,087
Speaker 0: I'm doing I'm basically right now.

604
00:38:34,127 --> 00:38:48,240
Speaker 0: I'm working on a thing for uploading new episodes, which is going to work a lot like the way, you know, you upload videos on YouTube, obviously less fancy, but, you know, drag an MP3 in, click, upload, type in a bunch of metadata, push, save that whole that whole thing.

605
00:38:48,582 --> 00:38:48,663
Speaker 1: Yeah.

606
00:38:48,683 --> 00:38:48,865
Speaker 1: Right.

607
00:38:49,108 --> 00:38:50,120
Speaker 1: I also I got a little bit.

608
00:38:50,482 --> 00:38:52,080
Speaker 1: So I started this up again.

609
00:38:52,140 --> 00:38:56,403
Speaker 1: I had been doing it for a while because we hadn't really been reviewing a lot of tabletop games.

610
00:38:56,905 --> 00:39:05,143
Speaker 1: I guess something happened in the world over the last three years that prevented us from going to a bunch of conventions and playing a ton of tabletop games that we normally would.

611
00:39:05,184 --> 00:39:06,919
Speaker 1: that fuels a good chunk of geek nights.

612
00:39:07,462 --> 00:39:15,180
Speaker 1: But it turns out, since we stopped playing a lot of board games, we had done five more episodes where we reviewed a tabletop game.

613
00:39:15,741 --> 00:39:18,800
Speaker 1: So I have now cut them out and put them on YouTube.

614
00:39:19,040 --> 00:39:20,499
Speaker 1: Just the main bits, just the reviews.

615
00:39:22,102 --> 00:39:34,044
Speaker 1: And I'm going to link to, if you don't know, there's a playlist out there that has 78 of our geek nights tabletop game reviews with no news, no main bits, none of that other geek nights crap.

616
00:39:34,104 --> 00:39:36,519
Speaker 1: It's just the review and nothing else.

617
00:39:37,402 --> 00:39:40,419
Speaker 0: What I think is really interesting and probably a mistake that we made.

618
00:39:41,282 --> 00:39:41,664
Speaker 0: Right.

619
00:39:42,226 --> 00:39:45,381
Speaker 0: You know, given that we don't really care about marketing, putting the main bit at the end.

620
00:39:45,602 --> 00:39:51,068
Speaker 0: And well, no, is that if you look around and say like the tabletop gaming community.

621
00:39:51,088 --> 00:39:51,291
Speaker 0: Right.

622
00:39:52,220 --> 00:39:53,130
Speaker 0: Nobody can sit.

623
00:39:53,211 --> 00:39:54,019
Speaker 0: Nobody like knows.

624
00:39:54,381 --> 00:39:56,320
Speaker 0: But you never see them saying like, oh, best you see.

625
00:39:56,400 --> 00:39:57,809
Speaker 0: I'll see occasionally threads like best.

626
00:39:57,849 --> 00:39:59,660
Speaker 0: tabletop podcasts were never mentioned ever.

627
00:39:59,861 --> 00:40:00,081
Speaker 0: Right.

628
00:40:00,623 --> 00:40:04,980
Speaker 0: But 78 episodes is more episodes than most tabletop podcasts in total.

629
00:40:05,201 --> 00:40:05,481
Speaker 0: Right.

630
00:40:06,103 --> 00:40:14,739
Speaker 1: So it's only like not even 10 percent of we have more Wednesday episodes than most anime podcasts that have ever existed.

631
00:40:16,105 --> 00:40:16,959
Speaker 0: Yeah, pretty much.

632
00:40:17,180 --> 00:40:17,381
Speaker 0: Yeah.

633
00:40:17,843 --> 00:40:37,060
Speaker 1: But anyway, I think I've been doing this for the tabletop content because I find that even more so than anything else we do, a succinct, deep review of a tabletop game is the kind of thing that someone will send to a friend to try to convince them to play a And they're useful to travel around independent of the rest of the geek night's crap.

634
00:40:37,700 --> 00:40:43,300
Speaker 1: You don't need to hear about us talking about biking to work for 20 minutes just to find out if Princess of Florence is worth playing.

635
00:40:43,941 --> 00:40:44,799
Speaker 1: So this is out there.

636
00:40:45,220 --> 00:40:47,960
Speaker 1: And before you ask, why aren't you doing this for the anime reviews?

637
00:40:48,421 --> 00:40:48,902
Speaker 1: I was.

638
00:40:49,303 --> 00:40:57,400
Speaker 1: I stopped because even a fucking anime review that had no screenshots, no content of any kind from the anime.

639
00:40:57,782 --> 00:41:00,239
Speaker 1: It's just the audio of me and Scott talking about it.

640
00:41:00,581 --> 00:41:03,020
Speaker 1: Got a copyright takedown on YouTube and I was just fucking done.

641
00:41:03,782 --> 00:41:04,899
Speaker 1: Anime is dead to me on YouTube.

642
00:41:06,161 --> 00:41:07,971
Speaker 0: OK, well, because they just look they're not.

643
00:41:07,991 --> 00:41:11,700
Speaker 0: they're just indiscriminately, you know, they're not actually checking for a copyright violation.

644
00:41:11,900 --> 00:41:16,240
Speaker 0: They just see the name of an anime in the title or description of the thing.

645
00:41:16,742 --> 00:41:18,720
Speaker 0: And then they just like, you know, they just send you a thing.

646
00:41:18,880 --> 00:41:19,607
Speaker 0: They just check.

647
00:41:19,687 --> 00:41:21,000
Speaker 0: It's amazing in your video.

648
00:41:21,260 --> 00:41:27,100
Speaker 1: My YouTube channel only gets this for anime content and literally nothing else.

649
00:41:27,280 --> 00:41:27,381
Speaker 0: Right.

650
00:41:27,401 --> 00:41:29,680
Speaker 0: Well, that's because the anime we all know this.

651
00:41:29,700 --> 00:41:31,300
Speaker 0: How many times have we talked about this?

652
00:41:31,600 --> 00:41:35,060
Speaker 0: Japan is really wild with their copyright enforcement.

653
00:41:35,681 --> 00:41:41,420
Speaker 0: You know, like people in Japan are like going to actual criminal prison for like pirating some mangas or whatever.

654
00:41:41,561 --> 00:41:41,762
Speaker 0: Right.

655
00:41:41,802 --> 00:41:45,880
Speaker 0: They are super strict about it, you know, and obsessed with enforcing it.

656
00:41:45,940 --> 00:41:48,480
Speaker 0: Look at the way Nintendo, you know, what they did to the Bowser.

657
00:41:48,983 --> 00:41:49,350
Speaker 0: Oh, yeah.

658
00:41:49,513 --> 00:41:49,798
Speaker 0: Not this.

659
00:41:50,160 --> 00:41:59,720
Speaker 0: Not the not the look what Nintendo did, not the fictional character and not the president of Nintendo of America, but the pirate guy, Bowser.

660
00:42:00,860 --> 00:42:06,209
Speaker 1: Well, we'll talk about that on the next Tuesday show, because there's that situation is kind of fucked up.

661
00:42:06,331 --> 00:42:06,820
Speaker 1: But anyway.

662
00:42:07,240 --> 00:42:07,441
Speaker 0: Yeah.

663
00:42:07,924 --> 00:42:09,171
Speaker 0: Anyway, so A.I.

664
00:42:10,720 --> 00:42:21,880
Speaker 1: has been in the news a bunch, not just because of, well, just the the industry moving fast and not just because of Dali, but because something a little bit.

665
00:42:22,962 --> 00:42:29,072
Speaker 1: I don't want to say silly, but the story we're about to talk about, there's a lot of facets to it, and we're not going to be able to cover,

666
00:42:29,092 --> 00:42:36,260
Speaker 0: you know, and there's so much we don't know, we can only talk about what we do know as, you know, people who read a couple articles and some blog posts.

667
00:42:36,540 --> 00:42:38,317
Speaker 1: But I'm going to head off before we get into it.

668
00:42:39,781 --> 00:42:39,921
Speaker 1: The

669
00:42:40,062 --> 00:42:40,443
Speaker 1: A.I.,

670
00:42:40,503 --> 00:42:47,777
Speaker 1: the chat bot that is at the center of this cluster of stories is is absolutely not sentience.

671
00:42:48,283 --> 00:42:49,640
Speaker 1: I want to be clear about that.

672
00:42:49,801 --> 00:42:51,300
Speaker 0: I mean, what is sentient?

673
00:42:51,300 --> 00:42:52,760
Speaker 0: You want to have a philosophy episode?

674
00:42:53,164 --> 00:42:53,880
Speaker 1: Oh, we could.

675
00:42:54,361 --> 00:42:55,665
Speaker 1: I am glad you.

676
00:42:56,227 --> 00:43:02,800
Speaker 1: if there's one argument that I am well prepared to have and go on forever about is something appears to be sentient.

677
00:43:03,101 --> 00:43:04,080
Speaker 0: Is it not sentient?

678
00:43:04,320 --> 00:43:05,337
Speaker 0: If no one can tell.

679
00:43:05,840 --> 00:43:06,222
Speaker 1: Yep.

680
00:43:06,402 --> 00:43:12,600
Speaker 1: But there is a lot of hard evidence that this is not sentient in any meaningful way.

681
00:43:14,700 --> 00:43:16,979
Speaker 1: Basically, I'm going to super summarize this.

682
00:43:17,621 --> 00:43:30,120
Speaker 1: An engineer who works at Google believed and then claimed publicly that a Lambda chat bot is sentient and deserves sentient rights.

683
00:43:31,440 --> 00:43:31,560
Speaker 0: Right.

684
00:43:32,042 --> 00:43:37,700
Speaker 0: So my opinion on its sentience, because I can actually say why it's not sentient in my opinion.

685
00:43:38,220 --> 00:43:38,320
Speaker 0: Right.

686
00:43:38,361 --> 00:43:49,130
Speaker 1: I have a few articles queued up that also say in a hard science reason why even there's a there's a real simple reason that I alone, according to what I think, counts as sentient.

687
00:43:49,171 --> 00:43:49,314
Speaker 0: Right.

688
00:43:49,720 --> 00:43:51,185
Speaker 0: Do not consider it to be sentient.

689
00:43:52,689 --> 00:43:58,999
Speaker 0: is that this chat bot, the way it works, is you send some text to it and then it sends some text back.

690
00:43:59,381 --> 00:43:59,643
Speaker 0: Right.

691
00:44:00,286 --> 00:44:03,240
Speaker 0: It never just sends some text on its own.

692
00:44:03,562 --> 00:44:05,660
Speaker 0: It doesn't just say things out of nowhere.

693
00:44:06,043 --> 00:44:06,225
Speaker 0: Right.

694
00:44:06,306 --> 00:44:07,780
Speaker 0: It's just responding to an input.

695
00:44:08,062 --> 00:44:08,303
Speaker 0: Right.

696
00:44:09,371 --> 00:44:10,540
Speaker 0: It won't just say something.

697
00:44:11,182 --> 00:44:18,680
Speaker 0: And you might think, well, you could easily just sort of have some sort of clock hooked up to it so that it would say things on its own.

698
00:44:19,120 --> 00:44:19,220
Speaker 0: Right.

699
00:44:19,240 --> 00:44:31,859
Speaker 1: And there are humans who have particular kinds of executive dysfunctions where they are a human and no one's going to argue that a human is not sentient, but they will not take any action on their own.

700
00:44:32,261 --> 00:44:40,160
Speaker 1: But if you stimulate their brain in a specific way, they will spontaneously begin taking actions like a normal seeming person.

701
00:44:40,660 --> 00:44:40,861
Speaker 0: Sure.

702
00:44:41,223 --> 00:44:44,640
Speaker 0: But regardless, even if you did that to this bot.

703
00:44:44,961 --> 00:44:45,222
Speaker 0: Right.

704
00:44:45,323 --> 00:44:46,510
Speaker 0: It's like, well, what are you going to?

705
00:44:46,651 --> 00:44:47,416
Speaker 0: you still have to like.

706
00:44:47,496 --> 00:44:53,176
Speaker 0: you can't just be like, hey, there's no way to be like, hey, say something you have to actually say something at it.

707
00:44:53,258 --> 00:44:53,380
Speaker 0: Right.

708
00:44:53,440 --> 00:44:53,641
Speaker 0: It can.

709
00:44:53,742 --> 00:44:53,862
Speaker 0: it's.

710
00:44:53,983 --> 00:44:57,220
Speaker 0: all it's doing is processing text.

711
00:44:57,360 --> 00:44:57,702
Speaker 0: You give it.

712
00:44:57,782 --> 00:45:00,820
Speaker 0: if you don't give it any text, then it doesn't do anything.

713
00:45:00,901 --> 00:45:01,999
Speaker 0: It's just a text process.

714
00:45:02,461 --> 00:45:17,022
Speaker 0: I go even I go even though even though its responses to text may very well imitate thought, right, or give the illusion of thought, there is no thought on its own to happen there.

715
00:45:17,103 --> 00:45:27,519
Speaker 1: Well, case in point, imagine, imagine a Turing test, you know, in a chat room, the typical could chat with something and then decide if it is sentient or not or decide if it is human or not.

716
00:45:28,281 --> 00:45:46,667
Speaker 1: If you train a model with humans administering Turing tests and responding to Turing tests, you will get very human sounding responses, because a lot of humans will respond to the kinds of questions humans ask bots to see if they're human the way a human would like.

717
00:45:46,789 --> 00:45:47,719
Speaker 1: It's a self.

718
00:45:48,682 --> 00:45:50,719
Speaker 1: It's basically a self reinforcing model.

719
00:45:51,483 --> 00:45:53,780
Speaker 0: Well, no, but I think that, you know, let's say you did a Turing test.

720
00:45:53,861 --> 00:45:54,305
Speaker 0: Right.

721
00:45:54,346 --> 00:45:55,740
Speaker 0: The oldest test in the book.

722
00:45:55,921 --> 00:45:56,141
Speaker 0: Right.

723
00:45:56,583 --> 00:46:04,378
Speaker 0: What if you did a Turing test against this bot, no matter what text it sent back to you, no matter how convincing that text was, it would always be.

724
00:46:05,102 --> 00:46:09,080
Speaker 0: I send a sentence that sends back a sentence one, one, one, one.

725
00:46:09,200 --> 00:46:15,003
Speaker 0: It would never send back like two or three or send me back one when I didn't send it anything.

726
00:46:15,164 --> 00:46:15,466
Speaker 0: There are.

727
00:46:15,506 --> 00:46:17,700
Speaker 1: there are some chat bots that can do things like that, though.

728
00:46:18,260 --> 00:46:18,421
Speaker 0: Sure.

729
00:46:18,481 --> 00:46:28,840
Speaker 0: But if I was conversing with a human, even if the human was very illiterate and bad at writing, there wouldn't be the sort of one one one one pattern going on.

730
00:46:28,960 --> 00:46:38,020
Speaker 0: And therefore that would be a strong indicator that it would be human compared to the one one one one where that even if the text was vastly superior.

731
00:46:38,560 --> 00:46:38,781
Speaker 0: Right.

732
00:46:39,242 --> 00:46:44,378
Speaker 0: So anyway, so there's this guy, this guy will just I don't.

733
00:46:44,418 --> 00:46:45,020
Speaker 0: there's too much.

734
00:46:45,200 --> 00:46:46,979
Speaker 0: I never heard of this person before this week.

735
00:46:47,140 --> 00:46:47,321
Speaker 0: Yeah.

736
00:46:47,562 --> 00:46:53,299
Speaker 0: And the person the research I did on the Internet indicates this is a very interesting person.

737
00:46:53,380 --> 00:46:56,600
Speaker 0: Have it a lot complicated with a complicated history.

738
00:46:57,344 --> 00:46:58,215
Speaker 0: And you know what?

739
00:46:59,583 --> 00:47:01,340
Speaker 0: You know, it's not all positive.

740
00:47:01,782 --> 00:47:05,680
Speaker 0: It's not all negative, but congratulations on having a really interesting life.

741
00:47:06,201 --> 00:47:06,872
Speaker 0: This person.

742
00:47:06,892 --> 00:47:07,400
Speaker 0: Right.

743
00:47:07,680 --> 00:47:08,303
Speaker 0: What was their name?

744
00:47:08,604 --> 00:47:12,060
Speaker 1: Oh, their name is Blake Lemoine.

745
00:47:13,340 --> 00:47:25,780
Speaker 0: OK, so, yeah, very interesting individual who at least up until very recently was an employee at Google and was doing some sort of AI ethics type research.

746
00:47:26,321 --> 00:47:27,004
Speaker 0: Right.

747
00:47:27,024 --> 00:47:39,020
Speaker 0: And the it is well known that Google has gotten rid of many of its ethics people, which is not great considering the AI stuff they're doing, even whether you want to argue it's actually AI or not.

748
00:47:39,300 --> 00:47:41,700
Speaker 0: You know, the point is machine learning type stuff.

749
00:47:41,860 --> 00:47:47,647
Speaker 0: There's still a lot of ethical issues that you need somebody to, you know, keep a watchful eye on.

750
00:47:47,668 --> 00:47:49,060
Speaker 0: That's not just Google.

751
00:47:49,400 --> 00:47:49,645
Speaker 0: Yeah.

752
00:47:50,116 --> 00:47:50,320
Speaker 0: Right.

753
00:47:50,920 --> 00:47:53,560
Speaker 1: We'll talk about what those ethical issues actually are in a moment.

754
00:47:54,561 --> 00:47:54,781
Speaker 0: Sure.

755
00:47:54,801 --> 00:48:05,940
Speaker 0: So, yeah, this person was working there and they believe that this particular chat bot that I just said is not sentient, they believe otherwise.

756
00:48:06,923 --> 00:48:10,060
Speaker 0: And they went through all kinds of processes and H.R.

757
00:48:10,260 --> 00:48:13,520
Speaker 0: stuff and, you know, raising issues and getting help.

758
00:48:13,720 --> 00:48:17,440
Speaker 1: Looks like they tried to get a lawyer even to represent the AI.

759
00:48:18,322 --> 00:48:18,522
Speaker 0: Right.

760
00:48:18,542 --> 00:48:31,460
Speaker 0: They're doing all kinds of stuff that they documented and also their newspapers reporting on it that they talk to and all kinds of stuff that, you know, we really don't know anything about other than what's been published to report it to Google.

761
00:48:32,222 --> 00:48:38,319
Speaker 0: And the end result was that Google put them on administrative leave, which means we're going to you're weird and we're going to fire you soon.

762
00:48:38,420 --> 00:48:38,685
Speaker 0: Yeah.

763
00:48:38,725 --> 00:48:38,888
Speaker 0: Right.

764
00:48:38,908 --> 00:48:39,579
Speaker 0: That's what it means.

765
00:48:40,161 --> 00:48:53,260
Speaker 0: And you based on the available information and the checkered interesting history of the individual, it's kind of hard to, you know, not read it like both ways.

766
00:48:53,420 --> 00:48:53,641
Speaker 0: Right.

767
00:48:53,701 --> 00:48:58,900
Speaker 0: It's like, well, this person is just, you know, losing it on the one hand.

768
00:48:59,281 --> 00:49:11,265
Speaker 0: But B, you know, you could ease easily also believe that, yeah, Google's up to no good, even while you don't necessarily have to agree that the chat bot is sent out to believe that Google's up to no good.

769
00:49:11,305 --> 00:49:18,259
Speaker 1: But I guess the situation here, like I'm going to actually quote the way article because it because this pretty much sums it up.

770
00:49:19,080 --> 00:49:23,999
Speaker 1: The reporting around this was this engineer thinks it's sentient and Google denies it.

771
00:49:24,440 --> 00:49:41,202
Speaker 1: But to quote WAPO, most academics and practitioners say the words and images generated by artificial intelligence systems such as Lambda produce responses based on what humans have already posted on Wikipedia, Reddit, message boards and every other corner of the Internet.

772
00:49:41,564 --> 00:49:44,600
Speaker 1: And that doesn't signify that the model understands meaning.

773
00:49:45,001 --> 00:49:52,559
Speaker 1: Quote, we now have machines that can mindlessly generate words, but we haven't learned how to stop imagining a mind behind them.

774
00:49:54,622 --> 00:49:59,764
Speaker 1: And it goes on, there's a lot of discussion around this, but basically this person seems to have now gone to the press.

775
00:50:00,146 --> 00:50:11,300
Speaker 1: There's a lot of public conversation around this and the public conversation is getting further and further away from the actual technology involved here and the reality.

776
00:50:11,702 --> 00:50:16,463
Speaker 0: And, you know, because there's so many blanks that are not filled in due to things that have not.

777
00:50:16,543 --> 00:50:34,660
Speaker 0: It's interesting because enough has been reported and enough has been blogged by the person in question that there's a lot of information available relative to what you would normally have in these sort of scenarios where someone's like, let go more than there were in the you know, the previous researchers who got let go from Google.

778
00:50:34,780 --> 00:50:41,825
Speaker 0: Yeah, but still plenty of unknowns and plenty of people talking who don't understand technology, like Rims said.

779
00:50:42,206 --> 00:50:48,951
Speaker 0: So that empty space of knowledge gets filled with conspiracies, conjecture, rumors, nonsense,

780
00:50:49,011 --> 00:50:49,514
Speaker 0: B.S.,

781
00:50:49,554 --> 00:50:51,778
Speaker 0: et cetera, et cetera, that you all got to watch out for all that.

782
00:50:51,960 --> 00:50:57,579
Speaker 1: So first, let's talk about these like linguistic models and sort of how they work or what they mean, because basically what they do.

783
00:50:58,001 --> 00:51:02,519
Speaker 1: And again, we're super, super generalizing for a lot of reasons.

784
00:51:03,901 --> 00:51:18,087
Speaker 1: You feed in a ton of human language and then they will make predictions based on trends and relationships and statistics from that set of language so that if there is a set of text, generate more text.

785
00:51:18,127 --> 00:51:23,240
Speaker 0: that's like the text that I was given or related to the text or, you know, whatever.

786
00:51:23,620 --> 00:51:25,006
Speaker 1: Now, what these what these are

787
00:51:25,247 --> 00:51:25,528
Speaker 0: often

788
00:51:25,850 --> 00:51:38,187
Speaker 1: actually used for in the real world are things like when you try to get customer service and a human doesn't respond, a chat bot responds and it tries to get information out of you and get it get help you as much as possible before

789
00:51:38,429 --> 00:51:39,480
Speaker 0: those aren't even using that.

790
00:51:39,500 --> 00:51:40,859
Speaker 0: A lot of those just straight up scripted.

791
00:51:40,980 --> 00:51:47,548
Speaker 1: Yeah, they're scripted or they have extremely simplistic models, usually just looking for keywords.

792
00:51:48,070 --> 00:51:50,380
Speaker 1: Beyond that, they're often used.

793
00:51:51,101 --> 00:52:02,800
Speaker 1: There's a burgeoning industry of using them to prewrite simplistic articles, news articles, reviews, anything like low hanging fruit, press release nonsense.

794
00:52:04,521 --> 00:52:17,560
Speaker 1: Basically, you feed in the all the all the text around a situation into these models and they'll tell it what to write about and they'll spit out an article that clearly is unreadable, but it gets the broad strokes down.

795
00:52:17,600 --> 00:52:20,540
Speaker 1: It almost like it writes a crappy review of a phone.

796
00:52:20,962 --> 00:52:30,078
Speaker 1: And then you hire a person who's maybe way less skilled than someone who would write an entire article with research from nothing to clean it up and make it look like a human wrote it and publish it.

797
00:52:30,563 --> 00:52:33,120
Speaker 1: And that is a worrying and growing trend in journalism.

798
00:52:33,564 --> 00:52:34,919
Speaker 1: But that's a use for these things.

799
00:52:35,541 --> 00:52:40,562
Speaker 0: Yeah, I mean, the number one use is everybody knows is the series, Alexa's and OK, Google's.

800
00:52:40,642 --> 00:52:40,863
Speaker 0: Right.

801
00:52:40,944 --> 00:52:44,240
Speaker 0: Those are the that's the number one use consumers are aware of.

802
00:52:44,380 --> 00:52:44,581
Speaker 0: Right.

803
00:52:44,621 --> 00:52:45,584
Speaker 0: For all this nonsense,

804
00:52:46,968 --> 00:53:07,040
Speaker 1: a dangerous use that is coming more and more to the forefront, but already has existed in a practicable form for a long time with text is having something that is able to write novel content in the recognizable style of a known person, but without any input from said person.

805
00:53:07,581 --> 00:53:09,668
Speaker 1: You could get this to write an article.

806
00:53:10,090 --> 00:53:13,120
Speaker 1: that sounds a lot like how I write an article.

807
00:53:13,700 --> 00:53:17,099
Speaker 1: Look, if you read it, you think, wow, Rimm wrote that, but I didn't write it.

808
00:53:18,942 --> 00:53:19,103
Speaker 0: Mm hmm.

809
00:53:19,605 --> 00:53:25,880
Speaker 0: If you just if you just take all the Stephen King books, throw them into this thing and then say, write a Stephen King book and you'll be surprised.

810
00:53:26,600 --> 00:53:30,160
Speaker 1: Yeah, it comes out, especially if you look at some later Stephen King books.

811
00:53:30,460 --> 00:53:31,680
Speaker 1: But I digress.

812
00:53:33,143 --> 00:53:34,840
Speaker 0: Maybe those were written by now.

813
00:53:34,960 --> 00:53:49,908
Speaker 1: The other ethical side of this coin is I absolutely believe we will build machine intelligences that are equivalent to superior that to or at least analogous to human intelligence.

814
00:53:49,928 --> 00:53:50,760
Speaker 1: Like we're not special.

815
00:53:50,820 --> 00:53:55,720
Speaker 1: We're just a biological machine, but I don't think we're anywhere near that point so far.

816
00:53:55,881 --> 00:53:57,160
Speaker 0: Not in our lifetime, probably.

817
00:53:57,862 --> 00:54:00,540
Speaker 1: And of course, at least not as we approach that point.

818
00:54:01,261 --> 00:54:01,702
Speaker 1: There are.

819
00:54:01,743 --> 00:54:09,243
Speaker 1: there is a huge body of ethics around what is ethical to do to and with such a intelligence.

820
00:54:09,543 --> 00:54:14,560
Speaker 1: But we are so, so impossibly far from that technology.

821
00:54:15,220 --> 00:54:15,320
Speaker 0: Right.

822
00:54:15,340 --> 00:54:22,158
Speaker 0: Then what we have now is basically just a giant spreadsheet and then fancy spreadsheet lookups.

823
00:54:22,500 --> 00:54:22,600
Speaker 0: Right.

824
00:54:22,661 --> 00:54:28,919
Speaker 0: It's like there's no moral or ethical problem with turning off the computer or deleting the spreadsheet or anything like that.

825
00:54:29,120 --> 00:54:29,260
Speaker 0: Right.

826
00:54:29,782 --> 00:54:36,980
Speaker 0: But there are lots and lots and lots of ethical and moral issues regards to using the giant spreadsheet.

827
00:54:37,502 --> 00:54:37,804
Speaker 0: Right.

828
00:54:37,824 --> 00:54:42,591
Speaker 0: You know, you've seen plenty of people talking about how like image recognition is racist.

829
00:54:42,632 --> 00:54:43,219
Speaker 0: That's true.

830
00:54:43,940 --> 00:54:47,440
Speaker 0: You know, automatic like photo editing stuff racist.

831
00:54:47,700 --> 00:54:55,560
Speaker 1: Also true, because the data encodes the implicit biases and dimensionality of our existing flawed society.

832
00:54:55,821 --> 00:54:56,062
Speaker 0: Right.

833
00:54:56,403 --> 00:55:02,352
Speaker 0: This spreadsheet was filled with data from humanity and therefore the flaws of humanity.

834
00:55:02,698 --> 00:55:02,820
Speaker 0: Right.

835
00:55:02,920 --> 00:55:06,800
Speaker 0: And the systems we already have make their way into the system.

836
00:55:06,941 --> 00:55:09,720
Speaker 0: So it's like, you know, they don't just make their way into the system.

837
00:55:09,921 --> 00:55:11,460
Speaker 1: They define the system.

838
00:55:11,621 --> 00:55:16,200
Speaker 1: And then because of the scale of this computing, they amplify those flaws.

839
00:55:17,382 --> 00:55:27,399
Speaker 0: Yeah, it's like you could be like, you know, let's say you wanted to have something that like the computer helped you to, you know, find the like, you know, decide to give someone mortgages or not.

840
00:55:27,541 --> 00:55:31,603
Speaker 0: And it's like, well, the people who've been deciding to give mortgages or not all this time are racist.

841
00:55:32,045 --> 00:55:36,625
Speaker 0: So if you fill it with the history of all mortgages, hand it out forever.

842
00:55:36,645 --> 00:55:38,060
Speaker 0: And guess what?

843
00:55:38,442 --> 00:55:39,699
Speaker 0: It's going to keep being racist.

844
00:55:39,901 --> 00:55:40,123
Speaker 0: Right.

845
00:55:40,204 --> 00:55:41,939
Speaker 0: And only white people are going to get mortgage.

846
00:55:42,581 --> 00:55:43,484
Speaker 1: Yes.

847
00:55:43,504 --> 00:55:47,982
Speaker 1: So and this leads into like you remember a way back, we barely talked about some geek guys.

848
00:55:48,023 --> 00:56:01,481
Speaker 1: It was a long time ago, but Google Assistant, they announced a feature long time ago where Google Assistant could if you like used an app or like clicked on a restaurant and Google Maps and you wanted to make a reservation or like put in an order.

849
00:56:02,004 --> 00:56:21,540
Speaker 1: And the only way you could interact with that business was via a phone number like they didn't have like a grubhub or like a web form to fill out or anything, then it would call that business and a human sounding voice would just interact with the person on the other end of the line to answer your question.

850
00:56:21,641 --> 00:56:22,920
Speaker 1: Like, are you open today?

851
00:56:23,141 --> 00:56:26,698
Speaker 1: It's Memorial Day and get the answer and give it back to you.

852
00:56:27,462 --> 00:56:29,660
Speaker 1: And that opened a whole shitstorm.

853
00:56:29,740 --> 00:56:36,540
Speaker 1: On one hand, businesses were afraid that they would be inundated with thousands and thousands of calls that were automated.

854
00:56:37,940 --> 00:56:42,583
Speaker 1: Luddites were afraid that this would replace human communication, even though I kind of wanted to.

855
00:56:42,623 --> 00:56:46,364
Speaker 1: Like, I don't want to waste a human's time calling them to make a dinner reservation.

856
00:56:46,424 --> 00:56:49,000
Speaker 1: Like, I don't I don't want to waste humans sorting that out.

857
00:56:49,181 --> 00:56:51,020
Speaker 1: We could sort that out easily with technology.

858
00:56:51,720 --> 00:56:52,320
Speaker 1: It could be an interesting.

859
00:56:52,580 --> 00:56:54,239
Speaker 0: Get a better website and it won't have to call you.

860
00:56:54,621 --> 00:56:55,239
Speaker 0: Yeah, come on.

861
00:56:55,320 --> 00:56:55,723
Speaker 1: Exactly.

862
00:56:55,784 --> 00:56:57,760
Speaker 1: But most businesses don't have a better website.

863
00:56:58,000 --> 00:57:01,740
Speaker 1: I mean, that's why things like Grubhub exist in the first place, unfortunately.

864
00:57:02,460 --> 00:57:03,160
Speaker 0: Get your shit together.

865
00:57:03,301 --> 00:57:03,542
Speaker 1: Yep.

866
00:57:03,963 --> 00:57:12,480
Speaker 1: But in the end, Google kind of backed away from a lot of this and they agreed to, among other things, if Google Assistant calls a business, it will tell you it's a robot.

867
00:57:12,860 --> 00:57:16,479
Speaker 1: It will like explicitly say what it is before it interacts with you.

868
00:57:17,361 --> 00:57:21,440
Speaker 1: And that opened a whole bunch of ethical concerns that are still not fully explored.

869
00:57:21,862 --> 00:57:25,120
Speaker 1: And that is like baby's first use of this kind of technology.

870
00:57:26,302 --> 00:57:26,583
Speaker 0: Right.

871
00:57:27,205 --> 00:57:33,465
Speaker 0: So regarding the actual news story guy, right, with the chatbot that is not sentient, but they believe it is.

872
00:57:33,485 --> 00:57:44,960
Speaker 0: the major ethical concern that I see with that particular thing is that, you know, even though this person involved is, you know, I guess we'll keep using the word interesting.

873
00:57:45,121 --> 00:57:45,344
Speaker 0: Yeah.

874
00:57:45,404 --> 00:57:46,171
Speaker 0: Right.

875
00:57:46,252 --> 00:57:47,100
Speaker 0: Unique individual.

876
00:57:48,802 --> 00:57:51,800
Speaker 0: They are convinced that it's sentient, right?

877
00:57:52,562 --> 00:57:54,618
Speaker 1: If it's all that, I would argue so.

878
00:57:55,441 --> 00:58:01,020
Speaker 1: This person may have failed what I'm going to now call the reverse Turing test.

879
00:58:02,021 --> 00:58:16,340
Speaker 0: Maybe, but regardless, this bot thing, right, that is not sentient is software that is capable of convincing some percentage of the world population that it is sentient.

880
00:58:16,740 --> 00:58:29,759
Speaker 1: So if I loaded up the history of everything I've ever typed into this thing, say someone else did it nefarious, they could probably make a discord bot that could talk like if Scott messaged it, it would respond and very plausibly seem like me.

881
00:58:31,041 --> 00:58:37,620
Speaker 0: Yeah, but I mean, this that in and of itself could be used for nefarious things.

882
00:58:37,820 --> 00:58:38,504
Speaker 1: I mean, look at what.

883
00:58:38,544 --> 00:58:38,986
Speaker 1: look at.

884
00:58:39,006 --> 00:58:43,080
Speaker 1: look at the online discourse right now, the massive sea of disinformation, Russian bots everywhere.

885
00:58:43,641 --> 00:58:47,613
Speaker 1: There are a lot of people who are fooled by bots on social media.

886
00:58:47,794 --> 00:58:56,463
Speaker 1: already when other people will look at one sentence from one of those bots and their brain immediately says, wow, that's a bot.

887
00:58:57,005 --> 00:59:01,420
Speaker 1: But this technology makes more people susceptible to being fooled.

888
00:59:02,600 --> 00:59:08,440
Speaker 0: So here's a very you know, I think the I guess the saving grace is that it's very expensive to run this bot, at least for now.

889
00:59:09,000 --> 00:59:15,779
Speaker 0: Now, but but here's here's a really simple case that's just so obvious of how someone could use this for evil.

890
00:59:16,301 --> 00:59:16,542
Speaker 0: Right.

891
00:59:16,904 --> 00:59:22,500
Speaker 0: You know, there's all those people who call up old people and try to trick them into thinking, you know, tech support or something.

892
00:59:22,580 --> 00:59:23,353
Speaker 0: They see all their money.

893
00:59:23,556 --> 00:59:23,780
Speaker 0: Right.

894
00:59:24,581 --> 00:59:26,840
Speaker 0: And they have these call centers, you know, mostly in India.

895
00:59:27,182 --> 00:59:29,920
Speaker 0: You can watch on YouTube all those channels or they bust those people.

896
00:59:30,260 --> 00:59:30,762
Speaker 0: Right.

897
00:59:30,782 --> 00:59:38,320
Speaker 0: And it's like the call centers are full of real humans right in India, you know, calling up, you know, citizens of other countries trying to scam them.

898
00:59:38,400 --> 00:59:44,964
Speaker 0: And while this is effectively a mechanical bot, if you could get the bot to make those calls, you could get.

899
00:59:45,284 --> 00:59:55,140
Speaker 0: now one person could scam the whole world, keep all the money and not have to hire any call center people or rent any office space.

900
00:59:55,740 --> 00:59:55,982
Speaker 0: Right.

901
00:59:56,003 --> 00:59:57,680
Speaker 0: Same thing that happened with spam.

902
00:59:58,440 --> 01:00:03,743
Speaker 1: Spam appeared because you could just automate sending ridiculous emails to millions and millions of people.

903
01:00:04,487 --> 01:00:07,604
Speaker 1: And if only one percent of them fall for it, you still make a ton of money.

904
01:00:07,624 --> 01:00:08,480
Speaker 1: It costs you nothing.

905
01:00:08,842 --> 01:00:10,800
Speaker 1: It's way cheaper than mass mailers.

906
01:00:10,860 --> 01:00:12,660
Speaker 1: That used to be the way scams propagated.

907
01:00:13,580 --> 01:00:13,681
Speaker 0: Right.

908
01:00:13,701 --> 01:00:15,880
Speaker 0: And then how are the anti scam people?

909
01:00:16,220 --> 01:00:16,647
Speaker 0: How are they?

910
01:00:16,708 --> 01:00:17,339
Speaker 0: what are they going to do?

911
01:00:17,723 --> 01:00:17,845
Speaker 0: How?

912
01:00:17,885 --> 01:00:18,776
Speaker 0: what can they do about it?

913
01:00:19,120 --> 01:00:19,302
Speaker 0: Right.

914
01:00:19,383 --> 01:00:21,600
Speaker 0: It's like, you know, it's going to be really hard.

915
01:00:21,881 --> 01:00:23,792
Speaker 0: It's not like you can call up, you know, the.

916
01:00:23,953 --> 01:00:25,300
Speaker 0: I can't really be intimidated.

917
01:00:26,141 --> 01:00:34,260
Speaker 1: Imagine these eyes start texting every old person in America pretending to be their son and saying that and I'm like, I'm in jail.

918
01:00:34,561 --> 01:00:35,719
Speaker 1: I need 50 bucks for bail.

919
01:00:36,261 --> 01:00:37,106
Speaker 1: Grandpa, help me.

920
01:00:37,871 --> 01:00:39,079
Speaker 0: And it actually believed.

921
01:00:39,442 --> 01:00:39,603
Speaker 0: Right.

922
01:00:39,623 --> 01:00:42,080
Speaker 0: It actually talked like their grandchild.

923
01:00:42,441 --> 01:00:42,662
Speaker 0: Right.

924
01:00:42,722 --> 01:00:48,419
Speaker 0: Not just like a grandchild, but there's specifically like maybe new things about their grandchild.

925
01:00:48,540 --> 01:01:00,007
Speaker 1: I mean, I have already had to block people I otherwise liked on social media networks like Twitter because I would see them in my mentions arguing with bots like, how do you not see?

926
01:01:00,028 --> 01:01:00,639
Speaker 1: it's not a bot?

927
01:01:01,100 --> 01:01:07,099
Speaker 1: The better this technology gets, the more people will fail that reverse Turing test and interact with bots they think are people.

928
01:01:08,763 --> 01:01:10,980
Speaker 0: I think you could also see situations, right.

929
01:01:11,380 --> 01:01:17,068
Speaker 0: You know, how many online platforms, right, whether it's a game or a social network or whatever.

930
01:01:17,088 --> 01:01:17,190
Speaker 0: Right.

931
01:01:18,440 --> 01:01:19,465
Speaker 0: You know, it's like you have.

932
01:01:19,666 --> 01:01:24,360
Speaker 0: if you try to start something these days, it's really hard because like, you know, it's like I'll make a video platform.

933
01:01:24,540 --> 01:01:25,088
Speaker 0: It's like YouTube exists.

934
01:01:25,108 --> 01:01:25,939
Speaker 0: What are you going to do?

935
01:01:26,041 --> 01:01:26,182
Speaker 0: Right.

936
01:01:26,202 --> 01:01:26,565
Speaker 0: You need to.

937
01:01:26,868 --> 01:01:28,280
Speaker 0: you got to have the network effect.

938
01:01:28,380 --> 01:01:28,522
Speaker 0: Right.

939
01:01:28,542 --> 01:01:29,819
Speaker 0: Get this user base up.

940
01:01:30,421 --> 01:01:30,702
Speaker 0: Right.

941
01:01:30,722 --> 01:01:34,020
Speaker 0: To then get your, you know, to have your platform be worth using.

942
01:01:34,060 --> 01:01:38,302
Speaker 0: You can make the best video site in the universe, but it's not the software that makes it great.

943
01:01:38,443 --> 01:01:40,680
Speaker 0: It's all the users and the content that they're uploading.

944
01:01:41,083 --> 01:01:42,279
Speaker 0: And YouTube already has that.

945
01:01:42,401 --> 01:01:44,020
Speaker 0: So it's like, you know, right.

946
01:01:44,320 --> 01:01:44,883
Speaker 0: Or it's not?

947
01:01:45,043 --> 01:01:46,128
Speaker 0: it's not the.

948
01:01:46,208 --> 01:01:50,320
Speaker 0: I can make a Twitter really easily, but it doesn't have all the people that are on Twitter.

949
01:01:50,480 --> 01:01:51,303
Speaker 0: So whatever.

950
01:01:52,065 --> 01:02:04,360
Speaker 0: But what if I artificially filled up my brand new app, right, with a bunch of AI generated garbage, but it was actually legitimate content that people found interesting and believed was real people.

951
01:02:04,901 --> 01:02:16,882
Speaker 0: And that caused I basically fake the network effect to give myself fake customers to defraud investors into thinking I have users and that I don't make a social media network.

952
01:02:17,584 --> 01:02:21,480
Speaker 1: Dolly version 2.0 is putting great illustrations on it.

953
01:02:21,880 --> 01:02:31,280
Speaker 1: And then your GPT 10,000 algorithm has a million people risk sharing and resharing and interacting with and responding to that original bot.

954
01:02:31,920 --> 01:02:34,317
Speaker 0: You got your Twitch channel and you're trying to get whatever status.

955
01:02:34,942 --> 01:02:35,084
Speaker 0: Right.

956
01:02:35,104 --> 01:02:36,740
Speaker 0: You don't have enough users, but you could do it.

957
01:02:37,204 --> 01:02:38,378
Speaker 0: The people do the botting.

958
01:02:38,640 --> 01:02:39,125
Speaker 0: But guess what?

959
01:02:39,165 --> 01:02:40,720
Speaker 0: The botting, you know, is kind of obvious.

960
01:02:40,961 --> 01:02:48,990
Speaker 0: But what if all those bots are chatting in Twitch chat in a human believable way and nobody can tell that those are not Twitch?

961
01:02:49,090 --> 01:02:51,860
Speaker 1: and we are already seeing this today.

962
01:02:52,401 --> 01:02:58,679
Speaker 1: It's just if you listen to Geek Nights, you're probably the kind of person who recognizes when a bot comes into the chat.

963
01:02:59,241 --> 01:03:02,180
Speaker 1: But a lot of people already today don't.

964
01:03:03,321 --> 01:03:03,502
Speaker 0: Nope.

965
01:03:04,144 --> 01:03:21,565
Speaker 0: And so, yeah, the fact that the bot is now advanced to the level where the AI researcher person, whatever issues they may have, believes it's sentient, means it's too advanced to probably be safe for anyone to be using without oversight and regulation or at all.

966
01:03:21,766 --> 01:03:23,293
Speaker 0: Like, what do you what's even?

967
01:03:23,373 --> 01:03:24,760
Speaker 0: what's the legitimate use of this thing?

968
01:03:25,441 --> 01:03:25,561
Speaker 0: Right.

969
01:03:25,581 --> 01:03:30,380
Speaker 1: It's like, yeah, I so I can so forget ethics.

970
01:03:30,683 --> 01:03:32,160
Speaker 1: Here are some fascinating uses.

971
01:03:32,761 --> 01:03:38,360
Speaker 1: One, scaling out various types of entrapment schemes from law enforcement at scale, like.

972
01:03:38,360 --> 01:03:39,163
Speaker 1: that's a way to do it.

973
01:03:39,665 --> 01:03:55,000
Speaker 1: They go out to find people who are prone to, say, engaging in right wing extremism and then basically act like you're basically pulling them into a fake network of similar minded people when actually it's all just bots until eventually they do something.

974
01:03:55,160 --> 01:03:55,800
Speaker 0: Why don't you do that then?

975
01:03:56,100 --> 01:03:57,598
Speaker 1: I mean, you could probably could.

976
01:03:58,064 --> 01:03:59,059
Speaker 0: I'm saying why don't you do that?

977
01:03:59,644 --> 01:04:00,439
Speaker 1: Because I got a day job.

978
01:04:00,680 --> 01:04:01,610
Speaker 0: Good idea.

979
01:04:01,651 --> 01:04:02,500
Speaker 1: I'm not saying it's a good idea.

980
01:04:02,580 --> 01:04:04,759
Speaker 1: I'm saying it's these are practicable ideas.

981
01:04:06,923 --> 01:04:13,559
Speaker 1: OK, an example of an app you could sell, sell an app where you load it with all your own information and you use it.

982
01:04:14,080 --> 01:04:18,658
Speaker 1: You pick people on your contact list, like, say, you're the kind of person who doesn't like to talk to your mom.

983
01:04:19,561 --> 01:04:26,398
Speaker 1: You tell it to maintain your relationship with your mom and just give you a feed of updates of what your mom's up to.

984
01:04:26,761 --> 01:04:30,398
Speaker 1: But it talks to your mom on a day to day basis and she doesn't realize it.

985
01:04:33,006 --> 01:04:33,859
Speaker 0: It seems like a bad idea.

986
01:04:34,282 --> 01:04:35,540
Speaker 1: I didn't say it's a good idea.

987
01:04:35,560 --> 01:04:39,440
Speaker 1: I said it's a practicable idea that I think you can make money selling such an app.

988
01:04:39,520 --> 01:04:40,818
Speaker 1: And I think a lot of people would buy it.

989
01:04:42,601 --> 01:04:44,979
Speaker 0: Yeah, and then it tells your mom something not great.

990
01:04:45,540 --> 01:04:49,293
Speaker 1: Yeah, well, that's when the person who wrote who made the app.

991
01:04:49,915 --> 01:04:55,239
Speaker 1: one starts deep faking you and all your friends and scams them for money and to blackmail you.

992
01:04:57,340 --> 01:05:06,280
Speaker 0: So this technology, you also have to realize that, like, you know, if you any such at least with the current levels of technology, performance wise, right.

993
01:05:06,961 --> 01:05:10,740
Speaker 0: Such a bot, if you were to use it in that way, would have to run in the cloud.

994
01:05:10,800 --> 01:05:12,519
Speaker 0: You could run the whole thing on someone's job.

995
01:05:13,060 --> 01:05:13,201
Speaker 0: Right.

996
01:05:13,562 --> 01:05:23,340
Speaker 0: So you're going to have to send all your chats and all your mom's chats, all your full mom conversation to this cloud service unencrypted so that they can process it.

997
01:05:23,682 --> 01:05:23,924
Speaker 0: Oh, yeah.

998
01:05:23,945 --> 01:05:25,560
Speaker 0: And now you're basically right.

999
01:05:25,680 --> 01:05:39,960
Speaker 0: So now whoever runs this service, even if the service itself and what it does for the customer, we agree is legitimate and ethical, you now have a company that has access to all of these private conversations in an unencrypted fashion.

1000
01:05:40,701 --> 01:05:41,002
Speaker 0: Right.

1001
01:05:41,363 --> 01:05:48,540
Speaker 0: And even if they have a good privacy policy and abide by it, it is dangerous to have all of that data in a place.

1002
01:05:48,620 --> 01:05:48,922
Speaker 0: Yep.

1003
01:05:49,083 --> 01:05:52,280
Speaker 1: And there are technological solutions to that problem.

1004
01:05:52,541 --> 01:06:00,519
Speaker 1: But if the Internet of Things has taught us anything, no controls will ever be put on top of pretty much anything anyone builds.

1005
01:06:03,461 --> 01:06:06,120
Speaker 0: Anyway, do we have anything else to say because I'm hungry?

1006
01:06:06,845 --> 01:06:07,880
Speaker 1: I think we can wrap up there.

1007
01:06:08,060 --> 01:06:11,119
Speaker 1: So we should be on our pretty much regular geek night schedule for the foreseeable future.

1008
01:06:12,963 --> 01:06:16,399
Speaker 1: unless the Rangers make the Stanley Cup again in the foreseeable future.

1009
01:06:16,600 --> 01:06:17,919
Speaker 0: It's not going to happen for another 12 months.

1010
01:06:18,401 --> 01:06:18,483
Speaker 0: Yeah.

1011
01:06:18,625 --> 01:06:19,619
Speaker 0: If it happens at all.

1012
01:06:20,040 --> 01:06:20,361
Speaker 0: Right.

1013
01:06:20,381 --> 01:06:27,260
Speaker 1: So this has been Geek Nights with Rim and Scott.

1014
01:06:27,440 --> 01:06:32,563
Speaker 1: Special thanks to DJ Pretzel for the opening music, Kat Lee for web design and Brando K for the logos.

1015
01:06:32,764 --> 01:06:37,778
Speaker 0: Be sure to visit our website at FrontRowCrew.com for show notes, discussion, news and more.

1016
01:06:38,100 --> 01:06:45,439
Speaker 1: Remember, Geek Nights is not one, but four different shows, SciTech Mondays, Gaming Tuesdays, Anime Comic Wednesdays and Indiscriminate Thursdays.

1017
01:06:45,860 --> 01:06:48,996
Speaker 0: Geek Nights is distributed under a Creative Commons Attribution 3.0 license.

1018
01:06:50,280 --> 01:06:53,320
Speaker 0: Geek Nights is recorded live with no studio and no audience.

1019
01:06:53,520 --> 01:06:56,420
Speaker 0: But unlike those other late shows, it's actually recorded at night.

1020
01:06:56,740 --> 01:07:05,800
Speaker 1: And the Patreon patrons for this episode of Geek Nights are Alan Joyce, Link Eiji, Dread Loony, Tenebrae and a bunch of people who really don't want me to say their names.

1021
01:07:06,220 --> 01:07:16,823
Speaker 1: Chris Adon, Chris Reimer, Clinton, Walton, Dex, Finn, Kishaya85, Rebecca Dunn, Review Mad Bull, 34 Cowards, Sam Erickson, Sharon Von Hurl, Stop All the Downloading, Taylor Braun.

1022
01:07:16,843 --> 01:07:18,800
Speaker 1: The next station is DuPont, DuPont Station.

1023
01:07:19,523 --> 01:07:21,240
Speaker 1: And several people whose names I will not say.

1024
01:07:21,640 --> 01:07:23,520
Speaker 1: I simply leave you with an old classic.

1025
01:07:27,840 --> 01:07:30,280
Speaker 0: Good afternoon and welcome to Brandy Park.

1026
01:07:30,480 --> 01:07:38,000
Speaker 0: And you join us just as the competitors are running out onto the field on this lovely winter's afternoon with the going firm underfoot and very little sign of rain.

1027
01:07:38,460 --> 01:07:44,699
Speaker 1: And it looks as though we're in for a splendid afternoon sport on this, the 127th Upper Class Twit of the Year show.

1028
01:07:45,260 --> 01:07:48,740
Speaker 0: And there's a big crowd here today to see these prize idiots in action.

1029
01:07:49,080 --> 01:07:50,159
Speaker 0: Vivian Smith by Smith.

1030
01:07:50,620 --> 01:07:52,719
Speaker 0: He's in the Grenadier Guards and he can count up to four.

1031
01:07:53,240 --> 01:07:54,620
Speaker 0: Simon Zink Trumpet Harris.

1032
01:07:54,721 --> 01:07:57,300
Speaker 0: He's an old Etonian and married to a very attractive table lamp.

1033
01:07:57,700 --> 01:08:00,100
Speaker 0: Nigel Incubator Jones, his best friend as a tree.

1034
01:08:00,421 --> 01:08:02,399
Speaker 0: And in his spare time, he's a stockbroker.

1035
01:08:02,960 --> 01:08:04,220
Speaker 0: Gervais Brooke-Hamster.

1036
01:08:04,440 --> 01:08:07,600
Speaker 0: He's in the wine trade and his father uses him as a waste paper basket.

1037
01:08:08,280 --> 01:08:11,100
Speaker 0: And finally, Oliver Singin-Mollusk, another old Etonian.

1038
01:08:11,320 --> 01:08:13,720
Speaker 0: His father was a cabinet minister and his mother won the Derby.

1039
01:08:13,860 --> 01:08:16,380
Speaker 0: And he's thought by many to be this year's Outstanding Twit.

1040
01:08:17,439 --> 01:08:19,819
Speaker 0: And now the Twits are moving up to the starting line.

1041
01:08:19,899 --> 01:08:21,859
Speaker 0: And any moment now, they'll be under starters orders.

1042
01:08:22,823 --> 01:08:24,540
Speaker 0: I'm afraid they're facing the wrong way at the moment.

1043
01:08:24,540 --> 01:08:26,038
Speaker 0: But the starter will soon sort this out.

1044
01:08:26,581 --> 01:08:28,800
Speaker 0: And any moment now, we're going to have the big off.

1045
01:08:28,899 --> 01:08:30,580
Speaker 0: This is always a tense moment.

1046
01:08:31,903 --> 01:08:32,799
Speaker 0: And they're off!

