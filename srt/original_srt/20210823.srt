1
00:00:07,980 --> 00:00:10,165
Speaker 0: It's Monday, August 23rd, 2021.

2
00:00:10,165 --> 00:00:10,406
Speaker 0: I'm rim.

3
00:00:10,446 --> 00:00:12,191
Speaker 0: I'm Scott and this is geek nights.

4
00:00:12,251 --> 00:00:21,979
Speaker 0: tonight We're talking about Apple see Sam and privacy, you know, one of the biggest stories in the last few weeks.

5
00:00:23,142 --> 00:00:25,614
Speaker 1: We could actually done the main bit on computer sleeping.

6
00:00:26,519 --> 00:00:30,254
Speaker 0: Oh We should save that for another one Because I already set all this up.

7
00:00:31,303 --> 00:00:31,989
Speaker 0: Write it down.

8
00:00:32,432 --> 00:00:33,339
Speaker 0: I will write it down.

9
00:00:35,130 --> 00:00:37,638
Speaker 0: All right Let's do this.

10
00:00:38,761 --> 00:00:44,669
Speaker 0: So we went down to a private beach house with some friends to get away for a week And then we didn't have to race.

11
00:00:44,750 --> 00:00:46,940
Speaker 0: like it was easy to get home before the hurricane.

12
00:00:47,443 --> 00:00:57,759
Speaker 0: but that tropical storm escalated pretty quickly and Actually hit New York and if there's some I say fun flooding because the flooding in New York was not like super bad.

13
00:00:57,799 --> 00:01:01,452
Speaker 0: in the end We actually got pretty lucky and the storm petered out and hit other places worse.

14
00:01:01,653 --> 00:01:02,857
Speaker 1: It was just a whole bunch of rain.

15
00:01:03,198 --> 00:01:06,291
Speaker 0: Yep It was enough rain though to where I thought.

16
00:01:06,311 --> 00:01:08,360
Speaker 0: you know, it would be cool like what I've done in the past.

17
00:01:08,761 --> 00:01:12,095
Speaker 0: I'll go out for a run during the tropical storm or the hurricane like it won't be.

18
00:01:12,135 --> 00:01:16,452
Speaker 0: that sounds like a bad idea But okay, I mean unless the winds super bad like that's fine.

19
00:01:16,552 --> 00:01:18,018
Speaker 0: If it's warm like what I'm wet.

20
00:01:18,078 --> 00:01:18,540
Speaker 0: who cares?

21
00:01:18,600 --> 00:01:23,060
Speaker 1: Yeah, but it's not as bad of an idea as the people who go like surfing when the hurricane is coming, right?

22
00:01:23,180 --> 00:01:32,720
Speaker 0: But that is the best surfing Yeah, but it's also the deadliest Yeah, you do taking your life in your own hands in that case like like the one who's always in your own hands.

23
00:01:33,703 --> 00:01:41,680
Speaker 1: Sometimes it's in someone else's hands Sure, but it's still in your own hands, even when it's in someone else's hands, but so we get home.

24
00:01:41,781 --> 00:01:43,028
Speaker 0: I think maybe I'll go run.

25
00:01:43,089 --> 00:01:50,295
Speaker 0: I got on the balcony and it is raining Hard enough to or even I am like, yeah, I'm just not gonna go outside at all.

26
00:01:50,315 --> 00:01:51,539
Speaker 0: You know, it's not gonna work out.

27
00:01:52,282 --> 00:01:52,964
Speaker 1: No surprise.

28
00:01:53,466 --> 00:01:59,162
Speaker 0: I've biked home in similar rain, but that's different from doing like an hour-long run Mm-hmm.

29
00:01:59,884 --> 00:02:00,766
Speaker 0: So yeah, there is.

30
00:02:01,007 --> 00:02:03,575
Speaker 0: we missed a week of shows because I was out of town.

31
00:02:03,976 --> 00:02:08,699
Speaker 0: we came back for a brief One show week and we went out of town again.

32
00:02:09,222 --> 00:02:17,748
Speaker 0: we're finally back and with pretty much on a regular schedule for the foreseeable future and of course a ton of Honestly really important big deal.

33
00:02:17,788 --> 00:02:19,695
Speaker 0: tech news happened the one time.

34
00:02:19,735 --> 00:02:21,039
Speaker 0: We're not around to talk about it.

35
00:02:22,245 --> 00:02:22,649
Speaker 1: It happened.

36
00:02:22,689 --> 00:02:28,554
Speaker 1: I think before we left We just didn't have a Monday show between then exactly between the last Monday show we did.

37
00:02:28,654 --> 00:02:36,336
Speaker 0: and now on three Major stories happened and I'm not allowed to call for a tech news roundup.

38
00:02:36,376 --> 00:02:39,192
Speaker 1: I'm sure more stories It's more major stories than that happened.

39
00:02:39,253 --> 00:02:40,240
Speaker 0: But oh, yeah a bunch happened.

40
00:02:40,280 --> 00:02:42,636
Speaker 0: But these three were the biggest three that we got something to say.

41
00:02:42,656 --> 00:02:48,060
Speaker 0: Yep So instead of doing a normal main bit, well, basically the news we have the most to say about became the main bit.

42
00:02:48,120 --> 00:02:49,000
Speaker 0: We'll see how that goes.

43
00:02:50,602 --> 00:02:52,760
Speaker 0: So first things first, we'll dive right into it.

44
00:02:53,887 --> 00:02:58,062
Speaker 0: Only fans Basically pulled a tumbler Right.

45
00:02:58,122 --> 00:02:58,984
Speaker 1: So only fans.

46
00:02:59,024 --> 00:03:16,489
Speaker 1: I'm sure you know what this website is, but just in case you didn't it's a social network type site Where you pay to follow someone and most of the people that you're going to be paying to follow our porn stars or sex workers and That you know, it was.

47
00:03:16,509 --> 00:03:18,400
Speaker 1: it was a site that a lot of people used.

48
00:03:18,561 --> 00:03:21,517
Speaker 1: There's a lot of competition for it now that I've seen some people using.

49
00:03:21,618 --> 00:03:23,063
Speaker 1: but yeah It was it's a thing.

50
00:03:23,705 --> 00:03:26,795
Speaker 1: It was a thing and it's soon gonna not be a thing.

51
00:03:26,815 --> 00:03:34,195
Speaker 0: Yep, and Effectively with a bunch of caveats and a lot more complexity than we really want to get into.

52
00:03:34,617 --> 00:03:43,240
Speaker 0: they're banning Pornographic content the likes of which the most people who use only fans use it for that is what they seek out.

53
00:03:43,300 --> 00:03:44,686
Speaker 0: That is why that platform grew.

54
00:03:44,746 --> 00:03:45,670
Speaker 0: That's why it was there.

55
00:03:45,690 --> 00:03:48,039
Speaker 0: That was the primary reason people use it.

56
00:03:48,882 --> 00:03:49,484
Speaker 1: Yeah, a lot of.

57
00:03:49,544 --> 00:03:52,393
Speaker 1: I saw a lot of jokes on Twitter that were the equivalent of like.

58
00:03:52,954 --> 00:03:55,664
Speaker 1: oh in the news today orchard stop Selling apples.

59
00:03:55,745 --> 00:03:56,327
Speaker 0: It's help.

60
00:03:57,031 --> 00:03:59,000
Speaker 0: What's interesting is only fans.

61
00:03:59,800 --> 00:04:01,424
Speaker 1: McDonald's stop selling burgers.

62
00:04:01,544 --> 00:04:02,787
Speaker 0: the company acted like.

63
00:04:02,807 --> 00:04:09,383
Speaker 0: they were surprised that this announcement was met with people just dropping their platform Like they were.

64
00:04:09,584 --> 00:04:18,103
Speaker 0: they seemed surprised at the huge amount of vitriol anger and basically the fact that their business is done now Like they're just destroyed Mm-hmm.

65
00:04:18,644 --> 00:04:32,171
Speaker 1: But yeah, so I Mean, it's really surprised me and that usually the pattern of a Silicon Valley business is to start out completely ignoring laws Regulations rules of any kind.

66
00:04:32,191 --> 00:04:37,653
Speaker 1: right the way, you know You see uber continuing to do uber way of trying to change the rules.

67
00:04:37,733 --> 00:04:39,460
Speaker 1: In fact, they went far above and beyond.

68
00:04:41,262 --> 00:04:45,540
Speaker 1: That's another news that their rule change was ruled unconstitutional in the California court.

69
00:04:45,661 --> 00:04:48,960
Speaker 1: But usually you see a crunchyroll style pattern, right?

70
00:04:49,020 --> 00:05:00,660
Speaker 1: It's like start out with just a pirate anime website and then become legitimate anime website and then eventually they're bought into Funimation slash Sony and it doesn't matter anymore that they were once committing crimes, right?

71
00:05:00,900 --> 00:05:05,497
Speaker 1: So and many many Silicon Valley businesses operate this way.

72
00:05:05,598 --> 00:05:06,180
Speaker 1: just you know.

73
00:05:06,200 --> 00:05:07,195
Speaker 1: They start off that.

74
00:05:07,236 --> 00:05:20,420
Speaker 1: you know You can't afford you if you try to start any business even a small normal little business like say a restaurant And you try to follow every single fucking rule right right out of the gate from day one.

75
00:05:20,801 --> 00:05:25,474
Speaker 1: You know paying every tax getting every inspection, you know everything.

76
00:05:25,494 --> 00:05:27,800
Speaker 1: you're the costs to start the business.

77
00:05:27,880 --> 00:05:28,963
Speaker 1: We'll just ruin the business.

78
00:05:29,004 --> 00:05:34,460
Speaker 0: Yep, pretty much every convention you've ever been to started running like directly illegally.

79
00:05:35,547 --> 00:05:38,004
Speaker 0: Yeah, every day Even packs.

80
00:05:39,027 --> 00:05:49,791
Speaker 1: I can think of very few businesses that start just doing everything completely by the book 100% kosher and succeeding and going far.

81
00:05:50,051 --> 00:05:51,716
Speaker 1: It just doesn't happen that often right?

82
00:05:51,736 --> 00:05:51,916
Speaker 1: so?

83
00:05:51,936 --> 00:05:55,540
Speaker 1: and but in Silicon Valley It's to a greater extreme of the illegalness, right?

84
00:05:55,640 --> 00:05:59,353
Speaker 1: Just blatant disregard for all laws to get your business off the ground.

85
00:05:59,373 --> 00:06:05,308
Speaker 1: just straight capitalist extortion Exploitation not extortion and then whatnot.

86
00:06:05,348 --> 00:06:09,560
Speaker 1: So that's what only fans did just you know, ignoring all the porn laws, right?

87
00:06:09,580 --> 00:06:11,823
Speaker 1: They couldn't afford to actually Verify.

88
00:06:11,943 --> 00:06:13,228
Speaker 1: every single person is a man.

89
00:06:13,248 --> 00:06:21,729
Speaker 1: you imagine if every single person who went to only fans and said new account Right point my webcam at myself get naked Make money, right?

90
00:06:21,769 --> 00:06:26,344
Speaker 1: That's like a 10-minute process or at least it was You know very quick process.

91
00:06:26,384 --> 00:06:42,463
Speaker 1: if only fans actually verified and checked on every single person who was either a customer or a service provider On their site the cost and time required to do that what they would have been impossible to surmount They just go under

92
00:06:42,643 --> 00:06:47,240
Speaker 0: yeah, but what I what's interesting is that it appears pornhub had a similar problem.

93
00:06:47,280 --> 00:06:50,479
Speaker 0: You might remember last year where their payment processor went after them.

94
00:06:50,981 --> 00:06:56,000
Speaker 0: But they actually changed their policies to comply with the payment processor rules.

95
00:06:56,502 --> 00:07:01,400
Speaker 0: And if you have it, I'm if you're unaware pornhub is still around and doing fine.

96
00:07:02,182 --> 00:07:03,788
Speaker 1: Yeah, well, I mean because they're doing.

97
00:07:03,828 --> 00:07:07,580
Speaker 1: I don't know how well they're doing but they removed a lot of the porn on their site.

98
00:07:07,640 --> 00:07:16,459
Speaker 1: They basically don't let people just upload, you know, look at something like YouTube, right and you know Pornhub's doing a better job moderating its content than YouTube is.

99
00:07:17,323 --> 00:07:21,100
Speaker 1: Well, I mean pornhub but used to just allow people to upload whatever the hell horn be.

100
00:07:21,160 --> 00:07:23,170
Speaker 0: So what pornhub now it's like?

101
00:07:23,210 --> 00:07:24,919
Speaker 1: yeah, no almost no one can upload anything.

102
00:07:25,140 --> 00:07:29,054
Speaker 0: So what pornhub did to comply with those rules was pretty simple.

103
00:07:29,395 --> 00:07:38,352
Speaker 0: They only allow like verify ID users to post videos and No one can download videos directly from the site.

104
00:07:38,432 --> 00:07:40,279
Speaker 0: Like obviously there's an analog hole and everything.

105
00:07:41,043 --> 00:07:46,165
Speaker 0: analog hole for the analog hole but you can't actually download the videos.

106
00:07:46,687 --> 00:07:52,950
Speaker 0: and there's a couple other minor changes they made around moderation and that seemed to Get them around the new rules.

107
00:07:53,533 --> 00:07:57,620
Speaker 0: It's unclear to me Why I mean, there's so many things going on in the story.

108
00:07:57,901 --> 00:08:11,000
Speaker 0: But if we just focus on OnlyFans what I don't understand is if OnlyFans has such a user base and is so big Why could they not now that they're established do what pornhub did?

109
00:08:12,280 --> 00:08:12,922
Speaker 1: I don't know that's.

110
00:08:13,143 --> 00:08:18,100
Speaker 1: you think that would just be the common sense thing to do because the narrative of why this happened.

111
00:08:19,021 --> 00:08:24,340
Speaker 0: Someone who knows more about this than us could probably do an entire like YouTube series just on this space.

112
00:08:24,702 --> 00:08:42,635
Speaker 0: But essentially there is a long history of politicians Conservative interests and a variety of industries not just banking Generally just trying to keep porn out of the public discourse and the legacy of that and the damage of that All not just porn but also sex work is clearly a factor here.

113
00:08:42,695 --> 00:08:43,799
Speaker 0: that legacy continues.

114
00:08:43,960 --> 00:08:49,337
Speaker 0: There are very conservative forces in these spaces, but there's a bunch of other factors at play.

115
00:08:49,798 --> 00:09:24,699
Speaker 0: For example Pornographic content is one of the primary causes of chargebacks, charge disputes, lawsuits, escrow problems, extortion, money laundering, like all the things that even if you are the kind of payment Processor or bank who literally doesn't care about the content and all you care about is handling transactions for a fee It costs so much money to deal with that kind of nonsense that even if you think porn is fine as a business You're probably better off not allowing it so you don't have to deal with that space and all the problems that come up within it.

116
00:09:25,060 --> 00:09:37,374
Speaker 1: Yeah, well, I think also it's that you know, we see by you know, designating pornography even you know Legal consenting adult pornography as sort of like this special thing, right?

117
00:09:37,434 --> 00:09:38,116
Speaker 1: Yeah, you know, it's.

118
00:09:38,176 --> 00:09:39,620
Speaker 1: it's you have to go to a sex shop.

119
00:09:39,781 --> 00:09:41,880
Speaker 1: It's not just you know, you want to get a pornographic book.

120
00:09:41,980 --> 00:09:42,562
Speaker 1: It's not at.

121
00:09:42,964 --> 00:09:45,493
Speaker 1: you know Barnes and Noble borders doesn't exist anymore.

122
00:09:45,513 --> 00:09:49,150
Speaker 1: All right, you get it at the special shop I remember going to a Walden books.

123
00:09:49,371 --> 00:10:04,260
Speaker 0: You want to go you want to buy a book for a pseudo-girlfriend I had in like sixth grade because she was really into Danielle Steele and I went to buy it and the Walden books wouldn't Sell it to me because I was a sixth grader and they both share pornographic.

124
00:10:05,101 --> 00:10:06,428
Speaker 1: Right, but that wasn't poor.

125
00:10:06,468 --> 00:10:12,551
Speaker 1: but the point it was according to Walden books Whatever, but you can't buy a sex toy at Walmart.

126
00:10:12,893 --> 00:10:14,980
Speaker 1: For example, they don't sell dildos unless you're brave.

127
00:10:17,004 --> 00:10:20,646
Speaker 1: Sure depends what you want to use But you can't buy.

128
00:10:20,666 --> 00:10:22,193
Speaker 1: you know, it's like they just don't have those there.

129
00:10:22,274 --> 00:10:31,560
Speaker 1: and if that wasn't the case Right, if you just had porn as an accepted societal thing then porn would just be mixed in with other stuff.

130
00:10:31,680 --> 00:10:37,820
Speaker 1: You would write then it's like a payment processor isn't gonna cut off Walmart because there's a porn section at the Walmart.

131
00:10:38,202 --> 00:10:45,618
Speaker 1: But it's when there's an entire business and it's like the entire business is porn only Then it's very easy for the payment processor to cut it off.

132
00:10:45,739 --> 00:11:04,052
Speaker 0: Yep So I suspect in this case and again, we have no insider knowledge what OnlyFans was attempting to do this is just my personal guess is They started with porn They they ran into the problems I just described and eventually their payment processor was like look buddies enough is enough.

133
00:11:04,373 --> 00:11:21,470
Speaker 0: even pornhub follows these rules and rather than Continue to double down on trying to be in that space They tried to pivot to all the other kinds of things such a platform could do like Fitness instructors and tutoring and all that other stuff and just back away from the whole space.

134
00:11:23,264 --> 00:11:28,459
Speaker 0: Of course this led to a lot of backlash But also I don't know how much money there is in the rest of that.

135
00:11:28,861 --> 00:11:43,020
Speaker 1: Compared to it would be one thing if you if you intend, you know Put that up as your as your front and everybody knows wink wink The front and then the facade fell down and it's fitness instructors right in that facade now, right?

136
00:11:43,160 --> 00:11:44,614
Speaker 1: That's the wrong way to do it.

137
00:11:46,784 --> 00:11:59,480
Speaker 1: But I mean there is clearly I think it is a good core idea to have a somehow Some sort of site where people upload content and then people pay to access that content.

138
00:11:59,763 --> 00:12:00,854
Speaker 1: It's called patreon.

139
00:12:00,875 --> 00:12:06,820
Speaker 0: it exists Similarly had to deal with a lot of these issues back in 2017 and they're successfully navigated.

140
00:12:06,901 --> 00:12:09,704
Speaker 0: But as you note even to this day There's all that.

141
00:12:09,764 --> 00:12:19,700
Speaker 0: many of you we listen to geek nights are probably aware of a lot of pornography on patreon But it's not easy to search for that patreon pornography via patreon for a reason.

142
00:12:20,541 --> 00:12:30,499
Speaker 1: Yeah, you know Twitter is supposedly gonna try to do their super follow, which is effectively the same thing which is like yeah, if you pay you to follow me, you'll see more tweets that only people who pay to follow me see.

143
00:12:30,519 --> 00:12:33,018
Speaker 1: and Twitter does have a lot of porn on it.

144
00:12:33,962 --> 00:12:37,153
Speaker 1: Twitter is mostly okay with as long as you know, it's within certain lines.

145
00:12:37,935 --> 00:12:39,219
Speaker 1: So we'll see how that goes.

146
00:12:39,721 --> 00:12:40,342
Speaker 1: So I

147
00:12:40,402 --> 00:12:42,728
Speaker 0: found an article on xbiz that see

148
00:12:42,768 --> 00:12:53,636
Speaker 1: and also Facebook speaking of which right a lot of people correctly pointed out Facebook basically has more child porn another garbage on it than like any of these other platforms

149
00:12:53,857 --> 00:12:59,436
Speaker 0: with a possible Exception when we get to the main bit that Apple might have had more at least for a maybe.

150
00:12:59,496 --> 00:13:01,447
Speaker 1: but the point is Nobody's either.

151
00:13:01,467 --> 00:13:04,264
Speaker 1: the payment processors weren't cutting off Facebook.

152
00:13:04,888 --> 00:13:07,159
Speaker 1: right payments for ads and such?

153
00:13:07,541 --> 00:13:10,328
Speaker 0: Yeah, well for ads but there were.

154
00:13:10,448 --> 00:13:15,789
Speaker 0: I get the impression there were things that happen behind the scenes in these other websites as well.

155
00:13:16,643 --> 00:13:22,062
Speaker 0: I what I did learn Mastercard, I actually found the rules They're listed on this website I'll link to.

156
00:13:22,463 --> 00:13:28,340
Speaker 0: that are at stake here for the MasterCard part of the payment processing lifecycle that are affecting only fans.

157
00:13:28,702 --> 00:13:29,728
Speaker 0: So it's only.

158
00:13:29,748 --> 00:13:30,855
Speaker 0: it's not that many bullet points.

159
00:13:30,915 --> 00:13:31,800
Speaker 0: It's pretty straightforward.

160
00:13:32,921 --> 00:13:34,487
Speaker 0: One that one section.

161
00:13:34,848 --> 00:13:38,040
Speaker 0: This is the section that applies to what they call tube sites.

162
00:13:38,140 --> 00:13:42,234
Speaker 0: So pornhub x-video only lands anything you like.

163
00:13:42,354 --> 00:13:47,942
Speaker 0: Yeah anything that's live streams or direct interaction and stuff like that One.

164
00:13:48,363 --> 00:13:55,359
Speaker 0: you must enter a written contractual agreement with any individual contributing content that includes their consent their identity and their age.

165
00:13:56,587 --> 00:13:57,494
Speaker 0: That seems reasonable.

166
00:13:58,159 --> 00:14:07,360
Speaker 0: all Depicted in the content must in themselves give independent verifiable consent For the content to be distributed and downloaded.

167
00:14:08,885 --> 00:14:14,424
Speaker 1: You know if I can't if I sign an agreement and get an account and suddenly I invite someone over to be in a video It's like whoa, we got to know who that is.

168
00:14:14,444 --> 00:14:16,089
Speaker 0: Yeah, never mind.

169
00:14:16,229 --> 00:14:26,560
Speaker 0: a big part of the problem in this space is non-consensual pornography Where someone surreptitiously films say an encounter with a one-night stand and then that is the porn they publish.

170
00:14:26,863 --> 00:14:28,578
Speaker 0: That is a huge growing problem.

171
00:14:29,700 --> 00:14:33,940
Speaker 0: Wrench porns all kinds only verified users can upload content.

172
00:14:34,563 --> 00:14:47,616
Speaker 0: the big one I think is all content must be reviewed prior to publication or Real time if it's live streams and no content can violate the Mastercard brand Bram policies.

173
00:14:47,656 --> 00:14:48,440
Speaker 0: I'll get to that in a minute.

174
00:14:48,802 --> 00:14:59,860
Speaker 0: I dug into what that is a little more business risk assessment and mitigation policies Which is mostly around Mastercard saying these are the things that you cannot let happen with your payments because they are literally illegal.

175
00:15:01,602 --> 00:15:05,320
Speaker 0: The website must have a complaint process for reporting and removing material.

176
00:15:05,761 --> 00:15:10,574
Speaker 0: They must have policies to make sure the website cannot be used for human trafficking effectively.

177
00:15:11,155 --> 00:15:23,139
Speaker 0: They must provide monthly reports of flag content and what was taken down and No search terms or marketing partners or anything in their ecosystem can give the illusion that child porn is there.

178
00:15:24,082 --> 00:15:25,446
Speaker 0: They can't hit at child porn.

179
00:15:26,269 --> 00:15:27,613
Speaker 1: if someone searches for child porn.

180
00:15:27,633 --> 00:15:29,058
Speaker 1: It's got to say zero results.

181
00:15:29,278 --> 00:15:29,760
Speaker 1: Sorry, bro.

182
00:15:30,683 --> 00:15:35,080
Speaker 1: And that if someone searches for like, you know hot teens, it's got to be like no.

183
00:15:35,440 --> 00:15:39,840
Speaker 0: So as best I can tell with the caveat that this is like a couple days of research.

184
00:15:40,402 --> 00:15:45,179
Speaker 0: Those are the rules that OnlyFans is unwilling or unable to comply with.

185
00:15:46,206 --> 00:15:47,859
Speaker 1: Well, then that's it.

186
00:15:48,201 --> 00:15:59,345
Speaker 1: I think the real-time moderation one might be the hardest one for them Which is definitely the most expensive because you need so like you just need to pay people to sit there and watch it all day And that's not a job.

187
00:15:59,385 --> 00:15:59,928
Speaker 1: That's so.

188
00:16:00,329 --> 00:16:02,720
Speaker 0: what are those brand policies on Mastercard?

189
00:16:02,841 --> 00:16:28,220
Speaker 0: Well, that's a lot more complicated, but it seems to come into having controls to prevent directly like explicitly illegal activity from being paid for like if I buy a hitman or illegal drugs or illegal tobacco Coerce transactions, which is a euphemism for Blackmail or like literally just going up to an ATM and forcing someone to buy something or take money bribes.

190
00:16:28,543 --> 00:16:31,824
Speaker 0: Yeah, child trafficking Cryptocurrencies are covered by that.

191
00:16:31,884 --> 00:16:35,032
Speaker 0: buying and selling cryptocurrencies is heavily restricted by Mastercard.

192
00:16:35,072 --> 00:16:55,280
Speaker 0: similarly for all the reasons you can expect illegal gambling and There's a slightly more nebulous class of what they call offensive adult pornography but as far as I can tell it mostly applies to Explicitly illegal pornography or pornography that doesn't comply with those other rules that we were talking about.

193
00:16:55,802 --> 00:17:01,160
Speaker 1: Yeah, they're not they're talk they're not talking about, you know, just somebody's, you know, freaky kink or you're talking about.

194
00:17:01,240 --> 00:17:04,579
Speaker 1: You know really stuff that you know is shouldn't be.

195
00:17:05,602 --> 00:17:22,319
Speaker 0: So this is a really complicated space and it is a little bit too much of a simplification To say that Mastercard is directly anti porn or trying to censor Though that there that is definitely a factor considering the history of this industry and this space and these.

196
00:17:22,358 --> 00:17:41,022
Speaker 1: well Just you know the law right requiring certain things and as part of the problem is that you know a lot of the laws that are you know, well-meaning in an attempt to prevent or hinder job child porn and also Other crimes like, you know extortion bribery Etc.

197
00:17:41,363 --> 00:17:47,920
Speaker 1: black male right that the laws are not, you know, very sharp and delicate instruments.

198
00:17:48,101 --> 00:17:50,578
Speaker 1: They are blunt hammer and cause.

199
00:17:50,598 --> 00:17:58,220
Speaker 1: you know They cause a great deal of collateral damage mostly to consensual adult sex workers.

200
00:17:58,641 --> 00:18:09,213
Speaker 1: Because we have all these things in place, you know, not just because of companies But also because of the laws that make those company do do things right.

201
00:18:10,965 --> 00:18:12,682
Speaker 1: As a result, it's like yeah We're not.

202
00:18:12,702 --> 00:18:16,819
Speaker 1: we're gonna try to stop these things and as a result the thing that we don't want to stop is also stops.

203
00:18:17,401 --> 00:18:20,732
Speaker 0: Yep, so it's not just conservative agitation.

204
00:18:20,772 --> 00:18:22,177
Speaker 0: There's a lot more going on here.

205
00:18:22,237 --> 00:18:25,789
Speaker 0: There's a lot of Self-interested I don't want to deal with the sector.

206
00:18:25,829 --> 00:18:29,320
Speaker 0: that is extremely dangerous to deal with based on it like real kind.

207
00:18:29,360 --> 00:18:35,427
Speaker 1: Oh, yeah, of course also capitalism right because in socialism you would just have You know government sex workers.

208
00:18:35,548 --> 00:18:36,009
Speaker 0: Yeah.

209
00:18:36,049 --> 00:18:38,920
Speaker 0: Well, no no that that gets into another dicey territory.

210
00:18:39,080 --> 00:18:43,699
Speaker 0: I think a better way to look at it is in a you know, truly socialist society that was constructed.

211
00:18:43,759 --> 00:18:48,943
Speaker 0: Well Nobody would be doing sex work unless they wanted to Exactly.

212
00:18:48,963 --> 00:18:50,086
Speaker 0: That's a better way to put it.

213
00:18:50,406 --> 00:19:01,020
Speaker 0: government sex workers gets into a far more terrifying consequence of poorly written laws and This is a. this is a an argument that would need to be addressed by better laws.

214
00:19:01,402 --> 00:19:08,145
Speaker 0: But imagine if sex work were completely legal and regulated Arguably under existing laws.

215
00:19:08,466 --> 00:19:15,426
Speaker 0: someone could be forced to do it if they found a job in that sector and we're currently collecting unemployment Or risk losing their unemployment.

216
00:19:15,888 --> 00:19:16,811
Speaker 0: That doesn't feel right.

217
00:19:16,852 --> 00:19:17,193
Speaker 0: Does it?

218
00:19:17,956 --> 00:19:18,579
Speaker 1: No, it doesn't.

219
00:19:18,619 --> 00:19:20,846
Speaker 0: Yep This is why this is such a complicated space.

220
00:19:21,408 --> 00:19:36,260
Speaker 0: So only fans really shit the bed and fucked up here because they as from everything I can tell they could have taken steps to comply with these rules and other sites like Pornhub have indeed Complyed with these steps and continue to take payments.

221
00:19:36,741 --> 00:19:47,588
Speaker 0: so only fans pivoted away from the demographic and the people that made them a success and I kind of hope they go out of business because they really should have had those people's back.

222
00:19:47,808 --> 00:19:51,158
Speaker 0: their platform only got to the scale it got to because of those people.

223
00:19:51,459 --> 00:19:55,909
Speaker 0: if you're Gonna if you're gonna go into that industry You'd better be there for the people you're working with.

224
00:19:56,110 --> 00:20:09,198
Speaker 0: at least the people who invested in it are gonna get burned Yep, because I don't think OnlyFans is gonna survive on people paying to get a hockey player to chat with them like an old Call the hotline to talk to Corey Haim thing from the 80s?

225
00:20:10,374 --> 00:20:12,864
Speaker 1: Yeah Pretty sure.

226
00:20:12,884 --> 00:20:14,654
Speaker 1: the hotline just talked to a recording, right?

227
00:20:14,694 --> 00:20:14,976
Speaker 0: Yeah.

228
00:20:15,096 --> 00:20:18,228
Speaker 1: Yeah There has been at the phone that eat that I saw.

229
00:20:18,328 --> 00:20:25,109
Speaker 0: I watched a YouTube video a while ago talking about like that whole or like Preteen 900 number ecosystem from the 80s.

230
00:20:25,731 --> 00:20:28,419
Speaker 0: What a weird world that was the late 80s.

231
00:20:29,901 --> 00:20:33,393
Speaker 0: But there's a ton more here that we do not have time or expertise to get into.

232
00:20:33,454 --> 00:20:37,693
Speaker 0: but I'm gonna link to a whole bunch Resources if you want to learn more about that space.

233
00:20:37,975 --> 00:20:40,690
Speaker 0: and if you think to yourself Why don't I just make a website?

234
00:20:40,711 --> 00:20:41,395
Speaker 0: that does it right?

235
00:20:41,837 --> 00:20:42,260
Speaker 0: Good luck.

236
00:20:42,722 --> 00:20:46,039
Speaker 0: And if you think why don't I just make a payment processor and a bank?

237
00:20:47,802 --> 00:20:48,567
Speaker 0: I thought about that.

238
00:20:48,848 --> 00:20:50,216
Speaker 0: We were talking about that a lot.

239
00:20:50,236 --> 00:20:50,719
Speaker 1: Good luck.

240
00:20:52,369 --> 00:21:05,128
Speaker 0: in my Professional opinion it is possibly possible to do it without committing a felony and I Don't think I think of it like this, right?

241
00:21:05,148 --> 00:21:08,940
Speaker 1: There's this big, you know, there's this big field of lava, right?

242
00:21:09,442 --> 00:21:11,932
Speaker 1: And there's yes, you can cross this lava field.

243
00:21:11,972 --> 00:21:13,820
Speaker 1: There is clearly some stepping stones.

244
00:21:15,466 --> 00:21:18,840
Speaker 0: Few other people in metal boats like making their way around the lava.

245
00:21:18,920 --> 00:21:20,669
Speaker 0: They're out there, but there's not many touching.

246
00:21:20,750 --> 00:21:23,544
Speaker 1: touching the lava means federal prison Right.

247
00:21:23,826 --> 00:21:26,480
Speaker 1: and the other side is is you know what we want, right?

248
00:21:26,680 --> 00:21:37,414
Speaker 1: Which is you know consensual adult sex workers get paid as they deserve to be Right and can sexual adult customers are able to pay them as they you know, should right.

249
00:21:37,895 --> 00:21:44,840
Speaker 1: and yet to cross that Someone's got to cross the fire the lava with the stepping stones without touching the lava.

250
00:21:45,464 --> 00:21:46,270
Speaker 1: Someone else is doing it.

251
00:21:46,311 --> 00:21:46,613
Speaker 1: Not me.

252
00:21:46,653 --> 00:21:51,812
Speaker 0: I'm not touching the lava Ah, so before we go to the next news the list is not gonna risk it Oh, but also it's not just that.

253
00:21:51,933 --> 00:21:57,154
Speaker 0: even if you're looking to lava You see boats flipping over and sinking in the lava constantly big ones.

254
00:21:57,294 --> 00:21:59,140
Speaker 1: The only fans one is like on fire.

255
00:21:59,481 --> 00:22:05,557
Speaker 0: Yep, the Titanic has flipped over and is sinking on fire and you're sitting there and your little dinghy thinking I gotta get me some of that.

256
00:22:07,381 --> 00:22:08,827
Speaker 1: I'm gonna be the one who makes it across.

257
00:22:09,248 --> 00:22:12,080
Speaker 1: all these big guys didn't make it but I'm the one who's gonna make it.

258
00:22:12,280 --> 00:22:16,213
Speaker 0: Yep, so but possibly possible is what I would say, but the listener it just asked.

259
00:22:16,674 --> 00:22:18,580
Speaker 0: What about cryptocurrency one?

260
00:22:18,980 --> 00:22:40,740
Speaker 0: Cryptocurrency needs to be banned for a bunch of other reasons like so many reasons I can't get into but to The infrastructure bill that the US government is about to pass includes and this is a whole topic for another day Includes language that allows the federal government to regulate cryptocurrencies and guess what those regulations will will effectively do.

261
00:22:41,322 --> 00:22:59,520
Speaker 0: They'll require the same kinds of things that MasterCard is requiring of porn sites Verifying the identities verifying where the money came from and what is being used for all the things that all the other banks do Which literally remove all the quote good aspects unquote of cryptocurrency that libertarian shitbags like to talk about.

262
00:23:00,002 --> 00:23:12,040
Speaker 1: The failure of cryptocurrency to be used as a currency even if you forget about its environmental damage Even if you forget about all the other nonsense and whatnot, right is that it's not being used as a currency.

263
00:23:12,281 --> 00:23:14,289
Speaker 1: No one will accept it for anything.

264
00:23:14,329 --> 00:23:15,694
Speaker 1: You can't pay your rent with it.

265
00:23:15,714 --> 00:23:17,100
Speaker 1: You can't buy groceries with it.

266
00:23:17,120 --> 00:23:21,660
Speaker 1: So at some point it needs to be converted to dollars and at the point it could be converted to dollars.

267
00:23:22,364 --> 00:23:25,220
Speaker 1: The government can get you and in fact, they don't even need to get to that point.

268
00:23:25,280 --> 00:23:32,840
Speaker 1: They've seized plenty of bitcoins from plenty of people before they converted to dollars or after I know when it was still in Bitcoin form.

269
00:23:32,961 --> 00:23:42,799
Speaker 1: So it actually provides none and almost nobody Right is actually using any cryptocurrency in the dis in the purely distributed way that you know.

270
00:23:43,301 --> 00:23:56,918
Speaker 1: The one benefit of it is like oh, you know, you can just anonymously transfer it between point A and point B. Yeah, that's if you with your computer with your wallet on your computer Directly interact with the blockchain from your computer with no one.

271
00:23:56,958 --> 00:24:07,586
Speaker 1: in between 99.99% of cryptocurrency transactions people are using some cloud service middleman to hold their wallet for them and that's Effectively no different than using Chase Bank.

272
00:24:07,646 --> 00:24:11,400
Speaker 1: So that's how you're getting caught and no one does it the real way.

273
00:24:11,500 --> 00:24:17,580
Speaker 1: So they've just thrown out the Bennett who the one paper on paper, but not in reality benefit of cryptocurrency.

274
00:24:17,882 --> 00:24:18,688
Speaker 1: It's just all bullshit.

275
00:24:18,991 --> 00:24:20,180
Speaker 1: see our episode on cryptocurrency.

276
00:24:20,841 --> 00:24:25,760
Speaker 0: So in some other big news and this really escalated in the weeks while we were gone.

277
00:24:25,880 --> 00:24:27,669
Speaker 0: So I was I was only barely following.

278
00:24:27,729 --> 00:24:30,220
Speaker 1: I saw it escalating but it's continued to escalate.

279
00:24:30,302 --> 00:24:36,720
Speaker 0: So now we got to talk about yeah And and the one thing I'll say before we get into this is that I am shocked this did not happen sooner.

280
00:24:38,742 --> 00:24:40,551
Speaker 1: Like I don't know why this took so long to have.

281
00:24:40,631 --> 00:24:42,400
Speaker 0: I know it's kind of like way back when I was a kid.

282
00:24:42,480 --> 00:24:49,891
Speaker 0: I was like, why doesn't someone just make a virus that instead of fucking up your computer Just like steals money or spies on you or something.

283
00:24:49,931 --> 00:24:50,794
Speaker 0: Like why don't you buy this?

284
00:24:51,055 --> 00:24:52,580
Speaker 0: Why isn't someone making money with this?

285
00:24:52,720 --> 00:24:55,136
Speaker 0: Why are they just making viruses that they did eventually?

286
00:24:55,156 --> 00:25:04,860
Speaker 0: Yep so Effectively, there is a there is a culture in twitch which not to stray from the point and definitely not to blame the victims.

287
00:25:05,362 --> 00:25:10,260
Speaker 0: But I think this culture is a broken and toxic result of how twitch makes money in the first place.

288
00:25:10,802 --> 00:25:13,993
Speaker 1: I think this see our ancient talk about.

289
00:25:14,153 --> 00:25:18,660
Speaker 1: you know, the game makes the community In this case the user interface.

290
00:25:19,982 --> 00:25:29,777
Speaker 1: If you consider twitch to be a game right all the rules of twitch and the way the software works Causes the community that is on there in my understanding.

291
00:25:29,818 --> 00:25:34,736
Speaker 0: So So basically the the culture at stake here.

292
00:25:34,896 --> 00:25:35,860
Speaker 0: Is that on twitch?

293
00:25:36,780 --> 00:25:42,375
Speaker 0: People will raid other channels and by raid which is usually a positive thing.

294
00:25:42,455 --> 00:25:48,010
Speaker 1: It's like I'm done streaming There's still 20 there's still a bunch of people watching me.

295
00:25:48,030 --> 00:25:53,367
Speaker 1: right I will raid which you know Even though it's it's a negative term.

296
00:25:53,427 --> 00:25:54,792
Speaker 1: It sounds like a bad thing.

297
00:25:55,314 --> 00:25:57,120
Speaker 1: It's actually in twitch terms a good thing.

298
00:25:57,462 --> 00:26:04,380
Speaker 1: I will send them to this other person that I like and Even if they keep watching me, they'll just be sent to this other person.

299
00:26:04,822 --> 00:26:09,600
Speaker 1: They'll probably watch them for at least a little bit and if they like them, maybe they'll stay there, right?

300
00:26:10,060 --> 00:26:12,560
Speaker 1: But if not, then they'll go somewhere else on their own.

301
00:26:12,600 --> 00:26:15,666
Speaker 1: No one can force them to watch this other channel But it's still.

302
00:26:15,706 --> 00:26:19,826
Speaker 1: it's giving that other person like a boost of attention And you the most.

303
00:26:19,887 --> 00:26:26,760
Speaker 1: it's usually a very good thing when a very popular streamer Right will raid a very small channel.

304
00:26:27,543 --> 00:26:28,827
Speaker 0: Well, it's good.

305
00:26:29,088 --> 00:26:32,740
Speaker 0: like twitch users and content creators perceive it as being good.

306
00:26:33,141 --> 00:26:37,718
Speaker 0: But I've seen a lot of evidence that those audiences don't actually stick around.

307
00:26:37,758 --> 00:26:41,537
Speaker 1: they primarily But for the most part they don't stick around.

308
00:26:41,577 --> 00:26:46,021
Speaker 1: but you know You just get one or two people can be a bit, you know a big deal for somebody It can.

309
00:26:46,322 --> 00:26:56,640
Speaker 0: but a big part of rating too is people will do raid campaigns to help someone they like Get twitch partner verified or whatever like get their account above the line so they can model.

310
00:26:56,680 --> 00:27:05,100
Speaker 1: So you'll you'll often see a community of streamers that are all friends with each other and they basically raid each other in like a circle And that sort of merges their communities together.

311
00:27:05,140 --> 00:27:13,993
Speaker 1: Yeah So that like you get the same viewers will follow all of them and watch whoever is online at a given time and they'll stagger Their stream schedules and make like a little.

312
00:27:14,174 --> 00:27:18,109
Speaker 1: you know a little thing You know, so it's a good functionality.

313
00:27:18,150 --> 00:27:20,159
Speaker 1: the term raid isn't helpful, but you know.

314
00:27:21,421 --> 00:27:26,516
Speaker 0: Yep, so but the problem there is that someone's account will get just barely above the line.

315
00:27:26,577 --> 00:27:27,720
Speaker 0: They get partner status.

316
00:27:27,800 --> 00:27:31,454
Speaker 0: They feel like they made it but those people mostly don't stick around.

317
00:27:31,796 --> 00:27:37,820
Speaker 0: now weeks later Their account falls way below the line and they're making basically no money and they're not any better off.

318
00:27:38,861 --> 00:27:41,791
Speaker 1: Well, you know, I'm just saying it's a there's a there's a.

319
00:27:41,931 --> 00:27:44,680
Speaker 0: there is a just due to the sheer number of streams.

320
00:27:45,261 --> 00:27:52,140
Speaker 0: It's not a strictly zero-sum game, but an individual person can only watch so many minutes of twitch per day.

321
00:27:52,601 --> 00:27:56,819
Speaker 0: so there is a zero sum of how many eyeballs you can get on your twitch stream.

322
00:27:56,839 --> 00:28:06,960
Speaker 0: and For a lot of smaller creators the cert the raid ecosystem is a turn of the same people Through a set of channels and it doesn't make anything sustainable.

323
00:28:07,281 --> 00:28:07,984
Speaker 1: But that's not.

324
00:28:08,045 --> 00:28:08,788
Speaker 1: it is a key.

325
00:28:08,808 --> 00:28:09,390
Speaker 1: It is a cute.

326
00:28:09,511 --> 00:28:11,700
Speaker 1: It is a cute sometimes helpful feature.

327
00:28:11,740 --> 00:28:14,400
Speaker 0: Oh, yeah uses but I have long-term concerns about it.

328
00:28:14,545 --> 00:28:22,792
Speaker 0: but What started happening and it appears that this is the work of a very small number of people who have written bots and built Infrastructure to make this happen.

329
00:28:22,812 --> 00:28:23,518
Speaker 0: They've written code.

330
00:28:24,061 --> 00:28:38,252
Speaker 0: then it appears even that the primary person or the smaller people involved with it are joining people's channels and even doing things like warning them they're about to run the racism bot or Asking for feedback on the racism bot posting change logs on.

331
00:28:38,533 --> 00:28:40,078
Speaker 0: hey, this is version 2.1 of the bot.

332
00:28:40,841 --> 00:28:43,269
Speaker 0: I hope you enjoy it before the Nazi stuff starts coming out.

333
00:28:43,770 --> 00:29:00,627
Speaker 0: The bot will basically cause huge numbers of random automatically generated Twitter account or twitch accounts to join your channel and Just start spouting extremely racist nonsense Like since your content you can imagine

334
00:29:01,209 --> 00:29:06,302
Speaker 1: and since twitch puts moderation on the shoulders of the streamers and their moderators and not on Themselves

335
00:29:06,362 --> 00:29:09,371
Speaker 0: yet also gives them almost no tools to actually money

336
00:29:09,391 --> 00:29:09,572
Speaker 1: gives.

337
00:29:09,652 --> 00:29:12,140
Speaker 1: it gives them tools, but it gives them very weak tools.

338
00:29:12,160 --> 00:29:14,060
Speaker 0: Oh, those tools are unusable garbage.

339
00:29:14,361 --> 00:29:19,237
Speaker 0: I've actually been working on a spec and like what those tools we need to look like and it does give.

340
00:29:19,257 --> 00:29:23,994
Speaker 1: it does provide Apis to let people build better tools, which you usually would use.

341
00:29:24,054 --> 00:29:25,860
Speaker 1: but even those tools are weak sauce.

342
00:29:26,442 --> 00:29:31,378
Speaker 1: So in the absence of strong enough tools and the absence of twitch not doing anything on their own.

343
00:29:31,659 --> 00:29:43,530
Speaker 1: really You know if this happens to you, the only recourse you really have as a streamer right is You can just turn your stream chat into sub only mode.

344
00:29:43,551 --> 00:29:47,647
Speaker 1: because yeah, go ahead subscribe to me with all those boss five bucks each I'll take it.

345
00:29:48,032 --> 00:29:52,780
Speaker 1: It's like You know, you don't really have a lot of things you can do about it.

346
00:29:54,122 --> 00:29:58,840
Speaker 1: Or you could put your chat it's like slow mode I guess and then try to ban them one at a time.

347
00:29:59,941 --> 00:30:01,108
Speaker 1: That's really a pain in the ass.

348
00:30:01,128 --> 00:30:01,994
Speaker 1: You don't want to be doing that.

349
00:30:02,014 --> 00:30:04,185
Speaker 1: You want to be playing the video game You're playing right?

350
00:30:04,205 --> 00:30:05,350
Speaker 1: You don't be clicking.

351
00:30:05,411 --> 00:30:07,379
Speaker 1: ban on a hundreds and hundreds of chats in there.

352
00:30:08,344 --> 00:30:09,720
Speaker 1: I guess you could turn off chat completely.

353
00:30:10,041 --> 00:30:19,439
Speaker 0: I don't even know if that's an option on but again the raid culture and the culture of twitch is that No streamer wants to make their stream subs only they want it to be open to anyone.

354
00:30:20,203 --> 00:30:23,099
Speaker 1: So you'd only usually streamers like don't want.

355
00:30:23,139 --> 00:30:29,160
Speaker 1: yeah You don't want to be sub only because that's gonna be a turn-off for you know, people who just come across your channel.

356
00:30:29,220 --> 00:30:32,075
Speaker 1: They're not gonna want to stay if they can't chat anything.

357
00:30:32,095 --> 00:30:33,120
Speaker 1: They're just stuck watching you.

358
00:30:33,160 --> 00:30:35,033
Speaker 1: I guess watching you isn't entertaining enough.

359
00:30:35,516 --> 00:30:37,023
Speaker 1: for some reason They need to.

360
00:30:37,043 --> 00:30:40,332
Speaker 1: they need to spam emojis to feel like they belong like.

361
00:30:40,372 --> 00:30:45,812
Speaker 0: that is such a part of that culture that I Understand it, but I all but I don't enjoy it.

362
00:30:46,053 --> 00:30:46,475
Speaker 0: I like.

363
00:30:46,555 --> 00:30:52,330
Speaker 0: obviously I'm old I don't care anymore, but also That culture is start is.

364
00:30:52,351 --> 00:30:58,679
Speaker 0: I don't want to blame the victims here But the culture of how twitch is used is making this problem worse rather than better.

365
00:30:59,542 --> 00:31:03,965
Speaker 1: Yeah, it's like the only time I ever chat on twitch is If you know what

366
00:31:04,005 --> 00:31:06,997
Speaker 0: if it's a small stream and I'm like chatting with like a small

367
00:31:07,037 --> 00:31:13,480
Speaker 1: small stream And I'm legitimately talking to the person who's streaming, you know, like I actually have something to say to them.

368
00:31:13,581 --> 00:31:14,229
Speaker 0: Well, like what are we doing?

369
00:31:14,249 --> 00:31:15,100
Speaker 0: the live stream on gig now?

370
00:31:15,100 --> 00:31:18,475
Speaker 0: It's like I'm usually chatting and text in the live chat while we're doing the show.

371
00:31:18,495 --> 00:31:28,672
Speaker 1: Scott can't even see that right but if our chat if we were super popular and our chat became a stream of emojis that was just provided that would I mean I Would just turn it off at that point was devoid of content.

372
00:31:28,712 --> 00:31:30,260
Speaker 1: Well, we might as well just turn it off then it doesn't.

373
00:31:30,361 --> 00:31:31,071
Speaker 1: It's doing nothing.

374
00:31:31,091 --> 00:31:34,847
Speaker 1: Yeah Right, so it's like I don't know.

375
00:31:35,127 --> 00:31:35,368
Speaker 0: Yep.

376
00:31:35,628 --> 00:31:40,220
Speaker 1: But yeah, the twitch chat itself is separate from this raid bot problem.

377
00:31:40,220 --> 00:31:44,665
Speaker 1: Yep, but the raid bot problem has been going on and it's mostly Targeting.

378
00:31:45,274 --> 00:31:54,884
Speaker 1: right Yes, yes, but also LGBTQ people right they're going after Targeting white dude.

379
00:31:54,945 --> 00:32:02,808
Speaker 0: No, they're going after us to say that primarily and almost exclusively Underrepresented and oppressed minorities.

380
00:32:03,551 --> 00:32:09,497
Speaker 1: Yeah and and posting, you know insult, you know slurs right in there in their bots, right.

381
00:32:09,517 --> 00:32:14,477
Speaker 1: so The question is a right Twitch?

382
00:32:15,146 --> 00:32:15,328
Speaker 1: Right.

383
00:32:15,348 --> 00:32:16,600
Speaker 1: It's like what the hell are you doing about this?

384
00:32:16,780 --> 00:32:21,760
Speaker 1: Why is it so easy to make a twitch account that you can just make a zillion of them once someone does this?

385
00:32:21,900 --> 00:32:24,120
Speaker 1: It should be really easy to identify.

386
00:32:24,502 --> 00:32:28,863
Speaker 1: Shouldn't all of those accounts just be perma banned Right instantly?

387
00:32:29,063 --> 00:32:34,640
Speaker 0: if I someone is of twitch and if someone is programmatically creating so many new accounts as they get banned?

388
00:32:38,680 --> 00:32:40,249
Speaker 1: Yeah, there's like, you know, it's quit.

389
00:32:40,289 --> 00:32:43,525
Speaker 1: and Twitter if you try to do API stuff It's like you have an API key.

390
00:32:43,545 --> 00:32:44,992
Speaker 1: They just turn your key off.

391
00:32:45,032 --> 00:32:45,213
Speaker 1: great.

392
00:32:45,233 --> 00:32:46,920
Speaker 1: You can't make any more API commands.

393
00:32:47,001 --> 00:32:48,150
Speaker 1: Yeah, and all right.

394
00:32:48,170 --> 00:32:55,871
Speaker 0: What else you gonna do like The one it's twitch needs to take an extreme action And instead they're making the slowest most miltoast actions possible.

395
00:32:55,891 --> 00:32:58,500
Speaker 1: Like if I and twitch has the resources of Amazon.

396
00:32:58,500 --> 00:33:09,120
Speaker 0: Yeah if I were the product manager for twitch like overall like the head of product the first thing I would do is literally Ban like I would halt for a couple weeks all new account creation.

397
00:33:09,582 --> 00:33:19,480
Speaker 0: May have a team dedicated to banning all the accounts that are doing this big forensic analysis Like I like I would treat this as an existential crisis for the company.

398
00:33:20,941 --> 00:33:23,270
Speaker 0: Yeah, which is saying like oh, well, we have tools.

399
00:33:23,290 --> 00:33:24,154
Speaker 0: We're gonna make more tools.

400
00:33:24,395 --> 00:33:25,820
Speaker 0: Those tools do not work at all.

401
00:33:25,820 --> 00:33:29,394
Speaker 0: We don't have time to get into all the examples if you want to see the one.

402
00:33:29,695 --> 00:33:30,940
Speaker 1: just one example, right?

403
00:33:31,020 --> 00:33:35,000
Speaker 1: There are many many many examples, but the one example in a tweet I saw going.

404
00:33:35,323 --> 00:33:36,534
Speaker 0: I know which one you're about to talk about.

405
00:33:36,575 --> 00:33:36,999
Speaker 0: I think I have it.

406
00:33:37,341 --> 00:33:39,792
Speaker 1: Someone was like, hey, what if I post the word?

407
00:33:39,832 --> 00:33:41,600
Speaker 1: I want to post the word jogger, right?

408
00:33:41,860 --> 00:33:45,200
Speaker 1: Obviously they were referencing another word that has two G's in it, right?

409
00:33:45,400 --> 00:33:48,494
Speaker 1: But like hey, what if I want to post the word jogger in somebody's chat?

410
00:33:48,936 --> 00:33:49,740
Speaker 1: It's like well, okay.

411
00:33:49,780 --> 00:33:53,656
Speaker 1: So they go in and they type in their block list the word joggers that it cannot.

412
00:33:53,697 --> 00:33:58,160
Speaker 1: so anyone who types It is automatically that that message is automatically blocked, right?

413
00:33:58,685 --> 00:34:17,621
Speaker 1: Okay So but the problem is is you can type any Unicode character you want into twitch and there's several Characters in Unicode that look like J several that look like Oh several that look like G Several that look like E and several that look like are right in Cyrillic and several other alphabets that are available in the Unicode character Space.

414
00:34:18,203 --> 00:34:31,288
Speaker 1: so the actual number of ways you could type the word jogger that would be recognizable in English right to an English speaker is many many many many and You can't type them all in right.

415
00:34:31,389 --> 00:34:36,928
Speaker 1: and twitch doesn't automatically block all the variations To block all of them.

416
00:34:36,989 --> 00:34:45,235
Speaker 1: You would have to do it with an automated, you know API calls right to your own twitch account to add all the variations of just one word.

417
00:34:45,737 --> 00:35:00,111
Speaker 1: and that would take you Even if you did all that and wrote the code to do it or someone provided a tool to do it with a limit on API calls that you can make to twitch would take you like several several many days Just to add one word to your block list if you wanted to block all

418
00:35:00,191 --> 00:35:00,712
Speaker 0: possible.

419
00:35:00,793 --> 00:35:02,839
Speaker 1: Yeah visual variations of it.

420
00:35:03,100 --> 00:35:03,785
Speaker 0: now Look at twitch.

421
00:35:04,409 --> 00:35:05,720
Speaker 0: How do you solve this one?

422
00:35:06,401 --> 00:35:11,158
Speaker 0: Really simplistic machine learning models can handle that like an exit.

423
00:35:11,178 --> 00:35:13,605
Speaker 1: We already have For decades.

424
00:35:13,666 --> 00:35:14,809
Speaker 1: It's called a spam filter.

425
00:35:14,829 --> 00:35:15,792
Speaker 1: We use it on email.

426
00:35:15,832 --> 00:35:17,116
Speaker 1: Yeah, right if somebody just

427
00:35:17,698 --> 00:35:18,179
Speaker 0: apply

428
00:35:18,199 --> 00:35:33,905
Speaker 1: a basic email spam filter technology to twitch chats Right if the kinds of twitch chats that are getting blocked consider that mark as spam and if you see an account that is Getting a lot of their chats marked as spam Automatically ban it

429
00:35:34,185 --> 00:35:34,306
Speaker 0: from.

430
00:35:34,346 --> 00:35:36,934
Speaker 0: so this gets back to that that original conversation.

431
00:35:37,195 --> 00:35:42,040
Speaker 0: because the twitch culture likes an extremely spammy chat and likes raids.

432
00:35:42,882 --> 00:35:49,480
Speaker 0: The the fact that the creators are unwilling to entertain solutions that end either of those things, right?

433
00:35:49,520 --> 00:35:55,445
Speaker 1: It's like you could just make for example, you could make all of twitch just all of twitch no chat No chat.

434
00:35:55,485 --> 00:35:56,794
Speaker 1: Why is chat have to be a thing?

435
00:35:56,814 --> 00:35:57,740
Speaker 1: I'm just streaming.

436
00:35:57,820 --> 00:36:01,680
Speaker 1: Why do I have to have a chat if I don't want to let people run their own discord or their own chat or something?

437
00:36:02,081 --> 00:36:04,289
Speaker 1: That's a people on twitch would not like that.

438
00:36:04,349 --> 00:36:04,570
Speaker 1: Okay.

439
00:36:04,590 --> 00:36:07,360
Speaker 1: Well, you could make all chats sub only by default.

440
00:36:07,380 --> 00:36:12,360
Speaker 1: Well, that should actually encourage people to pay because you every chat is just always sub only all the time.

441
00:36:12,400 --> 00:36:13,624
Speaker 1: There is no public chat.

442
00:36:13,905 --> 00:36:14,848
Speaker 1: It's sub only chat.

443
00:36:15,209 --> 00:36:19,064
Speaker 1: You want to talk to me pay five bucks or use your prime sub right for the month.

444
00:36:19,450 --> 00:36:19,653
Speaker 1: it's like.

445
00:36:20,860 --> 00:36:32,640
Speaker 1: Streamers don't like that either because even though that might get some people more people to sub It would really hurt their engagement with people who have not subbed yet and getting them to sub right because they can't chat.

446
00:36:32,880 --> 00:36:37,997
Speaker 0: Yeah, but a lot of marginalized creators reach an audience of other marginalized people who couldn't afford.

447
00:36:38,037 --> 00:36:39,061
Speaker 0: you don't have $5.

448
00:36:39,061 --> 00:36:39,940
Speaker 0: Yeah, exactly.

449
00:36:40,462 --> 00:36:41,244
Speaker 0: So they don't want.

450
00:36:41,565 --> 00:36:43,492
Speaker 0: it's kind of like with the problems with Twitter.

451
00:36:43,612 --> 00:36:50,120
Speaker 0: as I always said the only ways To stop the mass abuse that happens on Twitter in an open platform.

452
00:36:50,301 --> 00:36:52,288
Speaker 0: There's really only two fundamental ways you can go.

453
00:36:52,730 --> 00:36:58,921
Speaker 0: you either make some sort of cost However small to create an account You.

454
00:36:59,062 --> 00:37:06,420
Speaker 0: well, I guess there's three ways you have a verifiable identity required to create an account or you accept verifiable identities.

455
00:37:06,741 --> 00:37:08,970
Speaker 1: Actually don't you know, they make it so that you can.

456
00:37:08,990 --> 00:37:14,641
Speaker 1: they make it easier to block someone right because you've have But they don't prevent the behavior There's.

457
00:37:14,742 --> 00:37:20,580
Speaker 1: there have been a studies that show that like yeah, most of the people on Twitter saying awful things Just have their real identities right there.

458
00:37:20,881 --> 00:37:28,203
Speaker 0: Because if we're willing to actually follow through and like go after those people with on available bands But they're not.

459
00:37:28,745 --> 00:37:32,120
Speaker 0: the third way to go is to do what we did with email.

460
00:37:32,502 --> 00:37:36,100
Speaker 0: They all and I say this a lot because younger people do not appreciate this.

461
00:37:36,641 --> 00:37:52,480
Speaker 0: The only reason email exists today and works at all is because as a society we collectively agreed that 99.99999% of all emails are vile spam and we're willing to let Lots of legitimate emails get blocked rather than see a single spam.

462
00:37:53,613 --> 00:37:55,927
Speaker 1: Yeah The email system is like you could.

463
00:37:56,147 --> 00:38:03,700
Speaker 1: no one can stop you from making an email server and just start sending emails to other email Servers, you know, there's absolutely nothing in the world that can stop you.

464
00:38:03,780 --> 00:38:04,162
Speaker 0: But what?

465
00:38:04,222 --> 00:38:04,464
Speaker 0: no one?

466
00:38:04,484 --> 00:38:07,399
Speaker 0: all your emails if you do that at all because no one will none you.

467
00:38:08,305 --> 00:38:10,320
Speaker 1: Yep, no one will get your emails right there neat.

468
00:38:10,440 --> 00:38:23,944
Speaker 1: You need to pass a rigorous sort of thing before any other email server will say, okay I will allow you to email but if twitch was like you must pass a very rigorous trial before we will allow a streamer to See your chats.

469
00:38:24,467 --> 00:38:25,754
Speaker 1: No one would pass that trial.

470
00:38:25,975 --> 00:38:29,366
Speaker 1: They want to get users So they had to lower their barrier to entry.

471
00:38:29,707 --> 00:38:29,848
Speaker 1: right.

472
00:38:29,888 --> 00:38:31,472
Speaker 1: that fundamental conflict is.

473
00:38:31,512 --> 00:38:40,560
Speaker 1: what's belying our entire all of our social networks Is that the networks want to lower the barrier to entry to get more users, but lowering the barrier to entry let's in shit

474
00:38:40,822 --> 00:38:54,329
Speaker 0: yep, and I think with twitch in particular There are no good ways to deal with this without harming some aspect of the twitch community Culture and interaction space that people enjoy though.

475
00:38:54,350 --> 00:38:55,172
Speaker 1: Well not harming.

476
00:38:55,212 --> 00:39:04,980
Speaker 0: not harming by some definitions way that drastically Drastically altered in a way that a huge percentage of twitch users would dislike.

477
00:39:06,124 --> 00:39:07,272
Speaker 1: and yeah, probably.

478
00:39:07,574 --> 00:39:25,657
Speaker 0: but I think the Avenue of least pain and harm would be for twitch to provide one provide actual effective moderation tools which all platforms are avoiding like something so simple as building block chains and shared block lists into the platform directly.

479
00:39:25,678 --> 00:39:34,877
Speaker 1: Twitter Blockchain like Bitcoin blockchain as like everyone that I've everyone that I've blocked and everyone that you've blocked are now blocked on both of us.

480
00:39:34,997 --> 00:39:45,760
Speaker 0: automatically then marginalized creators could band together and share a gigantic block list that would automatically update and Honestly solve a huge percentage of this problem that Twitter.

481
00:39:46,041 --> 00:39:47,546
Speaker 1: But why do they even have to do that?

482
00:39:47,627 --> 00:39:50,878
Speaker 1: The moderation should come from the platform itself.

483
00:39:51,059 --> 00:39:53,043
Speaker 0: No Band that.

484
00:39:53,765 --> 00:39:57,898
Speaker 1: if someone's banned from all these channels, why aren't they just banned from all of twitch forever?

485
00:39:57,959 --> 00:40:01,730
Speaker 1: Yeah, right It's if the hundred people block some shithead on Twitter.

486
00:40:01,770 --> 00:40:04,460
Speaker 1: Why is their Twitter account still allowed to continue existing up?

487
00:40:04,680 --> 00:40:10,420
Speaker 0: Why are Republican politicians who say literal lies about kovat still allowed to tweet like it's the same exactly?

488
00:40:11,264 --> 00:40:14,440
Speaker 1: The platform is should be responsible for the mud not you know.

489
00:40:14,520 --> 00:40:19,439
Speaker 1: The user shouldn't have to sit there right there and moderate and to do all this stuff.

490
00:40:20,582 --> 00:40:22,008
Speaker 1: It's you know, it's not their job.

491
00:40:22,049 --> 00:40:24,340
Speaker 1: they're there to watch streams or to stream.

492
00:40:24,761 --> 00:40:29,056
Speaker 1: They shouldn't have to be responsible for keeping racists and bots out of their chat.

493
00:40:29,096 --> 00:40:30,020
Speaker 1: Why is that their job?

494
00:40:31,180 --> 00:40:48,220
Speaker 0: yep, so twitch bears 100% of the blame for allowing this to happen because There are simple things twitch could have done or could still do that would not solve this but would have drastically Reduce this or even prevented the scale that is now occurring.

495
00:40:48,602 --> 00:40:56,172
Speaker 0: This is only gonna get worse and I have no confidence that twitch is gonna do anything Substantial to address it in the first anyway future.

496
00:40:56,634 --> 00:41:13,608
Speaker 1: But anyway on September 1st The all of these people who are you know, not happy about twitch not doing anything about this, you know racist raid problem Yeah are going to basically do a one-day twitch, you know One day twitch, I guess boycott right?

497
00:41:13,668 --> 00:41:14,430
Speaker 1: No streaming.

498
00:41:14,631 --> 00:41:16,196
Speaker 1: Yeah, no watching twitch.

499
00:41:16,236 --> 00:41:21,351
Speaker 1: don't cross the picket line Alright, so in September 1st, don't go to twitch.com.

500
00:41:21,411 --> 00:41:22,775
Speaker 1: Don't do any twitch anything.

501
00:41:22,835 --> 00:41:26,165
Speaker 1: Don't even watch like a VOD Right don't watch a clip.

502
00:41:26,586 --> 00:41:28,873
Speaker 1: don't do anything on September 1st.

503
00:41:28,933 --> 00:41:39,183
Speaker 1: No twitch streaming or watching whatsoever things Of the day.

504
00:41:39,203 --> 00:41:39,750
Speaker 0: What do you got?

505
00:41:40,770 --> 00:41:49,662
Speaker 1: So you go to the hardware store, right and such a simple thing You see like a big old piece of plywood right like a big flat piece of wood Right and every time in my life.

506
00:41:49,723 --> 00:41:56,901
Speaker 1: I saw such a piece of wood You know, I thought to myself for a second like man That obviously didn't come from a tree.

507
00:41:57,183 --> 00:42:04,864
Speaker 0: That was eight feet wide Right Where pieces of wood that big did in fact come?

508
00:42:04,884 --> 00:42:05,005
Speaker 0: from?

509
00:42:05,046 --> 00:42:05,629
Speaker 0: a tree that pig.

510
00:42:06,632 --> 00:42:08,005
Speaker 1: Sure, that time is not now.

511
00:42:08,086 --> 00:42:10,457
Speaker 1: Nope And you know, how did that?

512
00:42:10,538 --> 00:42:12,690
Speaker 1: how did that piece of wood come to be right?

513
00:42:12,830 --> 00:42:14,557
Speaker 1: Was it you know a particle board?

514
00:42:14,598 --> 00:42:15,361
Speaker 1: I understand right?

515
00:42:15,381 --> 00:42:17,670
Speaker 1: You can just get like wood chips and like compress them together.

516
00:42:17,730 --> 00:42:20,190
Speaker 1: You know with glue or something right up a particle board sucks.

517
00:42:21,193 --> 00:42:25,316
Speaker 1: It sucks ass but plywood doesn't suck is right And it's just sort of like.

518
00:42:25,356 --> 00:42:26,885
Speaker 1: hey, how did that fuck?

519
00:42:26,905 --> 00:42:27,830
Speaker 1: did they make that right?

520
00:42:28,070 --> 00:42:32,510
Speaker 1: What you know what was going on at the lumber mill that produced that eight feet wide piece of wood?

521
00:42:33,370 --> 00:42:48,330
Speaker 1: well Finally somehow I remembered to google that And on YouTube I found a video and the answer to the question of hey How did just some fucking logs that are about the size of like telephone poles end up as you know?

522
00:42:48,610 --> 00:42:50,014
Speaker 1: Ultra wide pieces of wood?

523
00:42:50,235 --> 00:42:53,283
Speaker 1: the answer is about at the three-minute mark a very ingenious device.

524
00:42:53,644 --> 00:42:55,570
Speaker 1: That was so simple and so obvious.

525
00:42:56,453 --> 00:42:59,143
Speaker 1: Why did I not just think of imagine that on my own?

526
00:42:59,183 --> 00:43:01,110
Speaker 1: but no at the three around the three-minute mark?

527
00:43:01,250 --> 00:43:03,424
Speaker 1: I saw the machine and I said, oh, that's how they fucking do it.

528
00:43:03,786 --> 00:43:04,289
Speaker 1: God damn it.

529
00:43:04,554 --> 00:43:07,134
Speaker 1: Yeah Why did I think of that?

530
00:43:07,294 --> 00:43:13,486
Speaker 0: So I find as I older I increasingly enjoy learning how very simple things that you don't think about are done.

531
00:43:14,332 --> 00:43:20,010
Speaker 1: Yeah, but also this video shows everything from literal tree with bark on it up to delivering.

532
00:43:20,030 --> 00:43:22,190
Speaker 0: Oh, yeah barkers like the first part of the video.

533
00:43:22,970 --> 00:43:29,550
Speaker 1: Yeah, it shows you everything from tree all the way to piece of wood that's on its way to the hardware store already packaged.

534
00:43:29,932 --> 00:43:31,282
Speaker 1: so it's pretty fascinating.

535
00:43:31,322 --> 00:43:36,708
Speaker 1: all the other steps and Things that go on in the the lumber mill of you know, a modern lumber mill.

536
00:43:36,909 --> 00:43:38,633
Speaker 1: so Check it out.

537
00:43:39,236 --> 00:43:40,200
Speaker 0: So check this out.

538
00:43:40,461 --> 00:43:48,306
Speaker 0: This is if this were something that was for sale It's it's in the same category as that the game and watch that you got the Nintendo one.

539
00:43:48,687 --> 00:43:50,522
Speaker 0: if this was for sale I would just buy it.

540
00:43:51,311 --> 00:43:56,369
Speaker 0: Supposedly instructions for making one are coming soon from the creator and I think I'm gonna make one.

541
00:43:57,091 --> 00:44:03,310
Speaker 0: It's this tiny little TV and it is 3d printed to look exactly like the TV from The Simpsons the original one.

542
00:44:03,390 --> 00:44:04,455
Speaker 0: Okay, it's got two little.

543
00:44:04,475 --> 00:44:04,757
Speaker 0: and then what?

544
00:44:04,777 --> 00:44:06,786
Speaker 1: you take a little LCD screen and stick it in there.

545
00:44:06,807 --> 00:44:12,629
Speaker 0: So It has a tiny little computer in it a 640 bar for a Raspberry Pi a Raspberry Pi.

546
00:44:12,649 --> 00:44:18,730
Speaker 0: Yeah It has d card on it And basically it has of one of the dot the knob on the bottom with the volume slider.

547
00:44:19,232 --> 00:44:21,564
Speaker 0: It has the speaker in it and the knob on the top.

548
00:44:21,685 --> 00:44:30,550
Speaker 0: if you turn it on It'll start playing a random Simpsons episode from the first 11 seasons and if you try it could be it could play anything.

549
00:44:30,691 --> 00:44:32,221
Speaker 1: But they chose to put the first 11.

550
00:44:32,221 --> 00:44:35,354
Speaker 0: Oh, yes, but I'm saying they The the.

551
00:44:35,394 --> 00:44:42,236
Speaker 0: what I like about this device is this is the exact kind of neat social currency Slash desk toy that I love.

552
00:44:42,677 --> 00:44:50,987
Speaker 0: it's like an advanced form of that game and watch just flick the little switch and a tiny TV Is just playing a random episode from the good run of The Simpsons.

553
00:44:51,248 --> 00:44:51,630
Speaker 1: turn it off.

554
00:44:52,172 --> 00:44:55,790
Speaker 1: Nothing stops you from putting in any essay later in there or anything else that you know.

555
00:44:56,211 --> 00:45:07,290
Speaker 0: But I like the idea that it just plays The Simpsons and I really love the fact that when this when this person made it They literally just compressed 11 seasons of The Simpsons and just stuck them on there.

556
00:45:07,370 --> 00:45:08,013
Speaker 0: And that's the only.

557
00:45:08,073 --> 00:45:09,900
Speaker 1: don't really have to compress them.

558
00:45:09,960 --> 00:45:15,396
Speaker 1: You could just put full-on, you know, mp4 H.264 videos in 1080p.

559
00:45:15,416 --> 00:45:17,061
Speaker 0: Well, they fit they have they fit 11.

560
00:45:17,061 --> 00:45:25,239
Speaker 0: So 11 seasons times the number of episodes a season of Roughly 640 by 480 video though.

561
00:45:25,419 --> 00:45:30,357
Speaker 1: It's probably I feel like you could fit the entire run of The Simpsons Just everything on a micro SD card.

562
00:45:30,478 --> 00:45:36,730
Speaker 1: Oh probably but I guess since you're since it's an art piece You probably only want the 4 by 3, you know episodes.

563
00:45:36,790 --> 00:45:38,336
Speaker 1: You don't want the 16 by 9 one.

564
00:45:38,457 --> 00:45:39,702
Speaker 0: and also because there's an art piece.

565
00:45:39,722 --> 00:45:41,469
Speaker 0: You probably only want the first 11 seasons.

566
00:45:43,133 --> 00:45:52,917
Speaker 1: Let's be clear about that As your screen is also probably not 16 by 9, you know what I would say, I think it's neat Good idea in the meta moment.

567
00:45:53,138 --> 00:46:08,848
Speaker 0: We will not be at PAX West 2021. in case you were wondering I feel like we have to announce that a few times because Most of the people who see us at cons don't bother going to our website and seem to watch our YouTube channel more than listen To the podcast itself, but we will not be there.

568
00:46:09,951 --> 00:46:15,190
Speaker 0: It is too soon for me to cram myself into a building with tens of thousands of nerds indoors.

569
00:46:16,510 --> 00:46:19,359
Speaker 0: So we're not gonna be the first packs in the u.s.

570
00:46:19,500 --> 00:46:23,791
Speaker 0: We've skipped since our first packs in 2008.

571
00:46:23,791 --> 00:46:26,302
Speaker 0: It's weird that there will be a packs that I am NOT at.

572
00:46:26,423 --> 00:46:27,950
Speaker 0: I even went to the first two packs Australia's.

573
00:46:28,570 --> 00:46:38,468
Speaker 1: So that's right Cuz even if I went and you know the even if I was you know We're willing to take the risk of getting a horrible disease, which I'm not Or giving it to someone else, which I'm not.

574
00:46:39,571 --> 00:46:47,369
Speaker 1: It feels like because of the horrible disease the packs itself will be greatly diminished and not worth paying the money to fly across the entire country to go to.

575
00:46:48,310 --> 00:46:52,985
Speaker 1: so if I lived in Seattle, there might be some amount of consideration.

576
00:46:53,026 --> 00:46:56,055
Speaker 1: probably still wouldn't go but Chances of going would be higher.

577
00:46:56,677 --> 00:46:57,921
Speaker 0: Yep, so we will not be there.

578
00:46:58,563 --> 00:47:02,576
Speaker 0: our panel from the last packs East online Can't packs Atari game design.

579
00:47:02,937 --> 00:47:03,680
Speaker 0: It's just on YouTube.

580
00:47:03,860 --> 00:47:06,068
Speaker 0: You can just watch it slides everything.

581
00:47:06,108 --> 00:47:06,550
Speaker 0: It's all there.

582
00:47:07,172 --> 00:47:09,985
Speaker 0: We're still reading the tale of Genji powered red.

583
00:47:10,045 --> 00:47:11,070
Speaker 1: I read a bunch of the beach.

584
00:47:11,110 --> 00:47:14,150
Speaker 0: Yeah, I think by the end of September we can do this show October at the latest.

585
00:47:14,752 --> 00:47:15,777
Speaker 1: You said August last?

586
00:47:16,239 --> 00:47:16,399
Speaker 0: Yeah.

587
00:47:16,420 --> 00:47:18,408
Speaker 1: Well, then the end of August is in a week.

588
00:47:18,709 --> 00:47:20,797
Speaker 0: Yep Well, I'll be I'll be more.

589
00:47:20,818 --> 00:47:23,629
Speaker 0: I'll be past the halfway point in the next week at the rate.

590
00:47:23,669 --> 00:47:24,010
Speaker 0: I'm going.

591
00:47:25,443 --> 00:47:30,570
Speaker 0: but Some notes, you know, cuz I've said the same thing about the book a few times along the way.

592
00:47:30,691 --> 00:47:36,408
Speaker 0: So to be a little more specific There were more ghost murders than I expected in the tale of Genji.

593
00:47:36,669 --> 00:47:37,533
Speaker 1: ghosts It's also.

594
00:47:37,553 --> 00:47:41,249
Speaker 1: there's also, you know more Hamlet style goes coming to talk.

595
00:47:41,711 --> 00:47:46,890
Speaker 0: Yep, ghosts played more of a factor than I expected from the tale of Genji and I'm here for that.

596
00:47:47,662 --> 00:47:51,062
Speaker 0: I also Did not expect on a modern reading like again.

597
00:47:51,142 --> 00:48:03,418
Speaker 0: as you get further into the book and you think back to Who's sick when and what they complain about being sick with it's clear that half the characters in this fucking book literally have malaria Mmm, like specifically malaria or

598
00:48:03,459 --> 00:48:11,430
Speaker 1: be just be well, yeah, I mean there are people who have malaria, but there's also Just because it's such an old time before modern medicine people are sick a lot.

599
00:48:11,490 --> 00:48:14,501
Speaker 1: So it's true to you know, it's true to life in that sense.

600
00:48:15,123 --> 00:48:16,247
Speaker 0: So we'll get right into it.

601
00:48:17,613 --> 00:48:19,179
Speaker 0: We're gonna talk about the Apple thing.

602
00:48:19,260 --> 00:48:20,987
Speaker 0: that has honestly.

603
00:48:21,208 --> 00:48:24,317
Speaker 0: interestingly I have a very mixed and ambivalent opinion on this.

604
00:48:24,698 --> 00:48:33,761
Speaker 0: but the people in technology spaces that I know professionally or that I follow drastically out Like I know some I know of some.

605
00:48:33,901 --> 00:48:39,824
Speaker 0: these are some people I know and I've talked to about this and they're also People I just follow because I respect like Lawrence Lessig.

606
00:48:39,844 --> 00:48:42,234
Speaker 0: I respect like there's people I respect Yeah, they're.

607
00:48:42,314 --> 00:48:44,564
Speaker 0: all of them are pretty much 50/50 on this issue.

608
00:48:44,584 --> 00:48:59,267
Speaker 0: half of them are Demanding that Apple not do this extremely afraid of the negative consequences and the other half of them are basically arguing It's not really that much different from what everyone else is doing and we don't really care that much.

609
00:48:59,749 --> 00:49:02,399
Speaker 0: So This is authority issue.

610
00:49:02,640 --> 00:49:05,069
Speaker 1: I'm pretty much seeing only outrage from everyone except me.

611
00:49:05,830 --> 00:49:14,184
Speaker 0: I've seen a number of privacy advocates argue that it's not a big deal Mostly citing the fact that pretty much every other platform already does this.

612
00:49:14,384 --> 00:49:16,170
Speaker 1: So let's explain what's going on.

613
00:49:16,250 --> 00:49:20,670
Speaker 1: Yeah, so first of all, so what Apple is doing is they're doing three things, right?

614
00:49:20,810 --> 00:49:33,750
Speaker 1: The second and I'm gonna do them in a different date they explain them in this page on their website Apple comm slash child - safety and I'll talk about the two not so exciting ones first and then the big deal one last even though on the website the big deal.

615
00:49:34,872 --> 00:49:36,599
Speaker 1: So the first one is there.

616
00:49:36,699 --> 00:49:53,390
Speaker 1: it's called Apple calls it communication safety in messages And basically what it is is if you are a child and you have an iPhone that's hooked up to your parents iPhone But the parental protections and everything in which case your parents can already just unlock your phone and see everything is going on in There anyway, right?

617
00:49:55,056 --> 00:49:57,346
Speaker 1: Your entire Apple stuff is beholden to your parents.

618
00:49:57,446 --> 00:50:02,529
Speaker 1: if that is the case So It's not really a change from that.

619
00:50:02,991 --> 00:50:07,988
Speaker 1: But what the system they have added does is it allows the parents to you know?

620
00:50:08,470 --> 00:50:22,249
Speaker 1: Enable a thing where it'll be enabled by default and I guess you could disable it if you really want to but basically use machine Learning to see if the child has received or sent a photo that very likely has nakedness in it.

621
00:50:23,491 --> 00:50:24,214
Speaker 1: Right which?

622
00:50:24,254 --> 00:50:32,130
Speaker 1: and if a child is getting it is either a child looking at porn They shouldn't be or a child sending child porn of themselves usually, right?

623
00:50:32,671 --> 00:50:36,888
Speaker 1: Yeah, which shouldn't be and all it does is it will blur it to.

624
00:50:37,149 --> 00:50:40,202
Speaker 1: you know Make it so the child can't doesn't see it immediately.

625
00:50:40,443 --> 00:50:42,170
Speaker 1: So someone sends a dick pic to your kid.

626
00:50:42,210 --> 00:50:45,226
Speaker 1: They don't just immediately see the dick or your dick pic to someone.

627
00:50:46,030 --> 00:50:50,150
Speaker 1: They know they'll get a blurry image and like a warning like are you sure you want to see this?

628
00:50:50,330 --> 00:50:53,890
Speaker 1: Which I think is a feature that you just enable for everybody like hey, someone sent you a photo.

629
00:50:53,950 --> 00:50:54,814
Speaker 1: That's probably a dick.

630
00:50:54,875 --> 00:50:55,860
Speaker 1: Are you sure you want to see it?

631
00:50:55,900 --> 00:50:56,121
Speaker 1: Yes.

632
00:50:56,201 --> 00:50:58,010
Speaker 1: No, why is that for kids only?

633
00:50:58,150 --> 00:50:58,431
Speaker 1: That's just me.

634
00:50:58,452 --> 00:51:02,150
Speaker 1: for everybody if they want to enable that that should be enabled by default.

635
00:51:03,450 --> 00:51:10,169
Speaker 1: Regardless, it'll tell the parent like hey your kid You know got a message that might be naked.

636
00:51:11,011 --> 00:51:14,986
Speaker 1: Letting you know right and I don't really see the problem.

637
00:51:15,046 --> 00:51:17,369
Speaker 1: I don't see this as a problem It's just a helpful thing.

638
00:51:17,791 --> 00:51:22,104
Speaker 1: That's not any different because parents already effectively have complete control of their children's phones.

639
00:51:22,425 --> 00:51:24,110
Speaker 1: that they're paying that the adults are paying for.

640
00:51:24,892 --> 00:51:26,398
Speaker 0: So one avenue.

641
00:51:26,478 --> 00:51:40,688
Speaker 0: I see that these the chatter I saw on that was primarily around young under 18 still under their parents control children who have conservative religious parents who don't approve of their sexuality or lifestyle.

642
00:51:40,709 --> 00:51:47,430
Speaker 0: and Yep, this was one of the few truly encrypted avenues they would have to communicate with people.

643
00:51:48,012 --> 00:51:52,930
Speaker 1: Well a as I said that I completely agree with what you just said.

644
00:51:53,070 --> 00:51:59,204
Speaker 1: But the parents already had complete control of the phones because that their kids phone is not the parent It's the parents phone that the kid is using.

645
00:51:59,224 --> 00:52:03,850
Speaker 1: with the parental controls The parents could already just I assume just unlock the kids phone and see whatever.

646
00:52:03,991 --> 00:52:05,239
Speaker 0: I think in that case we need to.

647
00:52:05,279 --> 00:52:14,590
Speaker 0: we need more of the old awareness Awareness campaigns that used to happen in the 90s among nerds Around how to do encryption when you don't trust the devices and how to do encryption.

648
00:52:14,710 --> 00:52:17,540
Speaker 0: Yes, now that is difficult and that will require some work.

649
00:52:17,921 --> 00:52:24,788
Speaker 0: like you can almost make a nonprofit around teaching Especially marginalized kids how to use encryption that is platform agnostic.

650
00:52:25,672 --> 00:52:29,350
Speaker 0: So I think there are effective ways around that for kids who need it, right?

651
00:52:29,410 --> 00:52:32,405
Speaker 1: But also Apple has informed everyone of this.

652
00:52:32,466 --> 00:52:33,310
Speaker 1: This isn't a secret.

653
00:52:33,510 --> 00:52:42,569
Speaker 1: So there I don't know if a kid is in that situation and afraid of their parents finding out their Sexuality because their parents are pieces of shit.

654
00:52:43,212 --> 00:52:47,049
Speaker 1: You're just not going to be sending any of that stuff on your iMessage now.

655
00:52:47,331 --> 00:52:50,369
Speaker 1: You'll find another way to send it if you're gonna send it at all.

656
00:52:50,971 --> 00:52:52,195
Speaker 1: Because you know about this.

657
00:52:52,275 --> 00:52:53,359
Speaker 1: Apple told you that it's there.

658
00:52:53,399 --> 00:52:54,643
Speaker 1: It's not like surprise.

659
00:52:54,703 --> 00:52:57,010
Speaker 1: We told you we told your parents that you're gay.

660
00:52:58,334 --> 00:52:58,843
Speaker 1: It's you know.

661
00:53:00,019 --> 00:53:01,734
Speaker 1: so It could be.

662
00:53:01,754 --> 00:53:03,421
Speaker 1: it could cause a problem for somebody.

663
00:53:03,482 --> 00:53:05,974
Speaker 1: I guess it was unaware or something But I don't.

664
00:53:06,437 --> 00:53:10,854
Speaker 1: I'm not too worried about that You know given that everyone knows.

665
00:53:10,894 --> 00:53:12,297
Speaker 1: they just announced it to the world.

666
00:53:12,998 --> 00:53:15,391
Speaker 1: All right, the second thing Really?

667
00:53:15,452 --> 00:53:17,718
Speaker 1: Not a big deal at all barely even worth mentioning.

668
00:53:18,179 --> 00:53:22,330
Speaker 1: is that if you go to Siri or search on your phone?

669
00:53:24,551 --> 00:53:30,989
Speaker 1: If you try to search for see Sam, which I guess is the new term for instead of saying child porn You're supposed to say see Sam.

670
00:53:32,052 --> 00:53:34,343
Speaker 0: So it's the child sexual abuse material.

671
00:53:34,484 --> 00:53:37,172
Speaker 0: It usually refers to Databases.

672
00:53:37,393 --> 00:53:40,421
Speaker 0: governments maintain there's a the whole this to get a whole thing on this.

673
00:53:40,983 --> 00:54:02,210
Speaker 0: but basically law enforcement agencies maintain Databases of all the child porn that's ever been uncovered Partly to help study it to identify victims to find victims who have not been helped yet like traffic kids and Also to be able to identify it when someone else is found to be distributing it.

674
00:54:02,391 --> 00:54:10,610
Speaker 1: There's a lot of reasons why these databases exist Sure, but child sexual abuse material is just like the phrase that you know, they're using instead of saying out for it.

675
00:54:10,750 --> 00:54:16,010
Speaker 1: So there are databases that because I guess they like that term better because it's more specific, right?

676
00:54:16,210 --> 00:54:16,290
Speaker 1: It's.

677
00:54:16,652 --> 00:54:33,599
Speaker 0: but it's also more generic and that it covers all the different Ways like all the kinds of material that could be abusive in this manner Sure, but anyway if you try to search for a child sexual abuse material On your iPhone or as Siri about it, right?

678
00:54:33,679 --> 00:54:37,450
Speaker 1: It is now going to give you better help in in those regards.

679
00:54:38,475 --> 00:54:41,109
Speaker 1: So that's I guess only positive, right?

680
00:54:42,552 --> 00:54:46,270
Speaker 1: So the big thing that Apple's doing right is the see Sam detection.

681
00:54:46,831 --> 00:54:49,799
Speaker 1: So first of all, let's talk about how it works.

682
00:54:49,879 --> 00:54:56,578
Speaker 1: The way it works is this you have some photos on your phone And not only do you have photos on your phone?

683
00:54:56,979 --> 00:54:59,248
Speaker 1: You have enabled iCloud photo.

684
00:54:59,268 --> 00:54:59,890
Speaker 1: You're not right.

685
00:55:00,070 --> 00:55:06,570
Speaker 1: So that's probably by default on most iPhones, but you could disable it if you wanted to but you have iCloud photo enabled.

686
00:55:07,234 --> 00:55:09,770
Speaker 1: What they're gonna do is on your phone.

687
00:55:10,551 --> 00:55:13,263
Speaker 1: It's going to take hashes of all your photos.

688
00:55:13,303 --> 00:55:14,930
Speaker 1: It's not going to share your photos with Apple.

689
00:55:15,091 --> 00:55:18,250
Speaker 1: No one will see the picture of your birthday cake, right?

690
00:55:18,470 --> 00:55:20,236
Speaker 1: They're just gonna see a number and they'll have.

691
00:55:20,316 --> 00:55:25,635
Speaker 1: no I there'll be no mathematical way to figure out what fucking photo that number came from.

692
00:55:25,816 --> 00:55:26,639
Speaker 1: just like no way.

693
00:55:26,799 --> 00:55:32,749
Speaker 1: it's impossible mathematically impossible, but They have a database of child porn, right?

694
00:55:33,650 --> 00:55:36,867
Speaker 1: Because the government has it and what they'll do is they'll see huh?

695
00:55:37,910 --> 00:55:42,706
Speaker 1: Did any of the numbers on any of your photos match any of this?

696
00:55:43,047 --> 00:55:43,850
Speaker 1: see Sam, right?

697
00:55:44,191 --> 00:55:47,190
Speaker 1: That's not so try not to say child porn now because they told me not to right.

698
00:55:47,371 --> 00:55:52,027
Speaker 1: We have these numbers from photos of see Sam and we have the numbers from your photos.

699
00:55:52,067 --> 00:55:59,803
Speaker 1: if there's any matches It is mathematically Extremely likely that you have that shit on your phone.

700
00:56:00,004 --> 00:56:03,661
Speaker 1: It would be almost impossible For you to have a photo.

701
00:56:03,983 --> 00:56:11,370
Speaker 1: not absolutely impossible But almost impossible for you to have a photo on your phone and have the hash produced from that photo match.

702
00:56:11,774 --> 00:56:13,169
Speaker 1: One of the hash is in that database.

703
00:56:13,954 --> 00:56:17,290
Speaker 1: Right and even if there is some kind of match, right?

704
00:56:17,650 --> 00:56:22,356
Speaker 1: They've set up this whole system to where like yeah Apple's computers are doing this part.

705
00:56:22,757 --> 00:56:24,784
Speaker 1: someone else's computers are doing this part.

706
00:56:25,045 --> 00:56:27,272
Speaker 1: humans are looking at this part Right.

707
00:56:27,553 --> 00:56:39,007
Speaker 1: for it to actually reach the point at which you're in trouble because of this It would have to be like, you know almost guaranteed that like yeah, you got that shit on your phone.

708
00:56:39,027 --> 00:56:39,589
Speaker 1: what's wrong with you?

709
00:56:41,271 --> 00:56:44,600
Speaker 1: So I'm really confident there aren't gonna be false positives.

710
00:56:44,660 --> 00:56:46,706
Speaker 1: So what are people worried about?

711
00:56:46,826 --> 00:56:52,590
Speaker 1: Well, people are worried not at This technology here itself, right?

712
00:56:53,932 --> 00:56:55,015
Speaker 1: What's actually happening?

713
00:56:55,095 --> 00:56:58,184
Speaker 1: They are worried about these slippery slope, right?

714
00:56:58,224 --> 00:57:07,987
Speaker 1: Cuz once this system is in place For example just to make something up Apple could take an image of I don't know

715
00:57:08,147 --> 00:57:10,709
Speaker 0: leaked Apple product design, let's say

716
00:57:11,532 --> 00:57:28,857
Speaker 1: Sure, right an iPhone 13 and then they could be like well now, you know We add the hash we add that number of this photo of the iPhone 15 to our database and if anyone out there has a photo of an iPhone 15 on their iPhone Well, we know that they're a leaker, right?

717
00:57:29,238 --> 00:57:32,930
Speaker 1: They could add, you know, they could apply this to things besides photos.

718
00:57:33,412 --> 00:57:37,509
Speaker 1: Right because it's happening locally on your phone where decrypted things could be.

719
00:57:38,192 --> 00:57:40,241
Speaker 1: So your decrypted iMessages right.

720
00:57:40,301 --> 00:57:52,198
Speaker 1: Apple doesn't see your iMessages when they're in transit but if they could look at your iMessages and They could be like, yeah, what's the hash of the message that says, you know I hate the president right event?

721
00:57:52,338 --> 00:57:55,990
Speaker 1: or if anyone has a text message as I hate the president, right?

722
00:57:56,150 --> 00:58:00,305
Speaker 1: They could detect that specifically right without you know, because they've got.

723
00:58:00,566 --> 00:58:01,570
Speaker 1: they don't have the messages.

724
00:58:01,651 --> 00:58:07,244
Speaker 1: They're not seeing your messages but they have hash representations of all your messages so they can sort.

725
00:58:07,285 --> 00:58:16,070
Speaker 0: what they can do is they can just Take messages and search for them specifically take a new message generate the hash for that message and see if anyone sent that message.

726
00:58:16,693 --> 00:58:17,900
Speaker 1: Yeah, did anyone say this?

727
00:58:18,263 --> 00:58:19,450
Speaker 1: Oh, we caught you, right?

728
00:58:20,150 --> 00:58:31,497
Speaker 0: So well, so this actually gets a little bit worse in in the US in particular And I've seen a few security researchers make this point about the US federal government and gag orders national security letters.

729
00:58:31,979 --> 00:58:46,039
Speaker 0: the US government could in theory and Has done this in the past with other platforms and technologies say I have a technology platform with end-to-end encryption and I do not have a Mechanism to decrypt that information like built-in.

730
00:58:46,461 --> 00:58:49,534
Speaker 0: the government cannot force me to build new technology Like.

731
00:58:49,554 --> 00:58:52,249
Speaker 0: that is not something they can do with an NSA letter or an NSA.

732
00:58:52,611 --> 00:58:58,626
Speaker 0: They can't be like you are hereby order to build this technology for us like that That they would have to make a government contract to do that.

733
00:58:58,847 --> 00:58:59,089
Speaker 0: Why?

734
00:59:00,210 --> 00:59:01,569
Speaker 1: Pay it do and you have to agree to do it.

735
00:59:01,610 --> 00:59:02,494
Speaker 0: But if Apple?

736
00:59:02,574 --> 00:59:08,270
Speaker 0: if it is no now that Apple has this specific capability It is eminently plausible.

737
00:59:08,370 --> 00:59:18,838
Speaker 0: the US federal government could issue a national security letter with a gag order to Apple saying add these hashes to Your list and inform us if anything hits.

738
00:59:19,179 --> 00:59:26,805
Speaker 0: we will not tell you what the hashes are for and you under penalty of law Cannot inform anyone that you have added these hashes, right?

739
00:59:26,986 --> 00:59:27,970
Speaker 1: They could fight it in court.

740
00:59:28,030 --> 00:59:31,690
Speaker 0: But oh, they can fight that in a secretly a special secret court.

741
00:59:31,810 --> 00:59:32,974
Speaker 1: Yes, guess what?

742
00:59:33,014 --> 00:59:33,957
Speaker 1: probably are there?

743
00:59:33,997 --> 00:59:38,089
Speaker 1: probably are many such fights happening in secret courts all the time that we don't know anything about.

744
00:59:38,189 --> 00:59:39,238
Speaker 0: guess What you know.

745
00:59:39,258 --> 00:59:40,409
Speaker 0: why you don't find out about those.

746
00:59:40,731 --> 00:59:45,930
Speaker 1: Nobody ever wins their cases because they're all you wouldn't find out even if they did win because it happened in secret court.

747
00:59:45,990 --> 00:59:56,885
Speaker 0: Yep, but the specific danger is that by having the capability they've opened themselves up to the government issuing a directive like that in a Way that if they don't have the capability the government can't force them to build it.

748
00:59:57,306 --> 01:00:03,049
Speaker 0: That seems to be Primary Concern among the people I see who are the maddest about this.

749
01:00:03,934 --> 01:00:05,481
Speaker 1: Sure, it is a true.

750
01:00:05,723 --> 01:00:06,647
Speaker 1: That is a true concern.

751
01:00:06,688 --> 01:00:13,330
Speaker 1: Yes So but yeah, that is pretty much like the only concern, right?

752
01:00:13,350 --> 01:00:16,590
Speaker 1: It's like if Apple you know, cuz it's really a slippery slope thing, right?

753
01:00:16,690 --> 01:00:27,015
Speaker 1: It's like you're building this system But all of the negatives that everyone comes up with whether they're plausible ones like the one you just said right or any others They're all ifs.

754
01:00:27,517 --> 01:00:30,368
Speaker 1: it's like every single negative that anyone has come up with.

755
01:00:30,488 --> 01:00:30,870
Speaker 0: that's true.

756
01:00:30,930 --> 01:00:32,237
Speaker 0: But is it is a?

757
01:00:32,719 --> 01:00:32,960
Speaker 1: is a?

758
01:00:33,061 --> 01:00:34,970
Speaker 1: this negative could happen?

759
01:00:35,110 --> 01:00:40,902
Speaker 0: There's one important danger of the other thing I brought up though in that There's no way to ever know if the if have.

760
01:00:40,922 --> 01:00:42,308
Speaker 1: there is no way to know.

761
01:00:42,388 --> 01:00:45,018
Speaker 0: that is We could find out if they happen.

762
01:00:45,038 --> 01:00:45,541
Speaker 0: That's the one.

763
01:00:45,661 --> 01:00:50,519
Speaker 1: if there's literally no way to know There's also no evidence that the thing did happen.

764
01:00:50,559 --> 01:00:54,210
Speaker 0: But again there but there is evidence that the thing has happened on every other platform.

765
01:00:55,631 --> 01:00:55,993
Speaker 1: That's true.

766
01:00:56,114 --> 01:00:59,029
Speaker 0: so you think in that it did not have capabilities like this.

767
01:01:00,170 --> 01:01:05,286
Speaker 1: It's Apple does, you know, right and that is a good point in Apple's favor right.

768
01:01:05,386 --> 01:01:08,798
Speaker 1: Apple has always Said hey, look, we're not here.

769
01:01:08,818 --> 01:01:12,250
Speaker 1: You know, we're the most privacy conscious of all your options, right?

770
01:01:12,330 --> 01:01:14,689
Speaker 1: We're here to sell you the product for a large amount of money.

771
01:01:15,071 --> 01:01:16,376
Speaker 1: You aren't the product right.

772
01:01:16,416 --> 01:01:20,130
Speaker 1: the prod, you know, like you are with all those people who give you stuff for free like Google.

773
01:01:20,673 --> 01:01:23,649
Speaker 1: Right, it's like everyone else is already doing this.

774
01:01:24,132 --> 01:01:29,348
Speaker 1: Right, if they some of them might be telling you they do it Some of them might not be telling you they do it.

775
01:01:29,751 --> 01:01:32,883
Speaker 1: Some of them might be doing it way worse than like Google doesn't we know?

776
01:01:32,923 --> 01:01:33,485
Speaker 0: Google does it?

777
01:01:33,505 --> 01:01:34,810
Speaker 0: they publish information about?

778
01:01:35,070 --> 01:01:39,650
Speaker 0: Statistics of like things how many things they take down and how many violations?

779
01:01:40,430 --> 01:01:43,099
Speaker 0: Looks like Google in the Latin.

780
01:01:43,280 --> 01:01:55,850
Speaker 0: in 2020 Google disabled almost a hundred thousand accounts because CSAM material was identified mm-hmm, so 210,000 URLs were de-indexed because they included CSAM material.

781
01:01:56,554 --> 01:01:57,541
Speaker 0: I think the difference is.

782
01:01:57,601 --> 01:02:02,981
Speaker 0: the difference is and again I have not audited the specifics of how every different company implements this stuff.

783
01:02:03,363 --> 01:02:09,299
Speaker 0: The difference is that all these other platforms are Inspecting unencrypted content that they have access to.

784
01:02:09,359 --> 01:02:12,590
Speaker 0: is that a store that they have encrypted on behalf of a user?

785
01:02:13,032 --> 01:02:21,870
Speaker 0: This is the first time an end-to-end encrypted thing itself is Inspecting Personal content not shared content and not content.

786
01:02:21,970 --> 01:02:25,502
Speaker 1: I don't think the iCloud photos are truly end-to-end encrypted.

787
01:02:25,542 --> 01:02:30,182
Speaker 1: the way I message or like, you know in such is I'm not actually a hundred percent sure myself because I don't have an app.

788
01:02:30,222 --> 01:02:33,974
Speaker 1: I'm pretty sure it's just You know, I don't think it's.

789
01:02:34,075 --> 01:02:35,580
Speaker 1: it's fully end-to-end encrypted.

790
01:02:35,600 --> 01:02:38,110
Speaker 1: there's a point at which Apple can see your your photos.

791
01:02:38,351 --> 01:02:41,606
Speaker 0: So there's a fascinating article on the Washington Post that I did.

792
01:02:41,666 --> 01:02:42,570
Speaker 0: it was it was published today.

793
01:02:43,454 --> 01:02:45,188
Speaker 0: Oh No, it's published a couple days ago.

794
01:02:45,514 --> 01:02:46,389
Speaker 0: I just saw it today.

795
01:02:46,891 --> 01:02:54,972
Speaker 0: but it's some researchers who built a very similar system to do almost the exact same thing and Effectively.

796
01:02:55,032 --> 01:02:56,397
Speaker 0: how they canceled it because it was.

797
01:02:56,477 --> 01:02:59,366
Speaker 0: they decided it was too dangerous based on what they discovered.

798
01:02:59,406 --> 01:03:05,220
Speaker 0: building such a platform Too easily abused you get I'm gonna leave easily abused.

799
01:03:05,260 --> 01:03:05,581
Speaker 1: Yes.

800
01:03:05,681 --> 01:03:43,638
Speaker 0: Yep, because like I said, I have a complex and Somewhat ambivalent opinion on this in that on one hand I would argue that if you truly truly care about Powerful end-to-end encryption where you need to protect yourself from state-sponsored actors which includes the US government because the US government is the primary entity that could issue a Security letter and leverage this technology from al-fizan's you need to be doing what we talked about the beginning of the show and have Actual end-to-end encryption that is platform agnostic that you yourself are managing using their own technology,

801
01:03:43,658 --> 01:03:43,858
Speaker 1: right?

802
01:03:44,298 --> 01:03:50,484
Speaker 1: You should be using your own platforms Like right your own hardware your own software.

803
01:03:50,524 --> 01:04:00,910
Speaker 1: if you're using anything Right run by some other provider at all Even if they're promising or verifying, you know giving you some sort of encryption like iMessages.

804
01:04:01,534 --> 01:04:03,063
Speaker 1: It's like you can't bet.

805
01:04:03,123 --> 01:04:03,969
Speaker 1: you can't trust that.

806
01:04:05,411 --> 01:04:10,124
Speaker 1: Right, if you really need the privacy run your own Linux server in your house.

807
01:04:10,645 --> 01:04:15,200
Speaker 1: get make your own phone Get some entropy like from your local environment.

808
01:04:17,930 --> 01:04:24,633
Speaker 1: Yeah, it's like, you know use Wi-Fi only you can't be going over Verizon t-mobile or anything like that Right.

809
01:04:24,733 --> 01:04:25,517
Speaker 1: It's like you can't?

810
01:04:25,537 --> 01:04:28,008
Speaker 1: it's just you know, it's just how it is.

811
01:04:28,671 --> 01:04:31,001
Speaker 1: As soon as you use someone else's service, that's it.

812
01:04:31,061 --> 01:04:32,910
Speaker 1: You gave them everything the end.

813
01:04:34,964 --> 01:04:36,713
Speaker 1: So That's one thing.

814
01:04:36,733 --> 01:04:52,890
Speaker 1: The second thing is Apple's getting a ton of flack for this which is partially because a They've decided to tell everyone not only that they're doing it But all the details of how they're doing it and because of their privacy conscious, you know message they've had for so long.

815
01:04:53,012 --> 01:04:56,036
Speaker 1: It's a bigger contradiction I think the latter Google to do it.

816
01:04:56,096 --> 01:05:11,027
Speaker 1: the latter is a bigger key because but if you look at what they're doing They're sticking to their privacy conscious, you know ideas and they're doing it in a better More privacy sensitive way than anyone else is doing it.

817
01:05:11,188 --> 01:05:12,933
Speaker 0: They are Except for.

818
01:05:13,014 --> 01:05:19,090
Speaker 0: obviously the only concern I have is the avenue where the government could force them secretly to leverage this technology.

819
01:05:19,592 --> 01:05:23,370
Speaker 1: But already it's been that way for Google Dropbox Microsoft Facebook.

820
01:05:23,510 --> 01:05:27,103
Speaker 1: That's already been that way for all of them who already have had these systems up.

821
01:05:27,223 --> 01:05:29,230
Speaker 1: Yeah in a far worse implementation.

822
01:05:29,491 --> 01:05:37,585
Speaker 0: What it means for me as a just like a person using technology is that This platform is now the same as all of those platforms.

823
01:05:37,645 --> 01:05:38,770
Speaker 0: It is not different or special.

824
01:05:39,494 --> 01:05:40,981
Speaker 1: It is not different.

825
01:05:41,021 --> 01:05:41,745
Speaker 1: It is slightly.

826
01:05:41,826 --> 01:05:43,515
Speaker 1: it is still Better.

827
01:05:43,777 --> 01:05:47,576
Speaker 1: it is not, you know, it is still Not private.

828
01:05:48,139 --> 01:05:50,330
Speaker 1: So but it is better than the other option.

829
01:05:50,350 --> 01:05:54,403
Speaker 0: What's also interesting is that Apple a gap between them has decreased.

830
01:05:54,744 --> 01:05:59,391
Speaker 0: Apple has been doing this for iCloud mail Since 2019.

831
01:05:59,391 --> 01:06:09,590
Speaker 0: Yep, no one cared because all the platforms that do this primarily are doing it on communication and Like storage but like storage that's shared or unencrypted.

832
01:06:09,710 --> 01:06:10,654
Speaker 0: I guess they're like.

833
01:06:10,694 --> 01:06:16,255
Speaker 0: this case is a slightly different twist because of Apple's Privacy message as you described.

834
01:06:16,275 --> 01:06:17,861
Speaker 0: that message cause the dissonance.

835
01:06:18,101 --> 01:06:20,650
Speaker 1: No Apple has also simply just told everyone.

836
01:06:20,750 --> 01:06:28,095
Speaker 1: Hey, if you just disable iCloud photo, which is a single button press and just keep your photos on your phone only This won't happen to you.

837
01:06:28,656 --> 01:06:31,024
Speaker 1: So there's they given you a way around it.

838
01:06:31,345 --> 01:06:32,730
Speaker 1: They just told you how to undo it.

839
01:06:33,173 --> 01:06:41,912
Speaker 0: I would do things I would personally Yeah, like I might switch to Apple when the next iPhones come out if the the error of the small phone But he just died.

840
01:06:41,952 --> 01:06:42,835
Speaker 0: I might just be fucked.

841
01:06:42,855 --> 01:06:44,039
Speaker 1: So we'll find out.

842
01:06:44,119 --> 01:06:44,962
Speaker 0: we're gonna find out.

843
01:06:45,524 --> 01:06:51,249
Speaker 0: I uh, I was salivating over that Supposedly leaked iPod or iPhone nano that they never made.

844
01:06:52,174 --> 01:06:52,898
Speaker 1: Yeah, never made.

845
01:06:52,938 --> 01:06:58,990
Speaker 0: back in the days of like in the tin 2010 I mean my iPod was still one of the best pieces of technology I ever owned in my life.

846
01:06:59,771 --> 01:07:06,911
Speaker 0: Yeah, pretty much if they made an iPhone nano like I would pay thousands of dollars for an iPhone I would have bought if they had made it in 2011.

847
01:07:06,911 --> 01:07:09,221
Speaker 1: I would have bought it and stuck with it forever if they made one today.

848
01:07:09,321 --> 01:07:11,511
Speaker 0: I would pay laptop prices in 2011.

849
01:07:11,511 --> 01:07:14,542
Speaker 1: I want I was specifically said I wanted an iPhone nano in 2010 2011.

850
01:07:14,542 --> 01:07:15,225
Speaker 1: They never made it.

851
01:07:17,354 --> 01:07:24,630
Speaker 0: But and if my threat because remember though They're only the avenue that I personally have the most concern about is the national security letter thing.

852
01:07:25,313 --> 01:07:33,063
Speaker 0: If the threat you are trying to protect yourself against is the US government You have got to you've got to use platform agnostic encryption.

853
01:07:33,243 --> 01:07:35,350
Speaker 0: Anyway, yeah there are iPhone.

854
01:07:35,972 --> 01:07:41,450
Speaker 0: I'm not fussed about this and this will not factor into my decision to use or not use Apple products.

855
01:07:42,332 --> 01:07:52,510
Speaker 1: Yeah as a more general point right is that almost as I said almost all the concerns Right are slippery slope concerns, right?

856
01:07:52,590 --> 01:08:02,849
Speaker 1: They're saying You know because the system as it is if we take Apple's word for it and we assume that there is or have not been Any national security letters adding other things to this?

857
01:08:03,430 --> 01:08:09,370
Speaker 1: Right if we just look at what it is and assume that we will not go over the slope in good faith, right?

858
01:08:09,410 --> 01:08:10,514
Speaker 1: We're not going down the slope.

859
01:08:11,257 --> 01:08:12,562
Speaker 1: This is only a positive thing.

860
01:08:12,583 --> 01:08:14,490
Speaker 1: This is gonna stop shit tons of CSAM.

861
01:08:14,490 --> 01:08:16,798
Speaker 1: Yeah, it's gonna catch all sorts of child predators.

862
01:08:16,818 --> 01:08:17,962
Speaker 1: This is a huge benefit.

863
01:08:18,263 --> 01:08:20,652
Speaker 1: No, it's like no one's focusing on that at all Right.

864
01:08:20,733 --> 01:08:24,729
Speaker 1: It's like if you don't go down the slippery slope, this is great.

865
01:08:26,290 --> 01:08:33,256
Speaker 0: However, the same argument is made by people who say we should abolish and end encryption Absolutely and require that's like.

866
01:08:33,277 --> 01:08:35,484
Speaker 1: there is a huge difference between that and this.

867
01:08:36,327 --> 01:08:44,919
Speaker 1: and the huge difference Is that if you abolish end-to-end encryption, right you were actually creating a Security problem, right?

868
01:08:44,939 --> 01:08:46,569
Speaker 0: Yeah, but you don't y'all to see for him.

869
01:08:47,310 --> 01:08:50,462
Speaker 1: Well, but you would also cause lots and lots of collateral damage.

870
01:08:50,502 --> 01:08:52,330
Speaker 1: That wouldn't be a slippery slope.

871
01:08:52,410 --> 01:08:54,676
Speaker 1: It would be a reality right of where.

872
01:08:54,898 --> 01:09:00,274
Speaker 1: yeah, if there's no end-to-end encryption, then that means I can't make a bank transaction Safely, right?

873
01:09:00,576 --> 01:09:01,398
Speaker 1: It's like you.

874
01:09:02,020 --> 01:09:05,050
Speaker 1: it's like already a built-in consequence here.

875
01:09:05,551 --> 01:09:07,962
Speaker 1: It's like they don't have to add more hashes.

876
01:09:08,524 --> 01:09:09,770
Speaker 0: So I did see one other.

877
01:09:10,252 --> 01:09:10,875
Speaker 1: It's not again.

878
01:09:10,915 --> 01:09:14,290
Speaker 1: It's not a guaranteed thing that any collateral damage will happen.

879
01:09:14,691 --> 01:09:25,707
Speaker 0: I found a story about a crypto audit project and a number of researchers pointed out that they were testing that had the neural hash Mechanism that Apple's using and they found lots of collisions very quickly.

880
01:09:26,913 --> 01:09:28,602
Speaker 1: Well, what they were doing was intentionally.

881
01:09:28,703 --> 01:09:29,506
Speaker 1: I looked at some of that.

882
01:09:29,527 --> 01:09:31,350
Speaker 0: Yeah some Definitely.

883
01:09:32,290 --> 01:09:33,658
Speaker 0: There's a lot of caveats on.

884
01:09:33,679 --> 01:09:35,290
Speaker 0: they were somewhat synthetic examples.

885
01:09:36,171 --> 01:09:40,600
Speaker 1: Well, not only that I saw someone who's like hey All of these images will make a hash collision.

886
01:09:40,621 --> 01:09:43,613
Speaker 1: a hundred percent of those images were Clearly like.

887
01:09:44,055 --> 01:09:59,956
Speaker 1: I didn't actually click through to the site because I had guessed what they were doing, but I saw a thumbnail of the site they had taken see Sam imagery and Just modified it to like make it blurry or whatever and it's like yeah Glad I didn't click through to you, but

888
01:09:59,976 --> 01:10:03,548
Speaker 0: also those are the collisions that you that would be a feature of the platform.

889
01:10:05,470 --> 01:10:09,370
Speaker 1: Exactly the point I you know, just like the more general point, right?

890
01:10:09,470 --> 01:10:14,730
Speaker 1: Is it not only with this but with a lot of things in our in technologically in the world?

891
01:10:15,331 --> 01:10:27,287
Speaker 1: There are slippery slopes that like yeah if you build, you know And I think we get this a lot because we've seen so many Twilight Zone episodes We've seen slippery slopes just ramp down to the bottom instantly.

892
01:10:27,327 --> 01:10:33,327
Speaker 1: in the real world We have seen slippery slopes in reality, you know nuclear power to nuclear bomb, right?

893
01:10:33,808 --> 01:10:35,694
Speaker 1: That's real That's not some made-up thing.

894
01:10:35,714 --> 01:10:36,155
Speaker 1: Right?

895
01:10:36,376 --> 01:10:39,785
Speaker 0: Well, no, actually that's not true because we went from nuclear bomb to nuclear power.

896
01:10:39,805 --> 01:10:46,320
Speaker 0: I Guess technically we didn't have nuclear power generation until long after we had nuclear bombs.

897
01:10:47,852 --> 01:10:51,645
Speaker 0: In fact, you get the bomb is arguably easier to make than a effective nuclear.

898
01:10:51,665 --> 01:10:51,826
Speaker 0: Yeah.

899
01:10:52,167 --> 01:10:56,775
Speaker 1: Anyway, I digress there have been real slippery slopes that have.

900
01:10:56,795 --> 01:11:04,333
Speaker 1: we have gone down in real life, right and We've just read, you know, so many UM play gods sci-fis.

901
01:11:04,675 --> 01:11:22,210
Speaker 1: right that you know, people are so worried about slippery slopes, but if We can there are many situations such as this where we can build the technology And as long as we don't go down the slippery slope We can gain a huge benefit.

902
01:11:22,685 --> 01:11:22,850
Speaker 1: right.

903
01:11:22,930 --> 01:11:26,166
Speaker 1: The negatives are further down the slope at the top of the slope.

904
01:11:26,226 --> 01:11:33,880
Speaker 1: if we hold Just hold and keep the snowball way at the top of the hill and don't go down the hill There's a huge benefits to be gained.

905
01:11:33,940 --> 01:11:58,784
Speaker 1: if we're so afraid of the bottom of the hill That we stay at the very tippy top and don't take the only one step down and stop there after one step We're actually sort of you know That's also a negative in that you can consider the positives that we would get one step down in this case lots and lots of CSAM dissemination just going to go getting off scotch-free not getting caught right.

906
01:11:58,884 --> 01:12:02,535
Speaker 1: all this child abuse just running rampant Right is like.

907
01:12:02,555 --> 01:12:04,842
Speaker 1: that's also a negative right?

908
01:12:05,204 --> 01:12:22,607
Speaker 1: So we need to figure out a way that we can Recognize the true and real dangers of going down the slope more than one step because they are super real right Recognize the damage of not going down the slope at all and find a way to go down the slope.

909
01:12:22,807 --> 01:12:23,670
Speaker 1: one step only.

910
01:12:24,256 --> 01:12:24,600
Speaker 1: fucking.

911
01:12:24,681 --> 01:12:26,936
Speaker 1: stay there Right.

912
01:12:27,117 --> 01:12:32,889
Speaker 1: make our society better right and not arguing against step one.

913
01:12:33,592 --> 01:12:39,270
Speaker 1: Be you know by pointing at step two as soon as someone goes to step two, yeah yell at them all you want.

914
01:12:39,792 --> 01:12:47,570
Speaker 0: Well, there's the step one, but while I agree with you in principle in this case We literally cannot know if step two ever happened because of the way u.s.

915
01:12:47,690 --> 01:12:48,092
Speaker 0: Law works.

916
01:12:48,152 --> 01:12:52,230
Speaker 1: the way to solve this problem is only in that specific case, right?

917
01:12:54,210 --> 01:12:56,241
Speaker 0: Primary concern of almost everyone.

918
01:12:56,282 --> 01:12:57,690
Speaker 0: I see it was credible in this space.

919
01:12:58,092 --> 01:13:01,509
Speaker 1: I can't imagine that becoming a huge a huge thing right?

920
01:13:01,589 --> 01:13:01,730
Speaker 1: has?

921
01:13:01,890 --> 01:13:04,220
Speaker 1: Have people been getting caught?

922
01:13:04,862 --> 01:13:15,232
Speaker 1: if that if that case is always true Right that the government is sending hashes to tell them to insert into the database to catch people doing other things besides legitimate See Sam.

923
01:13:15,714 --> 01:13:21,933
Speaker 1: then how come other people on the other platforms where these systems have been in place for a long time There should?

924
01:13:21,953 --> 01:13:26,850
Speaker 1: we should have been seeing people who have been getting busted for other non see Sam related crime.

925
01:13:27,051 --> 01:13:29,925
Speaker 0: You wouldn't because anything like that that's done under a gag order.

926
01:13:29,945 --> 01:13:32,892
Speaker 0: You're never gonna find out Someone.

927
01:13:33,335 --> 01:13:36,010
Speaker 1: if a human being goes to like is arrested you're right.

928
01:13:36,010 --> 01:13:41,138
Speaker 0: It's like no, you know, you know, but you would not know in the general public They have that sealed evidence.

929
01:13:41,158 --> 01:13:42,928
Speaker 0: in most cases that's never revealed to the public.

930
01:13:43,652 --> 01:13:46,406
Speaker 1: There aren't people being disappeared right or whatever.

931
01:13:46,426 --> 01:13:46,928
Speaker 0: How would you know?

932
01:13:48,010 --> 01:13:50,099
Speaker 1: How have you seen anyone you know getting?

933
01:13:50,119 --> 01:13:51,626
Speaker 0: all I all I know is stats.

934
01:13:51,686 --> 01:14:01,270
Speaker 0: I saw on Leaks attempting to show how many national security letters and gag orders get sent and it's a lot just generally across all technology.

935
01:14:02,232 --> 01:14:05,850
Speaker 0: But I'm saying the solution to this is to change the structure.

936
01:14:06,271 --> 01:14:13,013
Speaker 0: The US government should be much more limited in where and how gag orders are used and we need a lot more oversight over that Whole process.

937
01:14:13,374 --> 01:14:15,303
Speaker 0: that's the actual fundamental problem.

938
01:14:15,343 --> 01:14:16,850
Speaker 0: that's making this issue, right?

939
01:14:16,970 --> 01:14:26,410
Speaker 1: But the fact that we even know the system exists Right that Apple told us about it is a reason to have some measure of good faith in the system.

940
01:14:26,450 --> 01:14:29,350
Speaker 1: They could have just done this and never told anyone a fucking thing.

941
01:14:29,971 --> 01:14:37,310
Speaker 1: They could have implemented this whole system and never told anyone it existed that we would never even know about it.

942
01:14:37,510 --> 01:14:41,321
Speaker 1: You wouldn't know this was happening and you would just feel right as rain.

943
01:14:41,481 --> 01:14:44,510
Speaker 1: the fact that Apple told us that this existed and how?

944
01:14:44,610 --> 01:14:47,752
Speaker 1: It works when they didn't have to Right.

945
01:14:47,772 --> 01:14:56,510
Speaker 1: They didn't have to tell us is a reason to have good faith in the system unless this is like the the sort of You know double think strategy of the government, right?

946
01:14:57,693 --> 01:15:01,474
Speaker 1: Right, that's Right.

947
01:15:01,615 --> 01:15:17,190
Speaker 1: is a reason to believe that yeah It's gonna right until this until they say it's also a reason to believe that if things went to step two That someone would say something, you know, not necessarily Someone officially saying something but some sort of whistleblower.

948
01:15:17,390 --> 01:15:32,170
Speaker 0: I feel much more simple if this figures into your threat profiles enough to where you have a. Specifically you are already the kind of person who needs to be using platform agnostic encryption, right?

949
01:15:32,630 --> 01:15:33,373
Speaker 1: And yeah, I'm not gonna.

950
01:15:33,434 --> 01:15:40,518
Speaker 1: I'm not gonna get mad about this the way other people are getting mad about it until You know something actually happens, right?

951
01:15:41,079 --> 01:15:45,210
Speaker 1: I'm seeing this currently as a net positive of catching lots.

952
01:15:45,230 --> 01:15:48,844
Speaker 0: I mean when I need to communicate in a way that I know is verifiably secure.

953
01:15:48,864 --> 01:15:53,424
Speaker 0: I use Platform agnostic and an encrypted services.

954
01:15:54,088 --> 01:15:55,199
Speaker 1: Hmm I mean it.

955
01:15:55,442 --> 01:16:07,077
Speaker 1: you could just Seriously, if you don't want to set up fancy encryption if you just make a fucking Linux computer and two people SSH to it Yeah, and you'd write you edit a text file.

956
01:16:07,298 --> 01:16:10,770
Speaker 1: It's like the government ain't gonna find it even if the text file is unencrypted.

957
01:16:11,532 --> 01:16:12,254
Speaker 0: Well, so did you?

958
01:16:12,294 --> 01:16:12,896
Speaker 0: there was a thing.

959
01:16:12,916 --> 01:16:19,732
Speaker 0: a bunch of government people who were being investigated for corruption and other things got caught with this a Common way.

960
01:16:19,833 --> 01:16:25,228
Speaker 0: for a long time people would communicate anonymously and avoid government regulation and oversight and even avoid cops.

961
01:16:25,650 --> 01:16:30,506
Speaker 0: is Someone would log into a random email account write an email and not send?

962
01:16:30,567 --> 01:16:31,470
Speaker 0: just leave it as a draft.

963
01:16:31,912 --> 01:16:40,189
Speaker 0: Someone else logs into that same email account looks in the draft read the draft edits it But they never actually send the email anywhere.

964
01:16:40,972 --> 01:16:50,690
Speaker 0: smart there's a lot of well that that it's not staying inography, but that's a whole class of Security through obscurity communication mechanisms that are way out of scope for the show.

965
01:16:50,710 --> 01:16:52,903
Speaker 0: And I think we talked about it in like 2007 on geek nights.

966
01:16:55,791 --> 01:16:57,980
Speaker 0: Anyway, I'm hungry and I think this was a plenty long show.

967
01:16:58,021 --> 01:16:58,703
Speaker 0: you see what happens.

968
01:16:59,085 --> 01:17:00,230
Speaker 0: we skip a couple weeks of shows.

969
01:17:00,534 --> 01:17:01,284
Speaker 0: It's all bottled up.

970
01:17:02,561 --> 01:17:02,729
Speaker 1: Yep.

971
01:17:03,692 --> 01:17:05,509
Speaker 0: So, uh, see you on Wednesday when?

972
01:17:06,671 --> 01:17:07,334
Speaker 0: We do the show on.

973
01:17:08,177 --> 01:17:09,543
Speaker 1: I watched all four even movies.

974
01:17:09,784 --> 01:17:12,255
Speaker 0: Oh, I haven't watched them yet But I watched beat.

975
01:17:12,296 --> 01:17:13,642
Speaker 1: I watched Beast are season two.

976
01:17:13,743 --> 01:17:15,049
Speaker 0: Oh, I finished Beast our season two.

977
01:17:16,513 --> 01:17:16,835
Speaker 0: I was.

978
01:17:17,076 --> 01:17:26,870
Speaker 0: uh, all I will say is I was Surprised by exactly one thing and that thing was very surprising and it involved the number four.

979
01:17:28,571 --> 01:17:29,717
Speaker 0: Yeah, I didn't think that.

980
01:17:29,958 --> 01:17:32,150
Speaker 0: I thought I didn't think that was literally going to happen.

981
01:17:33,611 --> 01:17:38,708
Speaker 0: Okay, and then just literally happened and the show just moved on like yep that happened that that surprised me a little bit.

982
01:17:39,714 --> 01:17:42,290
Speaker 0: Okay, we can always review Beast our season two if we got nothing else.

983
01:17:43,191 --> 01:17:43,915
Speaker 0: Hmm or be.

984
01:17:43,955 --> 01:17:46,248
Speaker 0: I guess there's gonna be a third season coming to like wrap everything up.

985
01:17:47,093 --> 01:17:48,503
Speaker 1: Yeah, Ava movies would be better.

986
01:17:48,523 --> 01:17:53,702
Speaker 1: Beast our season two is Movies by Wednesday cuz I'm only four of them.

987
01:17:53,863 --> 01:17:56,274
Speaker 0: Yeah, you don't even need to watch tomorrow It's 730 p.m.

988
01:17:56,354 --> 01:17:58,706
Speaker 0: On a watch them all in two days.

989
01:18:00,132 --> 01:18:02,779
Speaker 0: Yeah, so, let's see tomorrow one day into the next day.

990
01:18:02,960 --> 01:18:03,903
Speaker 0: I'm working also the first.

991
01:18:04,384 --> 01:18:06,430
Speaker 0: you've already seen the dark literally all night.

992
01:18:07,092 --> 01:18:08,979
Speaker 1: You don't realize you've already seen the first one.

993
01:18:08,999 --> 01:18:12,210
Speaker 1: You could just watch the last one minute of the first one.

994
01:18:12,290 --> 01:18:21,490
Speaker 1: Oh, yeah, I figure oh wait, there's two there's another seat There's a scene in the middle of the first one That's less than like 30 seconds long and the end of the first one like literally the last less than minute.

995
01:18:21,751 --> 01:18:24,620
Speaker 0: Oh shit, like that matters like the Ava does.

996
01:18:24,660 --> 01:18:28,710
Speaker 1: yeah, but you can you can just watch those two scenes and otherwise I'm still not gonna.

997
01:18:28,931 --> 01:18:33,346
Speaker 0: I'm not gonna have watched four ish movies by Wednesday when we got to do a show.

998
01:18:33,386 --> 01:18:39,010
Speaker 0: cuz I have I Remember my entire tomorrow is taken up in the evening playing blades in the dark until like midnight.

999
01:18:39,752 --> 01:18:43,354
Speaker 1: If you just stay there Ava, so if you start watching them Oh, yeah, I know the danger.

1000
01:18:43,415 --> 01:18:44,419
Speaker 1: You might watch him all today.

1001
01:18:44,439 --> 01:18:47,050
Speaker 0: the reason Ava the cowboy bebop DVDs.

1002
01:18:47,251 --> 01:18:49,217
Speaker 0: These things are under lock and key for a reason.

1003
01:18:49,458 --> 01:18:51,926
Speaker 0: You can't see a screen playing certain things.

1004
01:18:51,966 --> 01:18:53,230
Speaker 0: You're gonna be trapped for it.

1005
01:18:54,132 --> 01:18:56,749
Speaker 0: Star Wars cannot be displayed on any screen.

1006
01:18:57,352 --> 01:19:02,750
Speaker 1: The the first movie is ninety nine point ninety nine percent a compilation of the first six episodes of the TV series.

1007
01:19:03,092 --> 01:19:12,907
Speaker 1: But starting with episode the second movie It's almost nothing like in fact that yet while being almost nothing like it It's all basically the same thing.

1008
01:19:12,967 --> 01:19:14,850
Speaker 0: also You know what?

1009
01:19:14,910 --> 01:19:16,114
Speaker 0: The problem is I realized that can't.

1010
01:19:16,214 --> 01:19:20,730
Speaker 0: I can't make that Simpsons TV because it would turn into the day killers and our IT.

1011
01:19:20,952 --> 01:19:21,748
Speaker 0: I'll leave you all with this.

1012
01:19:22,131 --> 01:19:29,798
Speaker 0: We had a bunch of VHS tapes from my family where we would do ad blocking by someone would record shows and then we'd all Watch the shows without the ads later.

1013
01:19:30,461 --> 01:19:32,090
Speaker 0: So that was a chore like taking out the garbage.

1014
01:19:32,593 --> 01:19:41,839
Speaker 0: So I had these VHS tapes that were in like the lowest quality mode So like eight hours of tape and it was just back-to-back Simpsons episodes with no openers closers or commercials.

1015
01:19:42,200 --> 01:19:45,209
Speaker 0: and if anyone put that in the VCR we were just trapped.

1016
01:19:47,778 --> 01:19:48,708
Speaker 0: I can't get that little TV.

1017
01:19:49,231 --> 01:20:05,157
Speaker 0: You'll see me sitting at my decks with two 4k monitors and I'll be staring at a tiny little screen playing like episode 36 of The Simpsons in 320 by 200.

1018
01:20:05,157 --> 01:20:07,700
Speaker 0: This has been geek nights with rim and Scott special.

1019
01:20:07,760 --> 01:20:12,580
Speaker 0: Thanks to DJ pretzel for the opening music cat leave for web design and Brando K for the logos.

1020
01:20:12,801 --> 01:20:17,820
Speaker 1: Be sure to visit our website at front row crew comm for show notes discussion news and more.

1021
01:20:18,100 --> 01:20:25,140
Speaker 0: Remember geek nights is not one but four different shows sci-tech Mondays gaming Tuesdays anime comic Wednesdays and indiscriminate Thursdays.

1022
01:20:25,860 --> 01:20:29,036
Speaker 1: Geek nights is distributed under a Creative Commons attribution 3.0 license.

1023
01:20:30,301 --> 01:20:33,390
Speaker 1: Geek nights is recorded live with no studio and no audience.

1024
01:20:33,571 --> 01:20:35,055
Speaker 1: But unlike those other late shows.

1025
01:20:35,175 --> 01:20:45,488
Speaker 0: It's actually recorded at night and The patreon patrons for this episode of geek nights now that we're back are Heidi McNichol Alan Joyce linky G dread.

1026
01:20:45,528 --> 01:20:49,820
Speaker 0: Lily Tenerbrae Chris Romer Clinton Walton Dex Finn just like a dude guy.

1027
01:20:50,161 --> 01:20:52,736
Speaker 0: Shoshaya high 85 Rebecca Dunn review bad bull 34 cowards.

1028
01:20:53,560 --> 01:20:54,022
Speaker 0: It's coming.

1029
01:20:54,705 --> 01:21:09,420
Speaker 0: Ryan Baron Sam Erickson Sherman one oral Taylor Braun under program law odd-do Pierce ab threat open seat, right You hold the key to my heart and a whole bunch of people who do not want me to say their names.

1030
01:21:09,801 --> 01:21:14,380
Speaker 0: I paused the patreon for the previous month because we missed so many shows, but we're back doing our normal thing.

1031
01:21:14,762 --> 01:21:16,232
Speaker 0: We'll pretty much be on a normal schedule.

1032
01:21:16,614 --> 01:21:28,203
Speaker 0: on the next Monday show We're talking about sleeping for computers and on the next Wednesday show We are very likely going to talk about all of the new Ava movies since they're all out and we've well by then We'll have seen them.

1033
01:21:28,664 --> 01:21:31,574
Speaker 0: But anyway for right now now that we're back.

1034
01:21:32,437 --> 01:21:33,119
Speaker 0: I leave you.

1035
01:21:33,942 --> 01:21:36,127
Speaker 0: Where is locked if we can't get to bro?

1036
01:21:36,147 --> 01:21:40,980
Speaker 0: Wait, I see him and it looks like he's wrestling someone.

1037
01:21:53,001 --> 01:21:56,554
Speaker 0: What's he doing with his favorite bro, what other weapon does he have?

1038
01:21:57,116 --> 01:21:58,120
Speaker 0: he needs his hands free.

1039
01:22:07,686 --> 01:22:11,413
Speaker 0: Oh He's got him on his knees, it's just a matter of time now.

1040
01:22:11,734 --> 01:22:12,958
Speaker 0: process toying with him.

1041
01:22:14,401 --> 01:22:15,852
Speaker 0: Pretzel better rocks.

1042
01:22:15,872 --> 01:22:20,650
Speaker 1: gotta teach me that one Fade of space may have got his helmet knocked off.

1043
01:22:21,414 --> 01:22:22,097
Speaker 1: Oh my god.

1044
01:22:22,117 --> 01:22:22,740
Speaker 1: He's hideous.

1045
01:22:23,701 --> 01:22:26,750
Speaker 1: Must have been four boys on base phrase again.

1046
01:22:26,770 --> 01:22:29,017
Speaker 0: Hey, that's not a phantom spaceman.

1047
01:22:29,037 --> 01:22:36,358
Speaker 0: That's it Oh Right phantom spaceman.

