1
00:00:08,540 --> 00:00:10,028
Speaker 0: It's Monday, November 1st, 2021.

2
00:00:10,028 --> 00:00:10,188
Speaker 0: I'm Rem.

3
00:00:13,703 --> 00:00:14,100
Speaker 1: I'm Scott.

4
00:00:14,360 --> 00:00:15,543
Speaker 0: And this is Geek Nights.

5
00:00:15,583 --> 00:00:21,780
Speaker 0: Tonight, on the 16th anniversary of fucking Geek Nights, we're talking about documentation.

6
00:00:23,582 --> 00:00:24,618
Speaker 1: Let's do this.

7
00:00:25,980 --> 00:00:30,314
Speaker 0: It's all- I'm happy that we started Geek Nights back in 2005 on Halloween.

8
00:00:30,554 --> 00:00:33,449
Speaker 1: Even though- We didn't start it on Halloween.

9
00:00:33,489 --> 00:00:34,820
Speaker 1: We started it before Halloween.

10
00:00:34,900 --> 00:00:41,120
Speaker 1: And then somehow, coincidentally, I don't even feel like we realized that the first real episode was Halloween.

11
00:00:41,400 --> 00:00:43,707
Speaker 1: We just sort of were like, "That's good enough to go

12
00:00:43,807 --> 00:00:44,189
Speaker 1: live.".

13
00:00:44,249 --> 00:00:46,696
Speaker 1: And then later on, it was just like, "When's our anniversary?

14
00:00:46,797 --> 00:00:47,378
Speaker 1: Oh, Halloween,

15
00:00:47,439 --> 00:00:47,720
Speaker 1: okay.".

16
00:00:47,880 --> 00:00:52,046
Speaker 0: So years ago, I actually told the exact story of how we did this.

17
00:00:52,087 --> 00:01:00,640
Speaker 0: Because remember, for the Patreon people, I remastered with better audio all of the beta episodes, re-uploaded them, and I recorded commentary tracks for all of them.

18
00:01:01,521 --> 00:01:03,844
Speaker 0: So I listened to all that early beta Geek Nights.

19
00:01:04,185 --> 00:01:15,120
Speaker 0: The decision point we made was specific- The one good news is, if anyone goes and listens to them on the Patreon now, while the content is unlistenable, the audio is at least listenable.

20
00:01:15,621 --> 00:01:23,579
Speaker 0: I learned a lot since back then, and I also was able to salvage some of that Logitech audio in ways that I even surprised myself.

21
00:01:23,679 --> 00:01:24,080
Speaker 0: Good lord.

22
00:01:25,842 --> 00:01:30,112
Speaker 0: So, way back, we actually published our first episode.

23
00:01:30,173 --> 00:01:33,240
Speaker 0: It was the following episode, like a November 1st or 2nd episode.

24
00:01:33,960 --> 00:01:39,655
Speaker 0: We then decided, "Hey, the episode we did one before this, actually, we think it was pretty good.

25
00:01:40,056 --> 00:01:41,219
Speaker 0: Let's also make that one

26
00:01:41,279 --> 00:01:41,560
Speaker 0: public.".

27
00:01:41,881 --> 00:01:46,098
Speaker 0: And we went back and published the previously unpublished October 31st episode.

28
00:01:47,803 --> 00:01:50,129
Speaker 1: I mean, I do recommend that pattern, right?

29
00:01:50,189 --> 00:01:54,660
Speaker 1: If you're going to make a continuing content of some kind, right?

30
00:01:55,160 --> 00:01:58,167
Speaker 1: I guess maybe that's not right.

31
00:01:58,208 --> 00:02:03,140
Speaker 1: Because let's say you're writing a series, like a book, and you're doing a chapter by chapter.

32
00:02:03,701 --> 00:02:09,000
Speaker 1: You can't write some beta chapters and make chapter three actually chapter one and throw out chapter one and two.

33
00:02:09,161 --> 00:02:10,896
Speaker 0: Maybe you could, but...

34
00:02:12,161 --> 00:02:16,239
Speaker 1: Yeah, but like a podcast, they're all standalone, so yeah, you can throw some out.

35
00:02:17,020 --> 00:02:27,079
Speaker 1: But it's like, I guess the art process is generally going to be like sketches, whether it's literal sketches or just practice drawings, practice whatever it is you do.

36
00:02:28,521 --> 00:02:33,112
Speaker 1: Make short movies, songs, practice meh ones.

37
00:02:33,854 --> 00:02:36,560
Speaker 1: You start out by not sharing them with anyone because they're too awful.

38
00:02:37,282 --> 00:02:43,240
Speaker 1: And then people seem to jump from not sharing with anyone to share with the world, and I guess you can do that.

39
00:02:43,480 --> 00:02:53,880
Speaker 1: But I guess the path we took of share with no one, share with close people only to make sure if they give you the thumbs up, then go to the world.

40
00:02:54,100 --> 00:03:02,100
Speaker 1: It's like you can hide some more of your trash from the world and also have some beta episodes hidden away for fun times in the future.

41
00:03:02,401 --> 00:03:09,138
Speaker 0: But also, this quote gets misused more and more lately, but the idea of making the machine.

42
00:03:09,178 --> 00:03:10,020
Speaker 0: that makes the machine.

43
00:03:10,461 --> 00:03:14,200
Speaker 0: Those episodes, the purpose wasn't to make content for us and every process is different.

44
00:03:15,202 --> 00:03:19,100
Speaker 0: Our goal there was specifically to figure out how to make a podcast at all.

45
00:03:19,500 --> 00:03:20,865
Speaker 0: We were learning a process.

46
00:03:21,407 --> 00:03:24,900
Speaker 0: The episodes that got recorded were just artifacts of that process.

47
00:03:25,781 --> 00:03:27,027
Speaker 1: Yep.

48
00:03:27,087 --> 00:03:29,860
Speaker 1: You might be wondering why I'm wearing sunglasses inside of a podcast.

49
00:03:30,161 --> 00:03:33,292
Speaker 0: I mean, you might be wondering why I have a mustache like.

50
00:03:33,332 --> 00:03:35,600
Speaker 0: it's the year 1997 and I'm back in high school.

51
00:03:36,521 --> 00:03:42,580
Speaker 1: Yeah, but it's that time of year where the earth tilts upwards in the wrong direction, so up away from the sun.

52
00:03:45,221 --> 00:03:57,420
Speaker 1: So the sun now, because I have a south facing window right here to my right, the sun late in the day, like currently five, but I guess starting next week, it'll be around like four.

53
00:03:58,801 --> 00:04:05,380
Speaker 1: There's a period right before sunset where the sun just blasts right straight through these giant windows, right into my face, my monitors.

54
00:04:06,121 --> 00:04:11,416
Speaker 1: And it caught me today at like an even worse angle than last Friday and blasted me right in the fucking eyes.

55
00:04:11,436 --> 00:04:12,620
Speaker 1: So my eyes hurt.

56
00:04:13,381 --> 00:04:22,380
Speaker 0: It's even funnier because around that time is when, because the buildings around the garden behind our building are set up, it's pitch black out the window right now.

57
00:04:22,620 --> 00:04:22,760
Speaker 1: Oh yeah.

58
00:04:22,780 --> 00:04:23,402
Speaker 1: It's dark now.

59
00:04:23,462 --> 00:04:23,602
Speaker 1: Yeah.

60
00:04:23,722 --> 00:04:27,248
Speaker 0: But, uh, but even though the lights on, yeah, it gets dark before then.

61
00:04:27,268 --> 00:04:28,009
Speaker 0: So yeah.

62
00:04:28,069 --> 00:04:34,520
Speaker 0: Anyway, uh, so my mustache is because I did something that I've threatened to do for years, but I never did it.

63
00:04:34,600 --> 00:04:37,125
Speaker 0: You know, there's a history in RIT anime of I was poor.

64
00:04:37,145 --> 00:04:41,813
Speaker 0: I was also lazy and I had very little free time despite being so lazy.

65
00:04:42,234 --> 00:04:45,500
Speaker 1: So I tend to, well, you also, I think you, you didn't care that much.

66
00:04:45,640 --> 00:04:49,585
Speaker 1: I also did not care, but, but you were, but you were up for it.

67
00:04:49,665 --> 00:05:02,060
Speaker 0: You were like, you know, there's, there's this weird dichotomy of, or not dichotomy, the duality of not caring about something enough to do a good job, but caring enough to do the thing at all.

68
00:05:03,100 --> 00:05:16,620
Speaker 1: So like, yes, I want to do that thing as, but not if it requires any work or effort or investment, but if I could just do it, it's like, yeah, it's like, would I love to drive a formula one car if I didn't have to practice driving to not die?

69
00:05:16,780 --> 00:05:17,041
Speaker 1: Yeah.

70
00:05:17,161 --> 00:05:17,461
Speaker 1: Yeah.

71
00:05:18,002 --> 00:05:19,525
Speaker 1: I remember you had to drive one.

72
00:05:19,865 --> 00:05:20,346
Speaker 1: I'll die.

73
00:05:20,386 --> 00:05:21,207
Speaker 1: Therefore I don't.

74
00:05:21,388 --> 00:05:21,608
Speaker 1: Wasn't?

75
00:05:21,628 --> 00:05:26,877
Speaker 0: there's a robotics club at RIT and you asked them, Hey, can I join, but just be the driver of your little robot car?

76
00:05:26,917 --> 00:05:27,758
Speaker 0: And they were like, fuck you.

77
00:05:27,818 --> 00:05:28,860
Speaker 0: No, of course not.

78
00:05:30,381 --> 00:05:34,067
Speaker 1: Well, no, I think that was literally just a race car, the formula, SAE, whatever.

79
00:05:34,087 --> 00:05:34,548
Speaker 1: Yeah.

80
00:05:34,568 --> 00:05:39,416
Speaker 1: I don't remember what that, exactly the details there, but yeah, but they weren't looking for a driver only.

81
00:05:39,536 --> 00:05:39,717
Speaker 1: Right.

82
00:05:39,737 --> 00:05:41,680
Speaker 0: They're looking for engineers to work on a car.

83
00:05:42,100 --> 00:05:56,300
Speaker 0: So I, I've threatened for years while a listener even photoshopped me at one point, but, uh, the costume I wore is basically my own body because at least in a photo, my own body looks very close to Sean Connery in the 1974.

84
00:05:56,300 --> 00:05:58,622
Speaker 0: Well, movie Zardoz.

85
00:05:59,303 --> 00:05:59,483
Speaker 1: Right.

86
00:05:59,503 --> 00:06:05,087
Speaker 1: So long ago, someone took the famous Zardoz Sean Connery photo and photoshopped rims head onto it.

87
00:06:05,107 --> 00:06:06,468
Speaker 1: I don't know who originally did that.

88
00:06:07,669 --> 00:06:10,491
Speaker 1: Uh, it was too, it was too uncanny.

89
00:06:11,252 --> 00:06:11,412
Speaker 1: Yeah.

90
00:06:11,432 --> 00:06:16,256
Speaker 1: Whoever did it, uh, it must, it must've been some listener or someone we know.

91
00:06:16,296 --> 00:06:16,956
Speaker 1: I don't know who it is.

92
00:06:17,016 --> 00:06:18,217
Speaker 1: Claim credit, whoever you are.

93
00:06:19,018 --> 00:06:21,400
Speaker 1: Um, I could probably find it if I put in a lot of effort.

94
00:06:22,080 --> 00:06:28,886
Speaker 1: Whoever it was, uh, you know, posted that Photoshop with, uh, you know, Zed from Zardoz, Sean Connery with rims head on it.

95
00:06:29,466 --> 00:06:37,913
Speaker 1: And it's like, Oh, you didn't even realize it was not a great job of photoshopping, but it was so, uh, so it's such a match that it didn't need to be a good job.

96
00:06:37,973 --> 00:06:46,160
Speaker 1: It's like, Oh, I didn't even realize that that wasn't, you know, um, so I basically made it real thanks to that person.

97
00:06:46,440 --> 00:06:46,680
Speaker 0: Yup.

98
00:06:46,741 --> 00:06:50,765
Speaker 0: And Emily actually printed out a papercraft Zardoz head and, uh, it was a whole thing.

99
00:06:51,225 --> 00:06:55,930
Speaker 0: So, uh, our friends had to see this and now you have to see this too.

100
00:06:56,390 --> 00:06:59,373
Speaker 0: It looks exactly like everyone predicted and expected.

101
00:06:59,934 --> 00:07:05,099
Speaker 1: And, uh, the only thing is I think you got the, you know, get the red suspenders instead of the, the bandolier aspect.

102
00:07:07,161 --> 00:07:11,666
Speaker 0: I constructed this costume basically by getting three things.

103
00:07:11,706 --> 00:07:17,812
Speaker 1: that will four things on Amazon, but that's more effort than I think any costume you've ever worn.

104
00:07:17,872 --> 00:07:20,194
Speaker 0: The boots were $29.

105
00:07:20,194 --> 00:07:21,896
Speaker 0: That costs more than I said.

106
00:07:21,916 --> 00:07:26,200
Speaker 0: I mean, every costume I wore at RIT over the course of five years combined.

107
00:07:27,341 --> 00:07:31,636
Speaker 1: I mean, that's not like cheap, but for a boot, that is the cheap, a cheap boot.

108
00:07:31,656 --> 00:07:32,840
Speaker 1: That is such a cheap boot.

109
00:07:33,460 --> 00:07:43,008
Speaker 0: Uh, I had, I already had a red Speedo and I bought a, I bought Japanese traditional underwear to use as the loincloth and I bought red suspenders and a cheap metal gun.

110
00:07:43,728 --> 00:07:44,369
Speaker 0: So there you go.

111
00:07:45,029 --> 00:07:56,277
Speaker 0: Uh, if I want to approve this, I could, I'd have to make the bandolier or buy it from a place that's not like shadow gray market Amazon because there are no red bandoliers on Amazon.

112
00:07:56,318 --> 00:07:57,558
Speaker 0: I'd have to get a real bandolier.

113
00:07:57,598 --> 00:07:59,580
Speaker 0: that probably costs actual dollars.

114
00:08:00,160 --> 00:08:07,986
Speaker 0: Yeah, I think you could, you could probably for something like that, it's like, yeah, you could be, I'll get two red belts and just put like loops on it and put some like shells and it would be fine.

115
00:08:08,006 --> 00:08:09,247
Speaker 1: You could do something like that.

116
00:08:09,347 --> 00:08:10,288
Speaker 1: That wouldn't be too bad.

117
00:08:10,328 --> 00:08:15,412
Speaker 1: But I think if you wanted it to be like better find another person who's a cosplayer, we might know some.

118
00:08:15,692 --> 00:08:16,533
Speaker 0: Oh yeah.

119
00:08:16,553 --> 00:08:19,255
Speaker 1: Hey, I'll pay you to make a red bandoliers, right?

120
00:08:19,275 --> 00:08:19,855
Speaker 1: Just two of them.

121
00:08:20,016 --> 00:08:20,276
Speaker 1: Yep.

122
00:08:21,096 --> 00:08:25,760
Speaker 0: I could probably find a red bandolier if I looked anywhere other than the bottom of the barrel on Amazon.

123
00:08:26,340 --> 00:08:35,423
Speaker 1: There might also be, you know, you might be able to get a white one and just dunk it in some red dye from the craft store, but they're going to be brown.

124
00:08:35,443 --> 00:08:36,323
Speaker 1: They're going to be brown though.

125
00:08:36,403 --> 00:08:39,823
Speaker 1: Usually they're all, they're all black or camouflage.

126
00:08:39,924 --> 00:08:40,865
Speaker 0: The ones that I found.

127
00:08:41,445 --> 00:08:43,626
Speaker 1: Well, a leather one's going to be brown, like an old school one.

128
00:08:45,066 --> 00:08:48,247
Speaker 0: So yeah, we had a little Halloween gathering and that was, that was what came out of that.

129
00:08:49,408 --> 00:08:51,388
Speaker 1: All right, let's move on to some news action.

130
00:08:51,568 --> 00:08:55,409
Speaker 0: We'll see how long I keep this mustache cause I look exactly like how I looked in high school.

131
00:08:55,489 --> 00:08:57,070
Speaker 0: It's, it's actually kind of creepy.

132
00:08:57,450 --> 00:09:00,573
Speaker 0: I look identical to like my senior photos from high school.

133
00:09:00,593 --> 00:09:02,655
Speaker 1: All right, so news.

134
00:09:02,715 --> 00:09:08,539
Speaker 1: So the biggest tech news sort of, uh, you know, this is one of those items where like, is it really a tech news, right?

135
00:09:08,579 --> 00:09:10,361
Speaker 1: It's a business news about a tech company.

136
00:09:10,881 --> 00:09:15,005
Speaker 1: But yeah, everybody knows Facebook is pulling a Google, changing the name.

137
00:09:15,345 --> 00:09:17,667
Speaker 1: So Facebook is the social network now.

138
00:09:17,687 --> 00:09:21,370
Speaker 1: The company is called meta or something like that.

139
00:09:21,711 --> 00:09:22,999
Speaker 0: There's a lot going on here.

140
00:09:23,019 --> 00:09:24,550
Speaker 0: We even talked about this a little bit on Thursday.

141
00:09:24,910 --> 00:09:30,694
Speaker 1: Oculus brand is gone and they've got, you know, they're going to use the meta brand over for all that other stuff.

142
00:09:31,295 --> 00:09:40,582
Speaker 1: And basically, you know, I think this plays into what a lot of people have been saying about Facebook, which is like, yeah, you see them doing all these desperate things, right?

143
00:09:40,622 --> 00:09:52,771
Speaker 1: Trying to get children to use Facebook, you know, fighting, you know, allow, you know, allow it, you know, being so permissive of, uh, uh, bad people, you know, posting bad things on Facebook just to keep the user counts up.

144
00:09:52,911 --> 00:09:53,131
Speaker 1: Right.

145
00:09:53,171 --> 00:09:59,636
Speaker 1: It's like, it seems like the evidence suggests, like Facebook itself is not the social network is not doing well.

146
00:10:00,157 --> 00:10:07,963
Speaker 1: They sort of, you know, even though it's so large, it will take a long time for it to like become small and dead, like MySpace.

147
00:10:08,624 --> 00:10:14,048
Speaker 1: But, you know, which wasn't, I guess, if not, if MySpace had been the size of Facebook, it'd probably still be around.

148
00:10:14,188 --> 00:10:15,349
Speaker 1: But it seems to be.

149
00:10:15,389 --> 00:10:16,470
Speaker 1: Facebook is in decline.

150
00:10:16,750 --> 00:10:16,990
Speaker 1: Yeah.

151
00:10:17,030 --> 00:10:34,404
Speaker 0: And they recognize, I've seen multiple, there's been multiple stories just recently, like the last couple of days of insiders leaking and revealing that even internally, basically everyone in Facebook agrees that the brand, the word Facebook is just pure poison to the outside world.

152
00:10:34,424 --> 00:10:41,790
Speaker 0: Like it is seen like it has shockingly, well, not shockingly, expectedly negative connotations to almost everyone.

153
00:10:42,570 --> 00:10:42,770
Speaker 1: Yep.

154
00:10:43,371 --> 00:10:44,672
Speaker 1: So yeah, Facebook is on its own.

155
00:10:44,952 --> 00:10:48,795
Speaker 1: It's, you know, it's still huge, but it's down, it's trending downwards.

156
00:10:48,855 --> 00:10:49,095
Speaker 1: Right.

157
00:10:49,155 --> 00:10:57,882
Speaker 1: So they got this other sideshow and I guess you can see the synergy there, which is like, okay, we made this sort of social network of the beginning of the information age.

158
00:10:58,342 --> 00:10:58,502
Speaker 1: Right.

159
00:10:58,522 --> 00:11:05,147
Speaker 1: We were the champs in terms of looking at screens and typing text, you know, let's take to survive in the future.

160
00:11:05,348 --> 00:11:05,588
Speaker 1: Right.

161
00:11:05,628 --> 00:11:08,430
Speaker 1: We got, we got to put out multiple game plans.

162
00:11:08,910 --> 00:11:09,150
Speaker 1: Right.

163
00:11:09,311 --> 00:11:16,736
Speaker 1: If we can't save Facebook, that our game plan will be, you know, even if it's not for quite a while, cause they got a long runway.

164
00:11:16,836 --> 00:11:19,158
Speaker 1: Cause how long will it take for Facebook to die die?

165
00:11:19,398 --> 00:11:19,638
Speaker 1: Right.

166
00:11:19,738 --> 00:11:20,239
Speaker 1: A long time.

167
00:11:20,559 --> 00:11:21,460
Speaker 1: Even if they do nothing.

168
00:11:21,560 --> 00:11:23,521
Speaker 1: So they got this long runway.

169
00:11:24,001 --> 00:11:32,508
Speaker 1: that even though I still say that, you know, the current VR tech is not going to achieve any sort of mass thing.

170
00:11:32,568 --> 00:11:35,230
Speaker 1: for, you know, maybe we'll be seniors by the time.

171
00:11:35,250 --> 00:11:35,610
Speaker 1: Right.

172
00:11:35,890 --> 00:11:37,352
Speaker 0: It's a lot sooner than that.

173
00:11:37,552 --> 00:11:40,795
Speaker 0: You're weirdly pessimistic about this hardware that you've literally never used.

174
00:11:41,696 --> 00:11:46,360
Speaker 1: Anyway, the point is you're not going to see that thing in people's homes being used frequently.

175
00:11:46,740 --> 00:11:47,741
Speaker 1: Right.

176
00:11:48,061 --> 00:11:48,682
Speaker 1: Anytime soon.

177
00:11:48,802 --> 00:11:51,524
Speaker 1: Anyway, the point is that is a future.

178
00:11:51,845 --> 00:11:52,045
Speaker 1: Right.

179
00:11:52,085 --> 00:11:57,370
Speaker 1: It's like, ah, the future social network will be the one we imagine in cyberpunk world in Snow Crash.

180
00:11:57,410 --> 00:11:57,550
Speaker 1: Right.

181
00:11:57,570 --> 00:12:00,492
Speaker 0: But that already gets away from what a social network has always been.

182
00:12:00,552 --> 00:12:14,904
Speaker 0: Because if a social network is about sharing information, then fire and forget content or fire and forget respond to something like that kind of interaction is the primary way people can share and disseminate information.

183
00:12:15,184 --> 00:12:18,286
Speaker 0: You actually descale if it shrinks back to.

184
00:12:18,787 --> 00:12:22,790
Speaker 0: I'm having a real physical conversation with someone in real time.

185
00:12:23,611 --> 00:12:35,919
Speaker 1: Well, I think the way it lines up is if you think of a social regardless of how you interact in the space, which is obviously drastically different, it's still a space that is not a real world space that people come together in.

186
00:12:36,119 --> 00:12:36,379
Speaker 1: Right.

187
00:12:36,499 --> 00:12:40,782
Speaker 1: But I guess the difference is a real Facebook page is a place people come together in.

188
00:12:40,902 --> 00:12:45,225
Speaker 1: a, you know, a chat room is a place people come from all around the world come together.

189
00:12:45,305 --> 00:12:51,770
Speaker 1: So a virtual roar, you know, room like VR chat is a virtual place that where people come together in.

190
00:12:52,110 --> 00:12:52,270
Speaker 0: Yep.

191
00:12:52,350 --> 00:12:58,577
Speaker 0: I guess this is where we need to differentiate, because social media is almost too broad of a term now, what these spaces are.

192
00:12:58,617 --> 00:13:12,010
Speaker 0: There are spaces where strangers come together and interact via video or via small chat or via throwing 100 page Tumblr posts at each other or via VR or via inside of a video game.

193
00:13:12,110 --> 00:13:23,738
Speaker 0: All these things have very different affordances, very different user experiences, very different externalities that are intrinsic or possibly not necessarily intrinsic, but are made intrinsic by capitalism.

194
00:13:24,178 --> 00:13:31,383
Speaker 0: It's there's so much going on here that it's very almost naive to talk about all of this as just social media at this point.

195
00:13:31,404 --> 00:13:31,904
Speaker 0: Right.

196
00:13:32,144 --> 00:13:38,649
Speaker 1: So the short term news is that a lot of the oculus stuff is no longer going to need a Facebook account.

197
00:13:38,709 --> 00:13:40,490
Speaker 1: I don't know if that's all of it, but at least some of it.

198
00:13:40,770 --> 00:13:40,950
Speaker 1: Yep.

199
00:13:41,731 --> 00:13:43,952
Speaker 1: I know the quest they said is which people.

200
00:13:44,353 --> 00:13:54,140
Speaker 0: people resisted what I posited way back when Facebook said they were getting rid of the oculus brand and requiring everyone who uses oculus, including dev kits to have a Facebook account.

201
00:13:54,580 --> 00:13:57,142
Speaker 0: I remember saying the gaming industry won't stand for this.

202
00:13:57,342 --> 00:14:00,724
Speaker 0: And so far it looks like the gaming industry didn't stand for this.

203
00:14:00,744 --> 00:14:03,246
Speaker 0: People truly did not make Facebook accounts.

204
00:14:03,546 --> 00:14:05,348
Speaker 0: They gave up on the platform instead.

205
00:14:06,268 --> 00:14:08,390
Speaker 1: So they gave up on that, which I guess is a positive.

206
00:14:09,611 --> 00:14:13,717
Speaker 1: The negative, I guess, is that it's still the same fucking Zuck dude running this.

207
00:14:13,737 --> 00:14:15,540
Speaker 0: The problem is not that Facebook exists.

208
00:14:15,560 --> 00:14:22,130
Speaker 0: The problem is that Facebook as an institution makes reliably harmful decisions in its space.

209
00:14:23,150 --> 00:14:42,590
Speaker 1: And as we've seen from all the recent, you know, revealings due to court drama and all these documents that got released about, you know, mostly Facebook, but also others is that, yeah, you know, it's not that they Zuck doesn't seem to have a malicious evil motive only other than just the capitalist motive.

210
00:14:42,710 --> 00:14:42,990
Speaker 1: Right.

211
00:14:43,111 --> 00:14:46,615
Speaker 1: All their decisions seem to the thing that they all have in common.

212
00:14:46,675 --> 00:14:54,304
Speaker 1: It's not like they're always deciding in favor of white supremacy or always deciding in favor of, you know, a good point.

213
00:14:54,584 --> 00:14:55,846
Speaker 1: But here's always just.

214
00:14:55,946 --> 00:14:59,370
Speaker 1: they're always deciding in favor of profits for Facebook.

215
00:14:59,470 --> 00:15:00,051
Speaker 1: That is us.

216
00:15:00,171 --> 00:15:01,974
Speaker 0: Every the externalities.

217
00:15:02,194 --> 00:15:03,116
Speaker 0: How do they get profits?

218
00:15:03,376 --> 00:15:04,238
Speaker 0: via engagement?

219
00:15:04,538 --> 00:15:07,883
Speaker 0: What drives engagement, anger and conspiracy theories?

220
00:15:08,184 --> 00:15:11,990
Speaker 0: Therefore, more anger, more conspiracy theories, more profits.

221
00:15:12,251 --> 00:15:19,630
Speaker 1: But there are instances where they decided against anger and conspiracy theories where it either didn't harm or helped the bottom line.

222
00:15:20,050 --> 00:15:20,330
Speaker 0: Yeah.

223
00:15:20,791 --> 00:15:24,857
Speaker 0: But if it harms that bottom line, they reliably always make right.

224
00:15:25,077 --> 00:15:28,341
Speaker 1: There's a decision between money and safety for the users.

225
00:15:28,381 --> 00:15:30,764
Speaker 1: They choose the money option and don't lose.

226
00:15:31,325 --> 00:15:34,870
Speaker 1: all the stories and anecdotes and all the documents are following that pattern.

227
00:15:35,030 --> 00:15:36,332
Speaker 0: And don't lose and all this shuffle.

228
00:15:36,412 --> 00:15:44,964
Speaker 0: A very important point that all the leaks from within Facebook are showing that there was at no point ignorance involved in this.

229
00:15:45,224 --> 00:15:49,670
Speaker 0: People inside of Facebook at all levels were constantly raising the alarm.

230
00:15:50,250 --> 00:15:51,066
Speaker 0: Like everyone there was smart.

231
00:15:52,350 --> 00:15:52,430
Speaker 0: Yeah.

232
00:15:52,450 --> 00:15:53,273
Speaker 0: They knew what they were doing.

233
00:15:53,293 --> 00:15:55,659
Speaker 0: They're like, look, this is causing white supremacy.

234
00:15:55,719 --> 00:15:57,123
Speaker 0: We need to do something about it.

235
00:15:57,624 --> 00:15:59,650
Speaker 0: And they were just categorically ignored.

236
00:16:00,371 --> 00:16:01,572
Speaker 1: Yep.

237
00:16:01,672 --> 00:16:13,286
Speaker 1: So I guess just the last thing is simply what everyone else is saying is true is like you what you keep trying to recreate this cyberpunk stuff, you know, VR land from all these books.

238
00:16:13,446 --> 00:16:16,330
Speaker 1: But didn't you miss the point that that was a dystopia, right?

239
00:16:16,770 --> 00:16:16,970
Speaker 1: Yeah.

240
00:16:17,271 --> 00:16:18,453
Speaker 1: In that world, not a good thing.

241
00:16:18,754 --> 00:16:19,475
Speaker 0: People went in.

242
00:16:19,516 --> 00:16:20,638
Speaker 1: Why are you trying to make this?

243
00:16:21,179 --> 00:16:24,165
Speaker 1: It's like reading Dante's Inferno and like using it as your playbook.

244
00:16:24,205 --> 00:16:26,189
Speaker 1: It's like, no, that's the wrong one.

245
00:16:29,011 --> 00:16:29,231
Speaker 0: Yep.

246
00:16:29,271 --> 00:16:29,752
Speaker 0: Shadowrun.

247
00:16:29,772 --> 00:16:37,905
Speaker 0: We need big mega corpse and like NFTs and cash cards and like, no, those are all like you read Jurassic Park and you're like, oh, great idea.

248
00:16:37,925 --> 00:16:41,170
Speaker 1: I mean, let me try to make some T-Rexes and velociraptors.

249
00:16:42,533 --> 00:16:43,489
Speaker 0: What book did you read?

250
00:16:44,050 --> 00:16:50,402
Speaker 0: Are the nerds whose reaction to Jurassic Park was, ah, that park would have been great if Nedry hadn't fucked it all up.

251
00:16:50,482 --> 00:16:54,910
Speaker 0: Like how to imagine missing the point of that movie so badly.

252
00:16:55,091 --> 00:16:58,130
Speaker 1: But Zuck is Nedry, the in charge guy.

253
00:17:02,191 --> 00:17:03,313
Speaker 0: So what do you think is going to happen?

254
00:17:03,613 --> 00:17:06,998
Speaker 0: I think like my guess, my pure guess is the one thing that's going to happen.

255
00:17:07,638 --> 00:17:15,550
Speaker 0: Facebook, the platform, like the social media platform is going to see fewer and fewer updates, less engagement and just decline into nothing over time.

256
00:17:17,191 --> 00:17:18,111
Speaker 1: I think a lot.

257
00:17:18,852 --> 00:17:26,540
Speaker 1: I can't predict what's going to happen, but I can say that it depends on what action governments take.

258
00:17:26,579 --> 00:17:27,641
Speaker 1: But I can't predict that.

259
00:17:27,800 --> 00:17:28,422
Speaker 1: So I don't.

260
00:17:28,502 --> 00:17:28,742
Speaker 1: Right.

261
00:17:29,003 --> 00:17:37,892
Speaker 1: Because if if there are antitrust actions that actually come into play, which is right now, I'd say 50/50.

262
00:17:37,892 --> 00:17:39,850
Speaker 0: Vote for Democrats if you want more of that, guys.

263
00:17:40,730 --> 00:17:41,832
Speaker 1: Yeah, which I do.

264
00:17:41,992 --> 00:17:42,152
Speaker 1: Right.

265
00:17:42,192 --> 00:17:43,835
Speaker 1: So let's say let's say I get my way.

266
00:17:44,095 --> 00:17:44,296
Speaker 1: Right.

267
00:17:44,336 --> 00:17:53,569
Speaker 1: So if the best possible thing happens, the government breaks up Facebook and all the other companies, all the big tech companies into their component products.

268
00:17:54,050 --> 00:17:54,190
Speaker 1: Right.

269
00:17:54,250 --> 00:17:58,416
Speaker 1: So Amazon, the store is now separate from Oz, the computers.

270
00:17:58,696 --> 00:17:59,017
Speaker 1: Right.

271
00:17:59,417 --> 00:18:00,859
Speaker 1: It's a big AT&T breakup.

272
00:18:01,120 --> 00:18:01,921
Speaker 1: And right.

273
00:18:01,941 --> 00:18:03,603
Speaker 1: Because we're saying this is Scott's favorite.

274
00:18:03,684 --> 00:18:04,505
Speaker 1: Right.

275
00:18:04,585 --> 00:18:07,749
Speaker 1: We don't make the mistake of letting them join together again, like we did with AT&T.

276
00:18:08,310 --> 00:18:08,751
Speaker 1: Whatever.

277
00:18:08,951 --> 00:18:09,512
Speaker 0: Oh, my God.

278
00:18:09,532 --> 00:18:20,550
Speaker 0: Scott, I know you don't like Star Trek, but so the new Lower Decks show, they address this directly in that in old Star Trek, there was an episode where there's a planet that was destroying itself and everyone was stupid.

279
00:18:21,070 --> 00:18:22,213
Speaker 0: And they worshiped a god.

280
00:18:22,233 --> 00:18:22,715
Speaker 1: Sounds like Earth.

281
00:18:22,835 --> 00:18:23,096
Speaker 1: OK.

282
00:18:23,176 --> 00:18:26,345
Speaker 0: And that God was actually just a leftover computer A.I.

283
00:18:26,405 --> 00:18:28,330
Speaker 0: that was telling them to do bad things for its own purposes.

284
00:18:28,851 --> 00:18:38,610
Speaker 0: So in Lower Decks, there's an episode where the plot is Starfleet, like 100 years later, goes back to this planet because they fucking started worshiping that computer again.

285
00:18:42,051 --> 00:18:43,675
Speaker 0: That is literally the plot of the episode.

286
00:18:43,715 --> 00:18:46,222
Speaker 0: Like, guys, what did we fucking tell you?

287
00:18:46,342 --> 00:18:46,703
Speaker 0: It's it's.

288
00:18:46,903 --> 00:18:49,310
Speaker 0: it's actually really funny how it plays out.

289
00:18:50,851 --> 00:18:53,776
Speaker 1: I'm just imagining them worshiping the computer in the Twilight Zone again.

290
00:18:54,137 --> 00:18:55,239
Speaker 0: That's the same one.

291
00:18:55,279 --> 00:18:56,602
Speaker 0: Like, don't eat the food.

292
00:18:56,662 --> 00:18:57,604
Speaker 1: It's radioactive.

293
00:18:57,644 --> 00:18:58,406
Speaker 0: Fuck you, Dad.

294
00:18:58,426 --> 00:18:59,267
Speaker 0: I do what I want.

295
00:18:59,328 --> 00:18:59,628
Speaker 0: Exactly.

296
00:19:01,990 --> 00:19:06,619
Speaker 0: I hate I hate how relevant Twilight Zone episodes are at every stage of my life.

297
00:19:07,200 --> 00:19:07,340
Speaker 1: Right.

298
00:19:07,420 --> 00:19:11,208
Speaker 1: So if that all if that all happens, I could actually see it.

299
00:19:13,211 --> 00:19:15,294
Speaker 1: I guess it then depends who runs it.

300
00:19:15,414 --> 00:19:16,516
Speaker 0: Two things have to happen.

301
00:19:16,756 --> 00:19:21,103
Speaker 0: The breakup has to be such that no one of these companies is too powerful to.

302
00:19:21,744 --> 00:19:26,050
Speaker 0: Zuckerberg himself cannot be in charge of any of these companies.

303
00:19:26,731 --> 00:19:28,434
Speaker 1: Well, or more than one, at least.

304
00:19:28,535 --> 00:19:28,875
Speaker 1: Yeah.

305
00:19:29,136 --> 00:19:35,530
Speaker 0: But it probably go for the VR one, knowing the problem is the VR one is the one that I think has the most long term real potential.

306
00:19:35,991 --> 00:19:39,956
Speaker 0: But it's also I think that I think will work.

307
00:19:40,197 --> 00:19:43,802
Speaker 1: So the two things that they you know, there's a lot of things they acquired.

308
00:19:43,902 --> 00:19:49,450
Speaker 1: But Facebook, Instagram and WhatsApp are both right now really meh.

309
00:19:49,790 --> 00:19:49,991
Speaker 1: Right.

310
00:19:50,031 --> 00:19:51,594
Speaker 1: Instagram got turned into Sky Mall.

311
00:19:52,155 --> 00:19:53,919
Speaker 1: And WhatsApp is who knows.

312
00:19:53,939 --> 00:19:57,305
Speaker 0: WhatsApp is how I communicate with old people.

313
00:19:57,406 --> 00:19:59,350
Speaker 0: I know I don't have it.

314
00:19:59,911 --> 00:20:01,433
Speaker 0: I have because of some people I know.

315
00:20:01,794 --> 00:20:10,970
Speaker 1: While both of those are really meh right now, I actually believe that if though like if you made me in charge of Instagram or WhatsApp right now, they could be saved.

316
00:20:11,350 --> 00:20:15,619
Speaker 1: Those are things that are not beyond saving Facebook.

317
00:20:15,999 --> 00:20:21,070
Speaker 1: The only reason I think Facebook itself, Facebook Marketplace could be its own thing that could be saved.

318
00:20:21,671 --> 00:20:24,537
Speaker 1: It's starting to have some scam problems and falling a bit.

319
00:20:25,679 --> 00:20:30,610
Speaker 1: But Facebook, the social network, you know, events, groups, that sort of thing.

320
00:20:31,530 --> 00:20:42,090
Speaker 1: I think the old any measure that would save it would actually kill it because because capital is just going to you're just going to scare people away and they'll go to some other thing.

321
00:20:42,432 --> 00:20:45,509
Speaker 1: And there's no one who would use even a saved.

322
00:20:46,130 --> 00:20:47,752
Speaker 0: Well, here's a very important point.

323
00:20:47,772 --> 00:20:49,334
Speaker 0: Look what's happening on all social media.

324
00:20:49,874 --> 00:21:04,410
Speaker 0: If you actually ban bad actors like the horse paced people, white supremacists, Republicans, et cetera, when you actually ban those people from platforms, then they migrate and mass to whatever platform will let them do their crap and they keep on going.

325
00:21:05,510 --> 00:21:09,155
Speaker 0: But they are segregated from the better platform at that point.

326
00:21:09,715 --> 00:21:22,270
Speaker 0: But without those people, these platforms can't make money because the money is driven by all these toxic engagements because the only way any of these things make money is selling your data and or selling ads to you via that data.

327
00:21:23,090 --> 00:21:29,860
Speaker 1: There's also the side effect, right, which I think we've talked about in the past of the people who are more likely to be.

328
00:21:29,900 --> 00:21:37,370
Speaker 1: that kind of horrible person who believes in, say, a conspiracy theory is also a more gullible person who is more vulnerable to ads.

329
00:21:37,991 --> 00:21:38,211
Speaker 1: Right.

330
00:21:38,251 --> 00:21:43,436
Speaker 1: You can write, you know, you look at the alignment of scams, you know, fake medicine scams.

331
00:21:43,537 --> 00:21:43,997
Speaker 1: All right.

332
00:21:44,377 --> 00:21:46,199
Speaker 1: And milking those people for money.

333
00:21:46,219 --> 00:21:51,746
Speaker 1: And it's like you make more money off having that kind of horrible person as your customer.

334
00:21:51,966 --> 00:21:55,790
Speaker 1: If you have me as your customer, I block all your ads and I don't buy a damn thing.

335
00:21:55,951 --> 00:21:56,213
Speaker 0: Yep.

336
00:21:56,253 --> 00:21:57,461
Speaker 0: You give me a free to play game.

337
00:21:57,602 --> 00:21:58,649
Speaker 0: I play it for free.

338
00:21:59,690 --> 00:22:00,013
Speaker 1: That's right.

339
00:22:00,093 --> 00:22:02,130
Speaker 1: I don't ever pay for single microtransactions.

340
00:22:02,810 --> 00:22:05,092
Speaker 0: So here's why government has to step in.

341
00:22:05,433 --> 00:22:17,604
Speaker 0: The problem is the ad revenue will follow the worst content for all the reasons we described, unless there are laws and regulations that ban those ads and or that content.

342
00:22:17,944 --> 00:22:24,130
Speaker 0: Because if you remove that, you remove the moral hazard from a company to even go after that business in the first place.

343
00:22:24,471 --> 00:22:29,150
Speaker 0: If the white supremacists aren't making you money, even the evil company will kick them off their platform.

344
00:22:30,336 --> 00:22:30,450
Speaker 1: Yep.

345
00:22:30,850 --> 00:22:38,470
Speaker 0: And the only platform that will keep them is the the actively pro white supremacist platform as opposed to the pro money at all cost platform.

346
00:22:40,214 --> 00:22:40,516
Speaker 1: All right.

347
00:22:40,536 --> 00:22:41,928
Speaker 1: Well, so that's, you know.

348
00:22:42,458 --> 00:22:42,586
Speaker 0: Yep.

349
00:22:42,810 --> 00:22:46,870
Speaker 1: He basically told everyone his plan with the typical evil villain presentation.

350
00:22:48,097 --> 00:22:49,290
Speaker 0: So I can say one thing.

351
00:22:49,531 --> 00:22:52,482
Speaker 1: You can't complain that the evil villain didn't tell us what they were up to.

352
00:22:52,522 --> 00:22:54,590
Speaker 1: We just don't have a Superman to go punch him in the face.

353
00:22:55,371 --> 00:22:55,472
Speaker 0: Yep.

354
00:22:55,734 --> 00:22:57,788
Speaker 1: So I think to see what we can do.

355
00:22:58,290 --> 00:23:11,729
Speaker 0: But in this space, what we're going to see, too, is I think the platform is not going to succeed with a lot of developers because so far I follow and talk to a lot of VR developers just because that's the thing I'm very interested in.

356
00:23:12,750 --> 00:23:20,050
Speaker 0: Categorically, they have all said things over the last week to the effect of you can't fool me, Facebook, meta is just Facebook.

357
00:23:20,530 --> 00:23:21,664
Speaker 0: I'm waiting this one out.

358
00:23:24,032 --> 00:23:25,785
Speaker 1: So even if they have the best hardware, that's not.

359
00:23:25,866 --> 00:23:26,390
Speaker 1: it's not enough.

360
00:23:26,630 --> 00:23:26,759
Speaker 0: Right.

361
00:23:26,781 --> 00:23:26,845
Speaker 0: Yep.

362
00:23:27,451 --> 00:23:28,221
Speaker 1: Betamax was better.

363
00:23:28,322 --> 00:23:28,869
Speaker 1: It wasn't enough.

364
00:23:29,191 --> 00:23:29,452
Speaker 0: Yep.

365
00:23:29,714 --> 00:23:32,330
Speaker 0: The best product is not how you make money in a capitalist society.

366
00:23:32,832 --> 00:23:33,742
Speaker 0: And some other minor news.

367
00:23:33,782 --> 00:23:34,490
Speaker 0: We'll just link to this.

368
00:23:34,650 --> 00:23:37,030
Speaker 0: But this is not as scary as it seems.

369
00:23:37,270 --> 00:23:39,310
Speaker 0: It's only theoretical, but it's worth talking about briefly.

370
00:23:40,551 --> 00:23:55,970
Speaker 0: This article is making the rounds like yesterday and today of vulnerability in compilers, meaning malicious code could be presented in a way that appears nonmalicious until it's compiled and then it becomes malicious.

371
00:23:56,530 --> 00:24:06,670
Speaker 0: And the vulnerability has to do with, of all things, Unicode encoding of text in regard to left to right versus right to left text specifically.

372
00:24:06,831 --> 00:24:10,710
Speaker 1: This is one of those, you know, old tricks, right, where someone sends you a link.

373
00:24:10,870 --> 00:24:12,650
Speaker 1: It looks like Chase Bank dot com.

374
00:24:12,830 --> 00:24:18,630
Speaker 0: But actually, the A is Cyrillic character and the E is like some Estonian Cyrillic adjacent character.

375
00:24:19,371 --> 00:24:19,612
Speaker 1: Yep.

376
00:24:19,692 --> 00:24:24,809
Speaker 1: Because they're using Unicode characters, your eyeballs look at your screen and your font renders it.

377
00:24:25,131 --> 00:24:27,570
Speaker 1: And it looks like the word you think it is, but it's a different word.

378
00:24:27,710 --> 00:24:41,488
Speaker 1: So you can mess with source code in such a way using Unicode right to left characters and whatnot, so that when a human being reviews the code and the pull request with their eyes, that what they see looks like correct code.

379
00:24:41,528 --> 00:24:43,030
Speaker 1: that's going to do what you said it does.

380
00:24:43,472 --> 00:24:47,549
Speaker 1: And they run it through their compiler to compile it and then test it.

381
00:24:48,271 --> 00:24:49,974
Speaker 1: And now it's right.

382
00:24:50,094 --> 00:24:59,710
Speaker 1: if you somehow have, you know, affected the compiler in such a way you have, you know, got some sort of security flaw on their computer of the person testing the code, at least.

383
00:25:00,191 --> 00:25:07,970
Speaker 1: And if that code actually gets deployed, well, that you know, because it you know, it'd be really hard to get the code to also pass the unit tests if anyone tried to test the functionality.

384
00:25:08,654 --> 00:25:09,785
Speaker 1: But if you could do that, whoa.

385
00:25:10,735 --> 00:25:12,029
Speaker 1: You get that code deployed everywhere.

386
00:25:12,650 --> 00:25:22,570
Speaker 0: The net result is actually exploiting this particular vulnerability seems to be about as difficult as exploiting when hash collisions are discovered in a lot of algorithms.

387
00:25:23,132 --> 00:25:25,710
Speaker 1: It's pretty difficult to actually pull this off.

388
00:25:25,810 --> 00:25:35,510
Speaker 1: It's just an interesting theoretical possibility that the old Unicode trick of fooling people's eyes could apply to this sort of situation and not just a normal old phishing situation.

389
00:25:35,550 --> 00:25:46,370
Speaker 0: But the good news is the real story in this article is that researchers identified the possibility before there appear to have ever been actual practical exploits using this.

390
00:25:46,770 --> 00:25:52,890
Speaker 0: And because it was theoretically understood, it is now being patched broadly to prevent this particular attack.

391
00:25:53,230 --> 00:26:04,470
Speaker 0: So it's a success of security research as primary academic research, as opposed to the more common method of find vulnerabilities in the wild and then go patch them.

392
00:26:04,550 --> 00:26:09,221
Speaker 1: Yeah, it shouldn't be too hard for a compiler to prevent this from doing anything bad, right?

393
00:26:09,241 --> 00:26:12,749
Speaker 1: Just detect the characters that flip from right to left and then, you know.

394
00:26:13,675 --> 00:26:14,589
Speaker 0: The old bite eyes.

395
00:26:15,590 --> 00:26:15,683
Speaker 1: Yeah.

396
00:26:17,191 --> 00:26:18,523
Speaker 1: OK, so one more news.

397
00:26:20,510 --> 00:26:21,110
Speaker 1: Yeah, just quickly.

398
00:26:21,730 --> 00:26:28,550
Speaker 1: So we could probably talk a whole lot about Roblox itself, which is a whole thing if you're not aware about the Roblox phenomenon.

399
00:26:28,610 --> 00:26:32,059
Speaker 0: Which I'm curious, let us know, do you know what Roblox is or not?

400
00:26:32,119 --> 00:26:35,829
Speaker 0: Because I've found that people younger than us know what it is.

401
00:26:37,133 --> 00:26:42,910
Speaker 0: People who are older than us who don't have kids have never heard of it or think it's a game and literally don't know the rest.

402
00:26:43,050 --> 00:26:43,890
Speaker 1: It is a game, kind of.

403
00:26:43,951 --> 00:26:44,509
Speaker 1: I mean, yeah, it is.

404
00:26:44,810 --> 00:26:53,170
Speaker 1: It's a strange, it's a really unique thing that is simultaneously amazing and also garbage and child exploitation all at once.

405
00:26:54,871 --> 00:27:01,770
Speaker 1: So, but regardless, they had this enormous outage for something that has so many users and is such a big system.

406
00:27:02,291 --> 00:27:07,466
Speaker 1: It was down for like more than 24 hours, like a very large number of hours.

407
00:27:07,546 --> 00:27:09,030
Speaker 1: It was out and it's like out.

408
00:27:09,330 --> 00:27:18,950
Speaker 0: And the funniest thing is a lot of people would have no idea this happened because none of this news would even except every child on earth who has wealthy enough to have a computer or an iPad or something.

409
00:27:19,291 --> 00:27:30,690
Speaker 0: Oh my god, I found a Twitter thread where someone compiled, like it was a bot finding all these people who looked like parents asking what the fuck Roblox is and why their kids are screaming and crying and flipping out.

410
00:27:31,691 --> 00:27:31,871
Speaker 1: Yep.

411
00:27:32,733 --> 00:27:39,750
Speaker 1: And also, the thing is that the children who are using Roblox, right, are not on Twitter.

412
00:27:40,393 --> 00:27:41,889
Speaker 0: Yeah, Twitter's for old people, guys.

413
00:27:42,731 --> 00:27:50,850
Speaker 1: I'm just saying, any social broadcasting thing, it's like, you know, I guess not until you're more teens are you even using that, right?

414
00:27:51,830 --> 00:27:57,270
Speaker 1: You know, because even if you're an eight year old and even if your parents let you use Twitter, you're not tweeting.

415
00:27:58,092 --> 00:28:00,290
Speaker 1: They let you use Facebook or Instagram or whatever.

416
00:28:01,031 --> 00:28:08,390
Speaker 1: Maybe you'd have fun with filters and photos and Instagram, but, you know, I don't think an eight year old is going to be like, you know, posting on Instagram.

417
00:28:08,491 --> 00:28:11,526
Speaker 1: Why is Roblox down and searching on Instagram for Roblox?

418
00:28:11,586 --> 00:28:12,270
Speaker 1: Why is it down?

419
00:28:12,550 --> 00:28:17,190
Speaker 0: I feel like when I was maybe not when I was eight, but I was like talking to their friends on Instagram.

420
00:28:17,830 --> 00:28:19,658
Speaker 0: Ten to twelve is when I would be doing that kind of stuff.

421
00:28:19,718 --> 00:28:22,390
Speaker 0: Had that even existed or been a possibility at the time.

422
00:28:22,830 --> 00:28:23,171
Speaker 1: Right.

423
00:28:23,232 --> 00:28:28,150
Speaker 1: But, you know, I don't think that, you know, broadcast information, you know, like kids are probably aren't just reading the news.

424
00:28:28,250 --> 00:28:30,098
Speaker 1: So why would you be interested in, you know, news?

425
00:28:30,359 --> 00:28:30,620
Speaker 1: Right.

426
00:28:30,741 --> 00:28:32,890
Speaker 1: It's like, I don't know.

427
00:28:33,110 --> 00:28:35,906
Speaker 1: There is a eight year old going on forums somewhere.

428
00:28:35,987 --> 00:28:36,530
Speaker 1: There's a forum.

429
00:28:36,590 --> 00:28:37,553
Speaker 0: You know what they are doing?

430
00:28:37,613 --> 00:28:43,010
Speaker 0: So they are streaming the Roblox crap on Twitch and YouTube, usually with accounts, probably a lot.

431
00:28:43,210 --> 00:28:48,110
Speaker 1: Yes, but not in any of the places where like this information broadcasting going on.

432
00:28:48,210 --> 00:28:48,350
Speaker 1: Right.

433
00:28:48,390 --> 00:28:51,315
Speaker 1: The only news story that gets broadcast is Roblox is down.

434
00:28:51,395 --> 00:28:59,970
Speaker 1: But you can't see the actual terror and, you know, you know, pain that that outage caused.

435
00:28:59,970 --> 00:29:02,618
Speaker 0: These kids were losing money to the Internet.

436
00:29:02,739 --> 00:29:06,190
Speaker 0: These kids are paying for ads for the games they make.

437
00:29:06,690 --> 00:29:06,951
Speaker 0: They have.

438
00:29:06,971 --> 00:29:12,830
Speaker 0: there's a whole like real money ecosystem around Roblox that a lot of parents don't think even realizes there.

439
00:29:13,411 --> 00:29:15,870
Speaker 1: There's a lot when they check their credit card statement.

440
00:29:16,090 --> 00:29:16,251
Speaker 0: Yeah.

441
00:29:16,411 --> 00:29:21,488
Speaker 0: Or they will now because little Johnny was flipping the fuck out that Roblox was down for more than 24 hours.

442
00:29:22,730 --> 00:29:26,282
Speaker 1: Also, if you have to get a gift for some kid, you know, you buy you can.

443
00:29:26,362 --> 00:29:28,850
Speaker 1: I think you can buy like a Robux gift card at the drugstore.

444
00:29:28,870 --> 00:29:31,949
Speaker 0: Pretty sure you can just buy Roblox bucks with cash.

445
00:29:33,090 --> 00:29:36,769
Speaker 1: Almost any almost any kid's going to love some Robux as a gift this holiday season.

446
00:29:38,192 --> 00:29:39,616
Speaker 0: Oh, this article is spot on.

447
00:29:40,841 --> 00:29:43,590
Speaker 0: Roblox is valued at almost 30 billion pounds.

448
00:29:43,971 --> 00:29:48,370
Speaker 0: Yet these child workers see only a fraction of that money as their labor drives its valuation up.

449
00:29:49,311 --> 00:29:57,452
Speaker 1: If you make content for Roblox and then someone buys your content that you made, you split it with the Roblox company 3070.

450
00:29:57,452 --> 00:29:57,854
Speaker 0: That's worse.

451
00:29:57,874 --> 00:30:02,431
Speaker 0: They they get the 70.

452
00:30:02,431 --> 00:30:05,478
Speaker 1: It's people are complaining about Apple getting the 30.

453
00:30:05,478 --> 00:30:10,790
Speaker 1: Roblox gets the 70 70 and children made something.

454
00:30:10,850 --> 00:30:13,796
Speaker 1: So it's a child labor producing a digital work.

455
00:30:14,257 --> 00:30:20,670
Speaker 1: It sells the child gets 30 percent of the revenues from the sale and the company gets 70.

456
00:30:20,670 --> 00:30:20,951
Speaker 0: Yep.

457
00:30:21,632 --> 00:30:22,393
Speaker 1: That's all right.

458
00:30:22,713 --> 00:30:33,388
Speaker 0: I don't think we saw yet what the cause of the outage was, but it's interesting because this I don't think Roblox has ever had a major outage before, at least not since it got really big.

459
00:30:33,629 --> 00:30:34,329
Speaker 0: that I'm aware of.

460
00:30:35,210 --> 00:30:37,435
Speaker 1: So I think it's also just unique in like.

461
00:30:37,976 --> 00:30:44,350
Speaker 1: when in history have like, you know, it's not like Tickle Me Elmo has like an outage or a child's favorite toy.

462
00:30:44,471 --> 00:30:46,421
Speaker 1: All our NES is didn't simultaneously have.

463
00:30:46,883 --> 00:30:48,170
Speaker 1: I remember playing Nintendo.

464
00:30:48,711 --> 00:30:56,650
Speaker 0: I remember when broadcast TV like like one of the networks went down in Detroit for like days.

465
00:30:56,971 --> 00:30:57,514
Speaker 0: We just couldn't.

466
00:30:57,574 --> 00:30:58,920
Speaker 0: One of the channels was just gone.

467
00:30:59,442 --> 00:31:00,205
Speaker 0: That was an existential.

468
00:31:00,225 --> 00:31:01,270
Speaker 1: You got other channels, though.

469
00:31:01,630 --> 00:31:02,774
Speaker 0: Yeah, like six of them.

470
00:31:02,794 --> 00:31:07,188
Speaker 0: It was it was the ladies like hurt that many channels back then.

471
00:31:08,371 --> 00:31:17,070
Speaker 1: I'm just saying it's, you know, but there isn't a case for like every kid, you know, so many kids like everyone's same favorite toy all broke at the same time.

472
00:31:17,210 --> 00:31:18,152
Speaker 1: Nobody could play.

473
00:31:18,533 --> 00:31:18,794
Speaker 0: I do.

474
00:31:18,914 --> 00:31:25,489
Speaker 0: All bicycles just stopped working when the BBS that I got my pirated games from when I was very young, dialing into disappeared.

475
00:31:26,252 --> 00:31:28,989
Speaker 0: I guess I don't know if I got raided or whatever, but it got shut down.

476
00:31:29,832 --> 00:31:35,570
Speaker 0: That was how I found all the other nerds in my school, because when I got into school, some people were talking about that.

477
00:31:35,711 --> 00:31:38,209
Speaker 0: But most people had no idea what the fuck that was.

478
00:31:38,571 --> 00:31:45,390
Speaker 0: And I actually made a bunch of friends by figuring out who else was using that anonymous, pirating BBS in my neighborhood.

479
00:31:46,510 --> 00:31:47,533
Speaker 0: Anyway, one last news.

480
00:31:48,174 --> 00:32:04,330
Speaker 0: And I talk about this on a Monday because something that technology people, especially computer types, people who do the kind of jobs that me and Scott do really need to internalize is that the main value of technology is not developing the next new technology.

481
00:32:04,751 --> 00:32:13,210
Speaker 0: The main value of technology is taking existing technology and deploying it in more places, more fairly and more equitably.

482
00:32:13,897 --> 00:32:15,050
Speaker 1: And more wisely.

483
00:32:15,570 --> 00:32:16,331
Speaker 0: Yes, more wisely.

484
00:32:16,852 --> 00:32:20,016
Speaker 0: That impacts people more than any new technology.

485
00:32:20,096 --> 00:32:31,010
Speaker 0: Making Facebook better will not impact nearly as many people as making more trains or making more clean water or taking an old vaccine and getting it in more people's hands.

486
00:32:31,730 --> 00:32:33,609
Speaker 0: So a story that just happened.

487
00:32:34,150 --> 00:32:34,252
Speaker 1: Veins.

488
00:32:34,475 --> 00:32:34,820
Speaker 1: Veins.

489
00:32:35,084 --> 00:32:35,429
Speaker 1: Arteries.

490
00:32:35,771 --> 00:32:35,952
Speaker 0: Yep.

491
00:32:36,475 --> 00:32:39,570
Speaker 0: This is a huge story in New York City.

492
00:32:40,150 --> 00:33:00,470
Speaker 0: The Long Island Railroad now has the ability for its trains to terminate, not just in Penn Station, which is out of the way and sucks shit, but also in Grand Central Terminal, one of the most used and most important train stations in the history of the goddamn world.

493
00:33:01,033 --> 00:33:04,550
Speaker 0: Central to New York City, the best train station we got.

494
00:33:05,550 --> 00:33:19,770
Speaker 0: It is the amount of economic loss from people taking Long Island Railroad trains to Penn Station only to then have to transit back to Grand Central to get to the place they were trying to get to, to connect to the rest of the transportation infrastructure more reliably.

495
00:33:20,551 --> 00:33:25,410
Speaker 0: Just investing in what it took to make these trains go to one different place.

496
00:33:25,893 --> 00:33:28,589
Speaker 0: This is a story that is decades in the making.

497
00:33:29,111 --> 00:33:38,970
Speaker 1: When we graduated college, right, and I was working in, right, I would take the train to Grand Central every day, work day, right, to come to work.

498
00:33:39,671 --> 00:33:42,660
Speaker 1: And they were like, yeah, we're going to do this project.

499
00:33:42,701 --> 00:33:45,510
Speaker 1: So that was a fucking 6, 15 plus years ago.

500
00:33:46,090 --> 00:33:51,534
Speaker 1: It's like this was proposed in 1963.

501
00:33:51,534 --> 00:33:53,050
Speaker 1: When did they actually start digging, though?

502
00:33:53,332 --> 00:33:55,129
Speaker 0: Oh, not so actually a long time.

503
00:33:55,270 --> 00:33:56,189
Speaker 1: More than 10 years ago.

504
00:33:56,450 --> 00:33:57,669
Speaker 0: Definitely more than 10 years ago.

505
00:33:58,090 --> 00:34:02,110
Speaker 0: The story is so long that I'm struggling to scroll through this fast enough to actually find all the milestones.

506
00:34:03,431 --> 00:34:03,694
Speaker 1: Right.

507
00:34:03,714 --> 00:34:05,269
Speaker 1: But yeah, they ran a train.

508
00:34:06,030 --> 00:34:12,050
Speaker 1: You know, it's not ready yet, but they ran a train with passengers from Long Island to Grand Central.

509
00:34:12,272 --> 00:34:12,737
Speaker 1: It made it.

510
00:34:12,838 --> 00:34:14,130
Speaker 1: They got off at Grand Central.

511
00:34:14,469 --> 00:34:20,790
Speaker 1: Then they walked out to the street and it's going to be available to everybody next year at some point.

512
00:34:20,949 --> 00:34:21,914
Speaker 0: OK, so they start.

513
00:34:21,934 --> 00:34:26,989
Speaker 0: they started work and supposedly they started work in the 70s in 1975.

514
00:34:26,989 --> 00:34:31,043
Speaker 0: They canceled the whole thing because New York City was out of money in the 90s, 1990s.

515
00:34:32,750 --> 00:34:48,809
Speaker 0: So the late nineteen hundreds, a new set of studies showed that the majority of all Long Island railroad riders, which is one of the most used rail lines in the world, were trying to get somewhere closer to Grand Central than Defense Station.

516
00:34:50,851 --> 00:35:03,090
Speaker 1: There's more places of employment near 42nd and Lex than there are on 34th and like, you know, you go behind the Madden Square Garden unless you're working at the post office.

517
00:35:03,211 --> 00:35:04,416
Speaker 1: I don't know where you go.

518
00:35:04,537 --> 00:35:07,350
Speaker 1: It's like there's no big, tall office buildings back there.

519
00:35:07,750 --> 00:35:16,250
Speaker 0: To build this new terminal under Grand Central Terminal was one of the larger engineering projects in New York City's history.

520
00:35:16,851 --> 00:35:19,235
Speaker 0: And the scale is enormous.

521
00:35:19,295 --> 00:35:25,587
Speaker 0: The scale and the work involved in the 11 billion dollars is this was using mostly 1970s technology.

522
00:35:28,710 --> 00:35:31,290
Speaker 1: Let me dig a tunnel, play a train track, put a signal.

523
00:35:32,130 --> 00:35:35,310
Speaker 0: The technology to lay tunnels has been largely perfected.

524
00:35:36,030 --> 00:35:42,370
Speaker 0: The boring company has not really done anything novel in this regard because that is not the problem.

525
00:35:42,511 --> 00:35:47,090
Speaker 0: Just like how tech bros like to say, oh, I made a machine that can 3D print houses faster.

526
00:35:47,590 --> 00:35:47,711
Speaker 0: Yeah.

527
00:35:47,811 --> 00:35:51,470
Speaker 0: Building houses physically is not the bottleneck on housing people.

528
00:35:52,890 --> 00:35:57,946
Speaker 0: You don't say we could build houses for everyone in the world pretty much instantly if we wanted to.

529
00:35:58,006 --> 00:35:59,210
Speaker 0: That is far from the problem.

530
00:35:59,771 --> 00:36:03,164
Speaker 1: So let's let's let's just end this news section with a piece of shit.

531
00:36:03,184 --> 00:36:04,750
Speaker 1: news I saw today about piece of shit.

532
00:36:04,850 --> 00:36:05,872
Speaker 1: Ellen Musk.

533
00:36:06,634 --> 00:36:09,801
Speaker 1: The headline I saw said U.N.

534
00:36:10,302 --> 00:36:14,030
Speaker 1: says they could feed all the hungry people if they just had like eight billion.

535
00:36:14,490 --> 00:36:14,992
Speaker 1: Right.

536
00:36:15,233 --> 00:36:16,236
Speaker 1: And piece of shit.

537
00:36:16,296 --> 00:36:20,250
Speaker 1: Ellen Musk, you know, he could have said, here's the eight billion.

538
00:36:20,651 --> 00:36:20,872
Speaker 1: Right.

539
00:36:21,113 --> 00:36:22,077
Speaker 1: I'm just going to give it to you.

540
00:36:22,257 --> 00:36:25,190
Speaker 0: He wouldn't even notice having that losing that eight bill.

541
00:36:25,791 --> 00:36:26,353
Speaker 1: Of course not.

542
00:36:26,373 --> 00:36:27,155
Speaker 1: But you know what he said?

543
00:36:27,175 --> 00:36:31,670
Speaker 1: He said, prove it to me and I'll sell Tesla stock and give you eight billion.

544
00:36:32,531 --> 00:36:35,650
Speaker 1: It's like, yeah, prove it to me means you're not going to fucking do it.

545
00:36:35,931 --> 00:36:39,930
Speaker 1: What is going to be one of those things where, like, yeah, there is no proof, right, that he would accept.

546
00:36:40,351 --> 00:36:41,294
Speaker 1: He's just being an asshole.

547
00:36:41,335 --> 00:36:45,250
Speaker 0: Well, you know, if they proved it to him, he just called them pedophiles and not give them the money anyway.

548
00:36:45,950 --> 00:36:46,151
Speaker 1: Right.

549
00:36:46,191 --> 00:36:46,532
Speaker 1: Exactly.

550
00:36:46,592 --> 00:36:53,730
Speaker 1: And also, I'd like to point out none of the other billionaires are, you know, giving eight billion either so they can all go fuck themselves as well.

551
00:36:54,270 --> 00:36:59,669
Speaker 1: If I had eight billion, even if eight billion was literally all the dollars I had, I'd be like, here you go.

552
00:37:00,492 --> 00:37:00,653
Speaker 0: Yep.

553
00:37:00,915 --> 00:37:03,248
Speaker 1: I have zero dollars now, but there's no hungry people on earth.

554
00:37:03,288 --> 00:37:03,610
Speaker 1: So cool.

555
00:37:03,850 --> 00:37:04,192
Speaker 0: Eight billion.

556
00:37:04,433 --> 00:37:04,855
Speaker 0: The U.S.

557
00:37:04,875 --> 00:37:08,070
Speaker 0: Congress could just add that to the budget and not notice.

558
00:37:09,112 --> 00:37:09,293
Speaker 1: Yep.

559
00:37:10,421 --> 00:37:11,025
Speaker 1: And they don't either.

560
00:37:11,085 --> 00:37:11,649
Speaker 1: So fuck them, too.

561
00:37:12,010 --> 00:37:12,150
Speaker 0: Yeah.

562
00:37:12,591 --> 00:37:13,193
Speaker 0: Well, fuck.

563
00:37:13,253 --> 00:37:19,188
Speaker 0: in particular, the forty one Republican senators that themselves represent 20 percent of Americans.

564
00:37:20,230 --> 00:37:23,545
Speaker 1: I think even if like, oh, yeah, they're dead.

565
00:37:23,585 --> 00:37:24,570
Speaker 0: They're definitely.

566
00:37:27,273 --> 00:37:28,656
Speaker 1: If every single U.S.

567
00:37:28,676 --> 00:37:34,490
Speaker 1: senator was a clone of Chuck Schumer, I don't think you would get the eight billion for the starving people either.

568
00:37:34,850 --> 00:37:35,193
Speaker 0: I don't know.

569
00:37:35,596 --> 00:37:37,150
Speaker 0: I bet you could get the eight billion for the starving people.

570
00:37:37,150 --> 00:37:42,550
Speaker 1: You would need a hundred AOCs in the Senate or sixty something, whatever the number is.

571
00:37:49,741 --> 00:37:52,107
Speaker 0: But anyway, things of the day.

572
00:37:52,208 --> 00:37:55,276
Speaker 0: So I think this is old, but I found out about it recently.

573
00:37:55,738 --> 00:37:56,600
Speaker 0: It's called Orbis.

574
00:37:57,161 --> 00:37:59,126
Speaker 0: It is a free online Web site thing.

575
00:37:59,808 --> 00:38:04,500
Speaker 0: It's basically Google Maps directions for ancient Rome.

576
00:38:05,220 --> 00:38:13,579
Speaker 0: You choose the season, the modes of transportation, what you're prioritizing, like speed versus economy versus security, et cetera.

577
00:38:14,340 --> 00:38:15,249
Speaker 1: Avoiding barbarians.

578
00:38:15,270 --> 00:38:16,260
Speaker 1: Yeah, I was going to say security.

579
00:38:16,741 --> 00:38:19,007
Speaker 0: It'll outline like how much will it cost?

580
00:38:19,228 --> 00:38:20,371
Speaker 0: How long will it take you?

581
00:38:20,391 --> 00:38:23,700
Speaker 0: It's it's surprisingly thorough.

582
00:38:24,222 --> 00:38:28,640
Speaker 0: This application, this is I've been playing with this for a couple of days now.

583
00:38:28,920 --> 00:38:31,250
Speaker 0: It is really, really fucking neat.

584
00:38:31,752 --> 00:38:33,460
Speaker 1: And also needs to make a game on that.

585
00:38:33,841 --> 00:38:34,122
Speaker 0: Oh, yeah.

586
00:38:34,162 --> 00:38:34,604
Speaker 0: You could build it.

587
00:38:34,625 --> 00:38:37,800
Speaker 0: You could build a game just on top of this data, like straight up.

588
00:38:37,880 --> 00:38:39,124
Speaker 0: And it would probably be great.

589
00:38:39,806 --> 00:38:44,220
Speaker 0: In fact, I'm not going to say anything because I know I'm not going to do it.

590
00:38:44,300 --> 00:38:44,682
Speaker 0: Never mind.

591
00:38:45,505 --> 00:38:46,489
Speaker 0: OK, so what do you got?

592
00:38:46,610 --> 00:38:49,080
Speaker 1: So here's a really cool Web site called Famicom Party.

593
00:38:49,281 --> 00:38:56,700
Speaker 1: And someone's writing a book basically on how to make an NES and NES game in scratch from assembly in assembly.

594
00:38:57,560 --> 00:39:02,580
Speaker 1: So there's a lot of stuff out there about making games for NES and lots of old hardware.

595
00:39:03,541 --> 00:39:06,733
Speaker 1: And since the hardware is much simpler, doing it in assembly is feasible.

596
00:39:06,793 --> 00:39:08,479
Speaker 1: In fact, that's what they did it in back in the day.

597
00:39:09,862 --> 00:39:12,700
Speaker 1: But a lot of times, you know, you're going to use other tools, right?

598
00:39:12,821 --> 00:39:16,700
Speaker 1: I know there's an NES maker and a Game Boy maker tools out there that are quite good.

599
00:39:16,780 --> 00:39:28,220
Speaker 1: That will let you make a lot of games and actually output ROMs that will work on old hardware without having to learn assembly language, clearly, and not even having to learn any difficult programming at all.

600
00:39:29,201 --> 00:39:31,005
Speaker 1: So most people are not doing that.

601
00:39:31,065 --> 00:39:38,240
Speaker 1: And, you know, there are a lot of YouTube channels that are like teaching you specific things like retro game mechanics explained and whatnot.

602
00:39:38,320 --> 00:39:40,486
Speaker 1: But I haven't seen too many.

603
00:39:40,526 --> 00:39:45,260
Speaker 1: like ground up right here is literally everything, you know.

604
00:39:45,440 --> 00:39:46,742
Speaker 1: So this is really cool.

605
00:39:46,762 --> 00:40:01,800
Speaker 1: And I haven't actually started going through this yet, but it looks structurally like, yeah, you could come to this knowing, you know, I guess an amount about computers in general, but and then be able to make software that would run on an NES.

606
00:40:02,160 --> 00:40:14,300
Speaker 0: Not only that, but a lot of those videos that explain really deep glitches, really interesting speedrun stuff, just having read a little bit of this already, it would definitely make understanding those explanations very straightforward.

607
00:40:15,381 --> 00:40:15,561
Speaker 1: Right.

608
00:40:15,681 --> 00:40:19,127
Speaker 1: This could also be very useful to you, even if you don't develop an entire game.

609
00:40:19,628 --> 00:40:26,660
Speaker 1: If you want to like figure out some game genie code for a game or make a ROM hack, this would be very useful to go through.

610
00:40:26,981 --> 00:40:27,201
Speaker 1: Right.

611
00:40:27,482 --> 00:40:34,060
Speaker 1: Because then when you went to go get a ROM and read it and, you know, try to, you know, mess with it, you would understand what the hell was going on.

612
00:40:34,380 --> 00:40:34,541
Speaker 0: Yep.

613
00:40:35,865 --> 00:40:39,880
Speaker 0: In the better moment, the Geek Nights Book Club book, Taylor Genji, I paused it to read a shounen manga.

614
00:40:40,320 --> 00:40:44,500
Speaker 0: Going to be going straight back to Genji after that, because that shounen manga is shounen manga.

615
00:40:44,560 --> 00:40:47,709
Speaker 1: I'm reading Genji because I got the new Kindle Paperwhite.

616
00:40:48,331 --> 00:40:48,993
Speaker 1: It's pretty nice.

617
00:40:49,334 --> 00:40:50,738
Speaker 1: I'm reading Genji on it.

618
00:40:50,798 --> 00:40:51,440
Speaker 1: So there you go.

619
00:40:51,620 --> 00:40:59,100
Speaker 0: I also got a bunch of vacation time I got to burn, so I'm going to have a lot of time off doing like local travel for skiing and nothing.

620
00:40:59,400 --> 00:41:00,700
Speaker 1: Your October prediction is wrong.

621
00:41:00,940 --> 00:41:02,191
Speaker 0: My October prediction is wrong.

622
00:41:02,655 --> 00:41:03,120
Speaker 0: That was fair.

623
00:41:03,280 --> 00:41:03,520
Speaker 1: I was right.

624
00:41:03,980 --> 00:41:04,647
Speaker 0: You were right.

625
00:41:04,809 --> 00:41:05,840
Speaker 0: I think end of year is very feasible.

626
00:41:06,881 --> 00:41:15,260
Speaker 1: I am not working the week of Thanksgiving in the United States, which is, you know, one, two, three weeks from now.

627
00:41:15,280 --> 00:41:19,600
Speaker 0: I'm taking like three days off that week and I'm taking the whole week before the Christmas week off.

628
00:41:19,921 --> 00:41:23,331
Speaker 1: And I'm taking like after the week after Thanksgiving.

629
00:41:23,391 --> 00:41:25,377
Speaker 1: So Black Friday, obviously not working.

630
00:41:25,437 --> 00:41:26,280
Speaker 1: Weekend not working.

631
00:41:26,781 --> 00:41:29,048
Speaker 1: That Monday, Tuesday, Wednesday, I work.

632
00:41:29,369 --> 00:41:32,098
Speaker 1: That Wednesday after Thanksgiving is December 1st.

633
00:41:32,940 --> 00:41:35,367
Speaker 1: That is the last day I am working in the year 2021.

634
00:41:35,367 --> 00:41:40,641
Speaker 1: Not working again until January 3rd, 2022.

635
00:41:40,641 --> 00:41:43,860
Speaker 0: Man, I got at some point I got to get out of an industry that never closes.

636
00:41:44,902 --> 00:41:48,862
Speaker 1: And I still have 10 vacation days rolled over to 2022.

637
00:41:48,862 --> 00:41:52,580
Speaker 0: Never worked in my life in a place that could like shut down or close.

638
00:41:53,180 --> 00:41:57,180
Speaker 0: It's either been factories or fabs or hospitals or capital markets.

639
00:41:57,220 --> 00:42:01,400
Speaker 1: My previous job and current job, the company just closes Christmas to New Year's.

640
00:42:03,365 --> 00:42:08,900
Speaker 0: If I ran a game studio, we would just close for that holiday season and we'd pick one like to like a month in the summer.

641
00:42:09,481 --> 00:42:10,525
Speaker 1: You know how much I would close?

642
00:42:10,585 --> 00:42:14,620
Speaker 1: I'd be closing on Jewish holidays, Muslim holidays, Chinese New Year.

643
00:42:14,720 --> 00:42:17,168
Speaker 0: I would definitely close the month of July.

644
00:42:17,249 --> 00:42:18,332
Speaker 0: The best summer times.

645
00:42:18,413 --> 00:42:20,540
Speaker 0: Why should anyone be working during the month of July?

646
00:42:20,540 --> 00:42:25,920
Speaker 1: I'd be closing on if there's a bank holiday in any country in which we operate, closed whole company.

647
00:42:26,220 --> 00:42:26,863
Speaker 1: Not right today.

648
00:42:27,064 --> 00:42:28,731
Speaker 1: It was like All Saints Day.

649
00:42:28,811 --> 00:42:29,012
Speaker 1: Right.

650
00:42:29,032 --> 00:42:30,780
Speaker 1: So people in France, I think, are not working.

651
00:42:31,100 --> 00:42:35,458
Speaker 1: If I had a French office and those people got off, I'd say whole world is off because it's All Saints Day.

652
00:42:35,538 --> 00:42:35,920
Speaker 1: Deal with it.

653
00:42:36,120 --> 00:42:42,940
Speaker 0: So I guess I actually have an anecdote around that because, you know, I used to work in a company where we had a subsidiary office in India, in Hyderabad.

654
00:42:43,564 --> 00:42:44,591
Speaker 0: And we worked with a lot.

655
00:42:44,611 --> 00:42:45,960
Speaker 1: I wonder what kind of cool holidays they got.

656
00:42:46,240 --> 00:42:53,600
Speaker 0: So you basically register what your religion is and then you get that set of holidays for yourself.

657
00:42:55,563 --> 00:42:57,800
Speaker 1: I'm letting everybody off every religious holiday.

658
00:42:58,181 --> 00:43:03,120
Speaker 0: So I do know for a fact, if you did that, your office would basically never open.

659
00:43:04,201 --> 00:43:04,442
Speaker 1: Good.

660
00:43:05,284 --> 00:43:05,505
Speaker 0: Yeah.

661
00:43:05,806 --> 00:43:10,520
Speaker 0: But that leads to other externalities if our whole society decided to stop working.

662
00:43:10,660 --> 00:43:16,780
Speaker 1: But are there that many holidays that there's literally a serious holiday every day for nearly every day?

663
00:43:17,240 --> 00:43:18,645
Speaker 0: Like kind of for real.

664
00:43:18,705 --> 00:43:21,013
Speaker 1: It would be a problem if you would think there would be.

665
00:43:21,133 --> 00:43:23,160
Speaker 1: there's got to be days where there's no holiday.

666
00:43:23,720 --> 00:43:25,043
Speaker 1: Not many.

667
00:43:25,083 --> 00:43:32,980
Speaker 0: If you earnestly engage with every major faith group and also all of the secular groups that have holidays in the world.

668
00:43:35,400 --> 00:43:35,742
Speaker 1: I don't know.

669
00:43:36,144 --> 00:43:37,289
Speaker 1: You got to draw a line somewhere.

670
00:43:37,309 --> 00:43:37,731
Speaker 0: But anyway.

671
00:43:37,751 --> 00:43:37,911
Speaker 0: Yeah.

672
00:43:37,932 --> 00:43:39,820
Speaker 0: Well, the line I draw is much more simple.

673
00:43:40,161 --> 00:43:42,366
Speaker 1: The only ice cream day doesn't get a day off.

674
00:43:42,406 --> 00:43:48,760
Speaker 0: The only reason we pay people to do things is because otherwise they won't do those things.

675
00:43:49,440 --> 00:43:52,672
Speaker 0: So if people aren't doing the things we have to pay them more.

676
00:43:53,114 --> 00:43:54,499
Speaker 0: Somebody like my dad was a fireman.

677
00:43:54,559 --> 00:43:54,700
Speaker 0: Right.

678
00:43:55,320 --> 00:44:00,060
Speaker 0: Somebody's got to be a firefighter on like Christmas Day because a fire might happen.

679
00:44:00,481 --> 00:44:04,020
Speaker 0: You know, you do if you can't find people willing to work on those days.

680
00:44:04,420 --> 00:44:06,691
Speaker 0: You pay them a shit ton to work on those days.

681
00:44:07,033 --> 00:44:08,460
Speaker 0: And that's how you solve that problem.

682
00:44:08,881 --> 00:44:13,696
Speaker 0: Every other solution to that problem is unfair and capitalist.

683
00:44:14,017 --> 00:44:15,080
Speaker 1: Let's move on to the main bit.

684
00:44:15,340 --> 00:44:15,741
Speaker 1: Main bit.

685
00:44:15,802 --> 00:44:17,127
Speaker 0: Documentation.

686
00:44:17,829 --> 00:44:18,231
Speaker 1: Yeah.

687
00:44:18,291 --> 00:44:18,773
Speaker 1: Some listen.

688
00:44:18,813 --> 00:44:20,700
Speaker 1: The listener sent this one in on our forum.

689
00:44:21,140 --> 00:44:27,620
Speaker 1: So we've talked about documentation of us systems before, but never talked about documentation in general.

690
00:44:27,941 --> 00:44:28,203
Speaker 1: Right.

691
00:44:28,767 --> 00:44:30,880
Speaker 1: So the first thing you got to know, what is documentation?

692
00:44:31,020 --> 00:44:32,002
Speaker 1: Documentation?

693
00:44:32,303 --> 00:44:41,780
Speaker 1: is you have some technology of some kind and there are going to be people who need to interact with that technology from some purpose.

694
00:44:42,720 --> 00:45:01,247
Speaker 1: And they there's no way, no matter how well designed your technology is, that the person is going to be able to just know everything they need to know to get the full use out of that technology and use all the features and everything without instructions.

695
00:45:01,408 --> 00:45:17,120
Speaker 1: You need to write in a human readable language or explain in a human spoken language or both for accessibility reasons, instructions on all the various possible ways that the technology works.

696
00:45:17,560 --> 00:45:29,273
Speaker 0: Now, we will specifically exclude simplistic and toy examples of things that have a 100 percent intrinsically self-evident user experience, because those things tend to be not useful examples.

697
00:45:29,654 --> 00:45:31,340
Speaker 0: Most things need some documentation.

698
00:45:31,980 --> 00:45:32,863
Speaker 1: Well, consider this.

699
00:45:32,964 --> 00:45:33,205
Speaker 1: Right.

700
00:45:33,566 --> 00:45:36,578
Speaker 1: Think about a really simple Apple product like an iPod.

701
00:45:36,839 --> 00:45:37,040
Speaker 1: Right.

702
00:45:38,040 --> 00:45:39,607
Speaker 1: It's still an iPod.

703
00:45:39,627 --> 00:45:42,720
Speaker 1: I don't know if they still make any iPod things, but the iPod came.

704
00:45:43,261 --> 00:45:46,158
Speaker 1: You know, it doesn't need much of a manual because it's so straightforward.

705
00:45:46,199 --> 00:45:46,380
Speaker 1: Right.

706
00:45:46,800 --> 00:45:57,147
Speaker 1: But it comes with a really tiny manual that tells you there's still some things that you're not going to be able to figure out, even from like the best possible design.

707
00:45:57,168 --> 00:46:01,340
Speaker 0: I guess I'm saying I've played some games that literally have zero documentation.

708
00:46:02,840 --> 00:46:02,981
Speaker 1: Yeah.

709
00:46:03,001 --> 00:46:09,098
Speaker 1: Like even Mario, it's like you just need to be like, hey, forward goes, push the arrows direction you want to go, hold B, run faster.

710
00:46:09,920 --> 00:46:10,626
Speaker 1: You need to know that.

711
00:46:10,706 --> 00:46:10,848
Speaker 1: Right.

712
00:46:10,888 --> 00:46:12,077
Speaker 1: Because you're never going to figure that one out.

713
00:46:13,140 --> 00:46:14,408
Speaker 1: I think I did figure it out.

714
00:46:14,428 --> 00:46:16,260
Speaker 1: I don't think I understood that from a manual.

715
00:46:17,780 --> 00:46:22,820
Speaker 1: You could figure it out, but I guess the other point is not necessarily immediately obvious.

716
00:46:22,880 --> 00:46:33,480
Speaker 0: Well, you headed me off because my second part of that is a lot of things that appear to not have documentation have just embedded their documentation in a clever way that you don't recognize as documentation.

717
00:46:34,000 --> 00:46:38,080
Speaker 0: A tutorial pop ups that like guide you when you try to use a new thing.

718
00:46:38,320 --> 00:46:39,426
Speaker 1: I hate the guided pop ups.

719
00:46:39,466 --> 00:46:41,578
Speaker 1: I just want I always just ignore them all.

720
00:46:41,598 --> 00:46:42,040
Speaker 1: I don't know.

721
00:46:42,280 --> 00:46:44,488
Speaker 1: Like I even saw one recently that really pissed me off.

722
00:46:44,589 --> 00:46:47,900
Speaker 1: It was like the first is one of those little pop up tutorials.

723
00:46:48,863 --> 00:46:55,520
Speaker 1: And the first pop up said, all right, we know it was something along the lines of like, let's get this over with.

724
00:46:55,681 --> 00:46:59,360
Speaker 1: And I'm just like, if you know this is fucking bullshit, get this out of my fucking face.

725
00:46:59,620 --> 00:47:00,221
Speaker 0: Let me skip it.

726
00:47:01,002 --> 00:47:01,382
Speaker 1: Yeah.

727
00:47:01,643 --> 00:47:03,125
Speaker 1: But let me have a skip all button.

728
00:47:03,805 --> 00:47:04,346
Speaker 1: They didn't have it.

729
00:47:15,261 --> 00:47:22,220
Speaker 0: For many purposes, even though it probably doesn't work as well as people think it does, and it's probably not complete.

730
00:47:22,840 --> 00:47:22,980
Speaker 0: Yep.

731
00:47:23,462 --> 00:47:28,580
Speaker 0: I think complete documentation is increasingly rare across all technology domains.

732
00:47:28,600 --> 00:47:29,423
Speaker 0: All right.

733
00:47:29,443 --> 00:47:30,085
Speaker 1: Well, it depends.

734
00:47:30,165 --> 00:47:30,326
Speaker 1: Right.

735
00:47:30,366 --> 00:47:34,600
Speaker 1: So the first with the documentation we mostly been talking about here is user documentation.

736
00:47:34,922 --> 00:47:35,102
Speaker 1: Right.

737
00:47:35,183 --> 00:47:38,760
Speaker 1: Someone who is going to use the technology needs instructions.

738
00:47:39,121 --> 00:47:47,500
Speaker 1: And those instructions should tell the person every possible way to and also warnings on how not to operate the technology.

739
00:47:47,620 --> 00:47:48,765
Speaker 1: What button does what?

740
00:47:49,268 --> 00:47:49,429
Speaker 1: Right.

741
00:47:49,469 --> 00:47:50,333
Speaker 1: You drive in the car.

742
00:47:50,373 --> 00:47:51,720
Speaker 1: Here's what the steering wheel does.

743
00:47:51,880 --> 00:47:54,940
Speaker 1: Here's what the pedals do is how to use the red buttons on the radio.

744
00:47:54,960 --> 00:48:00,920
Speaker 0: Cars are a good example that still come with what I would consider absolutely complete and thorough documentation.

745
00:48:01,561 --> 00:48:01,781
Speaker 1: Yep.

746
00:48:01,841 --> 00:48:09,520
Speaker 1: The user manual that comes in the glove box, you know, the driver's guide or whatever, has complete user documentation for the car.

747
00:48:09,620 --> 00:48:09,820
Speaker 0: Yep.

748
00:48:09,961 --> 00:48:21,260
Speaker 0: Not it does not have complete service documentation, but it goes way deeper into service than people, I think, realize or expect because most consumers.

749
00:48:21,280 --> 00:48:27,240
Speaker 1: It's all the user accessible service things, how to change tires, how to fill the fluids, how to check the oil.

750
00:48:28,120 --> 00:48:30,446
Speaker 0: All the what all these doable.

751
00:48:30,967 --> 00:48:31,208
Speaker 0: Yep.

752
00:48:31,649 --> 00:48:35,840
Speaker 1: It doesn't tell you like it doesn't give you a map of what where the check balls are in the transmission.

753
00:48:36,541 --> 00:48:36,761
Speaker 1: Yeah.

754
00:48:36,861 --> 00:48:37,603
Speaker 1: It doesn't tell you that.

755
00:48:37,883 --> 00:48:46,660
Speaker 0: But I think a lot of people forget the fact that the only reason that documentation is so thorough is due to regulations required by law.

756
00:48:47,160 --> 00:48:47,381
Speaker 0: Yep.

757
00:48:47,781 --> 00:48:50,024
Speaker 0: Things that don't require that don't have that anymore.

758
00:48:50,085 --> 00:48:51,787
Speaker 0: So case in point, computers.

759
00:48:51,907 --> 00:49:00,800
Speaker 0: Before you could build your own computer, unless you were a turbo nerd, computers came with a giant manual, just like a car that went to about the same level of depth.

760
00:49:01,700 --> 00:49:06,780
Speaker 0: And no computer comes with anything even remotely like that anymore, nor do any computer components.

761
00:49:07,641 --> 00:49:16,120
Speaker 1: So when I was a kid, I had the stereo system and it was like, you know, it was all together unit, these two tall speakers, right, with several, you know, tweeters and woofers in them.

762
00:49:16,521 --> 00:49:21,259
Speaker 1: And then a main unit and the main unit on the bottom just had a storage area where you could put your vinyls.

763
00:49:21,440 --> 00:49:21,620
Speaker 1: Right.

764
00:49:21,921 --> 00:49:30,020
Speaker 1: And then above that was the system which had two cassette decks and a FM radio at the turntable at the top and then equalizer in the middle there.

765
00:49:30,360 --> 00:49:30,561
Speaker 1: Right.

766
00:49:30,961 --> 00:49:37,431
Speaker 1: And it came with a complete circuit diagram of the insides of itself.

767
00:49:38,192 --> 00:49:38,372
Speaker 1: Right.

768
00:49:38,512 --> 00:49:41,717
Speaker 1: On paper, like where every resistor, every everything.

769
00:49:42,258 --> 00:49:43,179
Speaker 1: You don't see that anymore.

770
00:49:43,420 --> 00:49:43,680
Speaker 0: Never.

771
00:49:45,000 --> 00:49:56,740
Speaker 0: And I really like there are consequences of this to this day, even now, like Biosis will not Biosis, but the modern equivalent of Biosis are so poorly documented that Uefi.

772
00:49:57,240 --> 00:50:01,930
Speaker 0: Yeah, the Uefi that there are options that I do not know what they do.

773
00:50:02,491 --> 00:50:05,757
Speaker 0: And if I look at a different Uefi, there is a different option.

774
00:50:05,777 --> 00:50:07,060
Speaker 0: that looks like it's the same option.

775
00:50:07,380 --> 00:50:10,906
Speaker 0: And if you read the documentation that does exist, it's basically meaningless.

776
00:50:11,266 --> 00:50:20,820
Speaker 0: And if you go to the Internet or like ask people or like look for consensus and you end up trusting some guy who may or may not have figured out the exact correct behavior.

777
00:50:21,600 --> 00:50:22,922
Speaker 1: A lot of the problem there is.

778
00:50:22,982 --> 00:50:29,613
Speaker 1: a lot of the people who make that software or the hardware for the Uefis are not English as a first language.

779
00:50:29,833 --> 00:50:34,220
Speaker 1: So the language that's written in that software is not complete.

780
00:50:35,401 --> 00:50:41,400
Speaker 0: Coupled with the fact that companies are very unwilling to hire formal, traditional, trained technical writers.

781
00:50:41,480 --> 00:50:42,079
Speaker 1: It's not going to hurt sales.

782
00:50:42,460 --> 00:50:50,390
Speaker 0: Nor are they willing to hire technical localizers and like the people who would translate documentation.

783
00:50:50,530 --> 00:50:50,930
Speaker 0: Exactly.

784
00:50:51,311 --> 00:50:56,818
Speaker 0: No one doesn't buy a motherboard because it didn't come with a 40 page manual that was written well.

785
00:50:56,858 --> 00:50:58,260
Speaker 0: They'll just figure it out.

786
00:50:59,080 --> 00:50:59,241
Speaker 1: Yep.

787
00:50:59,641 --> 00:50:59,941
Speaker 1: So, yeah.

788
00:50:59,961 --> 00:51:01,403
Speaker 1: So the user documentation.

789
00:51:01,443 --> 00:51:03,325
Speaker 1: now we've been talking about service documentation.

790
00:51:03,385 --> 00:51:09,232
Speaker 1: So service documentation is someone who's a user and or someone who's a fixer, right?

791
00:51:09,272 --> 00:51:11,194
Speaker 1: They're not, you know, they're going to.

792
00:51:11,214 --> 00:51:16,020
Speaker 1: their job is to fix the thing, you know, make it work, possibly even modify it.

793
00:51:17,040 --> 00:51:21,084
Speaker 1: But they have the things delivered to them, but they have to open it up and do something in the inside.

794
00:51:21,104 --> 00:51:29,931
Speaker 1: So now the documentation is sort of has this second purpose where it's not explaining the outer abstraction layer and how to interact with it.

795
00:51:30,392 --> 00:51:33,314
Speaker 1: It's explaining what is beneath the abstraction.

796
00:51:33,374 --> 00:51:34,535
Speaker 1: How does the thing work?

797
00:51:34,615 --> 00:51:35,336
Speaker 1: What does it do?

798
00:51:35,416 --> 00:51:36,216
Speaker 1: What are these parts?

799
00:51:36,577 --> 00:51:40,380
Speaker 1: Whereas the user documentation wouldn't even tell you what the parts work, right?

800
00:51:40,520 --> 00:51:41,782
Speaker 1: It's like you need to know what the parts were.

801
00:51:41,822 --> 00:51:44,685
Speaker 1: You just need to know how these buttons and switches work on the outside.

802
00:51:46,347 --> 00:51:56,680
Speaker 0: There's a big, big overlap in that space with a thing as a service platforms like software as a service platform as a service.

803
00:51:57,160 --> 00:52:09,250
Speaker 0: All a lot of the way modern companies interact with their customers where you have a product that you sell, but the product you sell is basically a Web site that you go to to do stuff or some APIs you interface with.

804
00:52:09,810 --> 00:52:15,275
Speaker 0: And there you have a service team that supports the people using your product.

805
00:52:15,735 --> 00:52:17,637
Speaker 0: They need documentation that explains the things.

806
00:52:17,657 --> 00:52:21,700
Speaker 0: you don't necessarily tell the people actually using your product specifically.

807
00:52:23,140 --> 00:52:23,721
Speaker 0: That's my world.

808
00:52:23,761 --> 00:52:27,023
Speaker 0: That's everything I've done for the last 15 years has been something as a service.

809
00:52:27,784 --> 00:52:33,348
Speaker 1: And then the final level is close to the final level, at least, is there.

810
00:52:33,788 --> 00:52:35,689
Speaker 1: You know, you're making something, right?

811
00:52:35,950 --> 00:52:37,751
Speaker 1: And you're making it with other people.

812
00:52:37,771 --> 00:52:38,672
Speaker 1: You didn't make it by yourself.

813
00:52:39,932 --> 00:52:43,155
Speaker 1: And you need to you know, all you're actually making this thing.

814
00:52:43,215 --> 00:52:50,140
Speaker 1: You need to have an even better level of understanding of the thing than people who have to fix the thing when it breaks.

815
00:52:50,500 --> 00:52:52,902
Speaker 0: So case in point, like I'll use an example from my company.

816
00:52:52,922 --> 00:52:54,063
Speaker 0: I just won't say any proper nouns.

817
00:52:54,444 --> 00:52:57,746
Speaker 0: We have a feature that's in a Web page that people use to do a thing.

818
00:52:58,207 --> 00:53:02,830
Speaker 0: So in that same Web page, we have an extensive user guide, like a fully written.

819
00:53:03,271 --> 00:53:05,052
Speaker 0: a team of tech writers wrote this thing.

820
00:53:05,353 --> 00:53:07,114
Speaker 0: It explains how the thing works.

821
00:53:07,374 --> 00:53:08,335
Speaker 0: Here's a table of shit.

822
00:53:08,455 --> 00:53:08,875
Speaker 0: Guess what?

823
00:53:09,116 --> 00:53:14,240
Speaker 0: We're actually going to tell you what each one of these fields means like full documentation, how to use the thing.

824
00:53:14,941 --> 00:53:18,404
Speaker 0: We have internally what we call a service run book or a service guide.

825
00:53:18,484 --> 00:53:19,685
Speaker 0: Companies call these different things.

826
00:53:20,185 --> 00:53:27,052
Speaker 0: That is the documentation that someone supporting a client or troubleshooting a problem would look at to understand.

827
00:53:27,212 --> 00:53:27,993
Speaker 0: Is this a problem?

828
00:53:28,253 --> 00:53:29,234
Speaker 0: How do I help the client?

829
00:53:29,594 --> 00:53:31,316
Speaker 0: Are there any outages I should know about?

830
00:53:31,696 --> 00:53:35,920
Speaker 0: How do I deal with situation X and I got locked or something weird like that?

831
00:53:37,200 --> 00:53:42,724
Speaker 0: But then what Scott described, this last one, the developers who made these things work on many different teams.

832
00:53:43,225 --> 00:53:44,526
Speaker 0: They move around between teams.

833
00:53:44,546 --> 00:53:45,787
Speaker 0: They move around between companies.

834
00:53:46,827 --> 00:53:57,816
Speaker 0: There are so many human beings that touched any feature that I'm describing that the developers have to have documentation that explains things like here's how it was architected.

835
00:53:58,216 --> 00:54:00,638
Speaker 0: Here is why we architected it this way.

836
00:54:01,178 --> 00:54:02,599
Speaker 0: This thing works weirdly.

837
00:54:02,639 --> 00:54:03,580
Speaker 0: We know it's weird.

838
00:54:03,960 --> 00:54:07,086
Speaker 0: Here's the engineering discussion around that.

839
00:54:07,447 --> 00:54:09,591
Speaker 0: And here's the link to the ticket where we all agreed.

840
00:54:09,631 --> 00:54:12,296
Speaker 0: someday we're going to re-architect this, but it's not worth it to do it yet.

841
00:54:13,097 --> 00:54:14,320
Speaker 0: All of those things need to exist.

842
00:54:15,480 --> 00:54:15,740
Speaker 1: Yeah.

843
00:54:15,820 --> 00:54:18,743
Speaker 1: If you had something like, I don't know, a tunnel, right?

844
00:54:18,763 --> 00:54:19,904
Speaker 1: We talked about tunnels today.

845
00:54:20,644 --> 00:54:22,866
Speaker 1: What's the user guide for the tunnel walkthrough?

846
00:54:23,126 --> 00:54:30,552
Speaker 0: User guide for the tunnel is literally the signs outside the tunnel telling you like, yeah, no propane tanks, no commercial vehicles.

847
00:54:30,832 --> 00:54:32,273
Speaker 1: Here's how to turn on your head.

848
00:54:32,313 --> 00:54:33,734
Speaker 1: Turn on your headlights just in case.

849
00:54:33,814 --> 00:54:34,074
Speaker 1: Yeah.

850
00:54:34,114 --> 00:54:35,395
Speaker 1: No, that kind of thing.

851
00:54:35,415 --> 00:54:35,616
Speaker 1: All right.

852
00:54:35,856 --> 00:54:36,336
Speaker 1: Say, all right.

853
00:54:36,376 --> 00:54:37,877
Speaker 1: The maintenance guide for the tunnel.

854
00:54:37,997 --> 00:54:40,699
Speaker 1: Wash the inside of the tunnel this way.

855
00:54:41,080 --> 00:54:41,340
Speaker 1: Right.

856
00:54:42,141 --> 00:54:43,862
Speaker 0: Every 10 years, get this kind of inspection.

857
00:54:43,902 --> 00:54:45,744
Speaker 0: Every 20 years, get this kind of inspection.

858
00:54:45,824 --> 00:54:48,147
Speaker 1: Every day, clean these vents in this way.

859
00:54:48,207 --> 00:54:51,730
Speaker 1: Here, change the filters in this way so people don't suffocate down there.

860
00:54:51,830 --> 00:54:52,391
Speaker 1: Right.

861
00:54:52,751 --> 00:54:54,193
Speaker 1: Here's how to change the light bulbs.

862
00:54:54,233 --> 00:55:00,339
Speaker 0: Here's the basic run book of like typical problems and like what is our process to deal with them or who to call.

863
00:55:00,819 --> 00:55:01,600
Speaker 0: Usually it's who to call.

864
00:55:01,920 --> 00:55:04,443
Speaker 1: If the tunnel is flooding, stop letting cars in.

865
00:55:04,503 --> 00:55:06,085
Speaker 1: Here's how to stop letting cars in.

866
00:55:06,265 --> 00:55:06,385
Speaker 1: Right.

867
00:55:06,746 --> 00:55:06,986
Speaker 1: Okay.

868
00:55:07,186 --> 00:55:12,552
Speaker 1: But then the other one is, okay, we dug this tunnel and you see this thing here.

869
00:55:12,612 --> 00:55:13,453
Speaker 1: that looks weird.

870
00:55:13,493 --> 00:55:17,418
Speaker 1: We put that there because there happens to be a rock there underground or something.

871
00:55:17,498 --> 00:55:19,220
Speaker 1: You know, I don't know anything about tunnels.

872
00:55:19,300 --> 00:55:27,966
Speaker 0: The lane is four inches narrower for these hundred feet because of and here's pages and pages and pages of architecture, diagrams and explanations.

873
00:55:28,106 --> 00:55:33,109
Speaker 1: So if you come and you have to deal with this tunnel long after I'm dead because the tunnel will outlive me.

874
00:55:33,770 --> 00:55:33,990
Speaker 1: Right.

875
00:55:34,090 --> 00:55:34,490
Speaker 1: Here is.

876
00:55:34,790 --> 00:55:39,033
Speaker 1: here is why we made this tunnel in this way where you come and you're an engineer and you know your stuff.

877
00:55:39,053 --> 00:55:42,796
Speaker 1: You literally do know your stuff and you look at the tunnel and something will look wrong to you.

878
00:55:43,196 --> 00:55:44,657
Speaker 1: This is why it looks wrong to you.

879
00:55:44,737 --> 00:55:45,498
Speaker 1: There was a reason.

880
00:55:45,578 --> 00:55:48,419
Speaker 1: Now it makes sense to you because you know your stuff because no one's reading this.

881
00:55:48,439 --> 00:55:49,220
Speaker 1: Who doesn't know their stuff?

882
00:55:49,220 --> 00:55:54,805
Speaker 0: So imagine the people who work the tunnel on a day to day basis need that second documentation like the run book on how to run it.

883
00:55:55,305 --> 00:55:59,469
Speaker 0: That will include a lot of scenarios.

884
00:55:59,709 --> 00:56:04,393
Speaker 0: and what is the standard operating procedure, which like, oh, there's a traffic jam.

885
00:56:04,413 --> 00:56:05,454
Speaker 0: Do you do anything or not?

886
00:56:05,774 --> 00:56:06,855
Speaker 0: I think it's flooding.

887
00:56:07,296 --> 00:56:12,100
Speaker 0: Here's the phone number to call to trigger like the emergency response, like that kind of detail.

888
00:56:12,641 --> 00:56:26,040
Speaker 0: But if there's a big problem, sometimes that run book will say, call these people and people are going to come in who have to decide what to do for a situation where there is no run book, like a crack appeared.

889
00:56:27,180 --> 00:56:27,481
Speaker 0: All right.

890
00:56:27,521 --> 00:56:28,562
Speaker 0: There's a giant crack.

891
00:56:28,842 --> 00:56:29,523
Speaker 0: What do we do?

892
00:56:29,964 --> 00:56:32,447
Speaker 0: There's no run book to cover every possible crack.

893
00:56:32,728 --> 00:56:41,820
Speaker 0: So now you need experts to read the the deep documentation that Scott described and then use their expertise to decide what to do.

894
00:56:42,220 --> 00:56:45,924
Speaker 0: Meaning the more information they have access to, the better.

895
00:56:45,944 --> 00:56:46,905
Speaker 0: Right.

896
00:56:47,005 --> 00:56:59,739
Speaker 1: If they if you just hire a tunnel expert to come and look at your tunnel crack and there's no instructions about that particular tunnel, you're going to be looking at a lot more cost, a lot more time, a lot more danger, a lot more everything that's bad and not a lot extra.

897
00:56:59,759 --> 00:57:00,140
Speaker 1: That's good.

898
00:57:00,340 --> 00:57:00,640
Speaker 1: Yeah.

899
00:57:00,660 --> 00:57:04,423
Speaker 1: Something will do anything for the person who made the tunnel.

900
00:57:04,823 --> 00:57:08,486
Speaker 1: That second person who comes along 100 years later is going to be helped out a lot.

901
00:57:08,746 --> 00:57:21,276
Speaker 0: So if you're making a documentation of any kind, especially get back into the software technology stuff most of the listeners know about, then it really comes down to who is the audience of your documentation.

902
00:57:21,376 --> 00:57:25,739
Speaker 0: But you can still break those audiences down usually to those three groups we just described.

903
00:57:26,199 --> 00:57:27,320
Speaker 0: have user.

904
00:57:27,340 --> 00:57:29,503
Speaker 1: There are other groups, but those are the three main ones.

905
00:57:29,563 --> 00:57:29,803
Speaker 1: Yeah.

906
00:57:30,084 --> 00:57:34,850
Speaker 0: But if you most companies, in my experience, don't even break it down to those three groups.

907
00:57:34,950 --> 00:57:41,519
Speaker 0: So most likely you'll be the hero in your company if you even get them to the point of having those three groups.

908
00:57:42,340 --> 00:57:42,500
Speaker 1: Yep.

909
00:57:43,021 --> 00:57:43,301
Speaker 1: All right.

910
00:57:43,621 --> 00:57:57,376
Speaker 1: So some other more specific things, right, is the question of, all right, you recognize the documentation is a good thing to have because you've engineered a technology of some kind, but it's not like it's free to make the documentation.

911
00:57:58,077 --> 00:58:01,320
Speaker 1: It takes time, effort, writing skills.

912
00:58:01,480 --> 00:58:05,123
Speaker 0: As a technologist, I also learned a lot about writing.

913
00:58:05,163 --> 00:58:06,404
Speaker 0: I'm a trained technical writer.

914
00:58:07,005 --> 00:58:12,250
Speaker 0: And the problem this presented in my younger career was I would document everything I did myself.

915
00:58:13,290 --> 00:58:13,651
Speaker 0: Always.

916
00:58:14,652 --> 00:58:16,293
Speaker 0: It took a huge amount of time.

917
00:58:16,834 --> 00:58:24,160
Speaker 0: I would spend about an hour writing documentation for every two to four hours of primary work I did.

918
00:58:24,761 --> 00:58:25,902
Speaker 0: That was just that was.

919
00:58:26,263 --> 00:58:28,165
Speaker 0: I assumed how everyone operated.

920
00:58:29,326 --> 00:58:33,231
Speaker 0: And then I got to a company where there wasn't any documentation of any kind because no one bothered.

921
00:58:33,712 --> 00:58:37,497
Speaker 0: And if I spent time writing documentation, I was admonished for wasting my time.

922
00:58:37,757 --> 00:58:38,558
Speaker 0: That's not my job.

923
00:58:38,598 --> 00:58:39,980
Speaker 0: That's the documentation team's job.

924
00:58:42,921 --> 00:59:00,540
Speaker 1: And the problem is that it's a fundamental problem in that a person who is most knowledgeable about the technology, right, is the person who could who has the skills to be building more technology, doing more engineering, and they may or may not have writing skills.

925
00:59:00,720 --> 00:59:07,933
Speaker 0: But even if they do have writing skills, the company does not want to pay a developer developer money to write documentation.

926
00:59:08,194 --> 00:59:11,640
Speaker 0: They want to pay documentation money, which usually is farmed out.

927
00:59:12,120 --> 00:59:21,572
Speaker 1: So even if you get an expert, someone who's a writer, as that person going to be, even if they're expert at writing, are they expert at this technology someone else made?

928
00:59:21,592 --> 00:59:23,535
Speaker 1: You're gonna have to have those people sit together and chat.

929
00:59:24,035 --> 00:59:25,777
Speaker 1: You know, it's like it's.

930
00:59:26,578 --> 00:59:27,800
Speaker 1: it's just a cost center.

931
00:59:27,860 --> 00:59:29,161
Speaker 1: And it's not.

932
00:59:29,221 --> 00:59:44,253
Speaker 1: it ends up not being worth it in capitalism unless it's a required by log, in which case, you know, you got to have it or be your technology is so big or so important that you're just you need to have the documentation.

933
00:59:44,353 --> 00:59:50,198
Speaker 1: Just practically speaking, it ends up being worth it to you money wise to have it because not having it would be disastrous.

934
00:59:50,498 --> 00:59:52,320
Speaker 0: That's more the world I live in today.

935
00:59:53,600 --> 01:00:02,334
Speaker 1: But so many places are building important stuff that a lot of things rely upon and documentation isn't written.

936
01:00:02,514 --> 01:00:06,280
Speaker 1: It isn't written well or it's not available or it's not available to the people.

937
01:00:07,040 --> 01:00:14,991
Speaker 0: We don't know the full story, but that card game with unique decks had a problem where no one seemed to know how their algorithm worked so they couldn't run it again.

938
01:00:15,011 --> 01:00:16,173
Speaker 1: Oh, yeah.

939
01:00:16,193 --> 01:00:16,754
Speaker 1: Key Forge.

940
01:00:16,974 --> 01:00:17,255
Speaker 0: Yeah.

941
01:00:18,016 --> 01:00:18,176
Speaker 1: Yeah.

942
01:00:18,196 --> 01:00:19,097
Speaker 1: We did an episode of that.

943
01:00:19,117 --> 01:00:20,199
Speaker 0: We talked about that on a Tuesday.

944
01:00:20,239 --> 01:00:21,200
Speaker 0: Go find the story.

945
01:00:21,320 --> 01:00:28,453
Speaker 1: The story seems to be that they had, you know, a system that had an algorithm to, you know, generate Key Forge decks and then run them through the printer.

946
01:00:29,295 --> 01:00:32,240
Speaker 1: And the people the rumor is they got ransomware.

947
01:00:33,100 --> 01:00:34,382
Speaker 1: So they lost all their software.

948
01:00:34,402 --> 01:00:35,864
Speaker 1: I guess they were really bad at software.

949
01:00:35,884 --> 01:00:37,606
Speaker 1: They didn't have it backed up anywhere or anything.

950
01:00:38,708 --> 01:00:44,636
Speaker 1: And now I guess with documentation, they could have at least had a new developer rewrite software.

951
01:00:44,656 --> 01:00:45,437
Speaker 1: that was equivalent.

952
01:00:46,038 --> 01:00:47,560
Speaker 1: But I guess they've just lost everything.

953
01:00:47,680 --> 01:00:48,301
Speaker 1: Some I don't know.

954
01:00:48,641 --> 01:00:51,265
Speaker 0: My advice to you out there is it really depends on the company.

955
01:00:51,365 --> 01:00:56,191
Speaker 1: But if you documentation is sort of a backup, if you think about it, I go one step further.

956
01:00:56,391 --> 01:01:03,240
Speaker 0: I treat documentation as a capital investment and an asset like having good documentation.

957
01:01:03,680 --> 01:01:04,962
Speaker 0: I consider it at least.

958
01:01:05,322 --> 01:01:10,987
Speaker 0: when I was in a C level position, which I am not now, I actually you know, I I went to a bigger company.

959
01:01:11,047 --> 01:01:13,089
Speaker 0: So I'm a smaller fish in a bigger company.

960
01:01:13,449 --> 01:01:24,960
Speaker 0: But when I was closer to like board of directors, executive work in a smaller company, I very much treated documentation as an asset that we would assign value to and high and treat as a priority.

961
01:01:25,300 --> 01:01:29,063
Speaker 0: That saved our asses so many times that it it.

962
01:01:29,744 --> 01:01:32,486
Speaker 0: I could say for a fact that it in the end made us money.

963
01:01:32,586 --> 01:01:34,747
Speaker 0: We profited by writing good documentation.

964
01:01:34,767 --> 01:01:36,068
Speaker 0: All right.

965
01:01:36,088 --> 01:01:39,971
Speaker 1: Let's talk about just end up here with some specific, real narrow things.

966
01:01:39,991 --> 01:01:41,633
Speaker 1: Something that always bothers me.

967
01:01:42,093 --> 01:01:44,315
Speaker 1: I read a lot of software developer documentation.

968
01:01:44,355 --> 01:01:44,975
Speaker 1: That's my job.

969
01:01:45,115 --> 01:01:45,335
Speaker 1: Right.

970
01:01:45,936 --> 01:01:51,500
Speaker 1: One thing that bothers me a lot is when the documentation is a not up to date.

971
01:01:51,560 --> 01:01:56,367
Speaker 1: You've changed the software since you wrote your documentation and you didn't update your documentation.

972
01:01:57,910 --> 01:02:01,415
Speaker 0: I look at the documentation and list five fields to put in the JSON.

973
01:02:01,695 --> 01:02:02,837
Speaker 0: I look in the example code.

974
01:02:03,218 --> 01:02:04,460
Speaker 0: There's six fields in there.

975
01:02:05,781 --> 01:02:08,326
Speaker 0: I copy paste that six field into the document.

976
01:02:08,366 --> 01:02:09,849
Speaker 0: It does not show up even once.

977
01:02:10,971 --> 01:02:11,192
Speaker 1: Yep.

978
01:02:11,352 --> 01:02:14,959
Speaker 1: It's a broken documentation is actually more frustrating than none, I think.

979
01:02:15,119 --> 01:02:17,282
Speaker 0: Oh, I. This is just me.

980
01:02:17,783 --> 01:02:23,710
Speaker 0: I treated software documentation like software issues and documentation is used as bugs.

981
01:02:23,830 --> 01:02:31,760
Speaker 0: If someone found a flaw in the documentation, there was a bug ticket, just like if there is a bug in the software and we treated it with the same level of severity.

982
01:02:32,520 --> 01:02:38,487
Speaker 1: Something else I really, really don't like is someone will write some software documentation and they'll give you an example.

983
01:02:38,607 --> 01:02:39,888
Speaker 1: First of all, you got to have examples.

984
01:02:39,948 --> 01:02:41,510
Speaker 1: I hate the ones where there's no example.

985
01:02:41,871 --> 01:02:42,051
Speaker 1: Right.

986
01:02:42,091 --> 01:02:46,876
Speaker 1: Don't just tell me what like, you know, oh, there's a function called this and describing what it does.

987
01:02:47,036 --> 01:02:50,360
Speaker 1: It's like show me an actual example of it in use.

988
01:02:50,720 --> 01:02:50,920
Speaker 1: Yeah.

989
01:02:51,020 --> 01:02:54,843
Speaker 0: So that's the first thing, because what I'm going to do, I'm going to look at the example first thoughts with it.

990
01:02:54,863 --> 01:02:56,584
Speaker 0: Then I'm gonna go back and read the rest of the documentation.

991
01:02:57,205 --> 01:02:57,665
Speaker 1: Exactly.

992
01:02:57,705 --> 01:02:58,926
Speaker 1: So you need to have examples.

993
01:02:59,046 --> 01:03:03,449
Speaker 1: But something that bothers me above that is incomplete examples.

994
01:03:03,529 --> 01:03:07,672
Speaker 1: For example, someone might have an example of how to use a function.

995
01:03:07,692 --> 01:03:08,832
Speaker 1: Right.

996
01:03:08,973 --> 01:03:09,793
Speaker 1: And they'll have.

997
01:03:09,813 --> 01:03:11,134
Speaker 1: they'll be calling the function.

998
01:03:11,374 --> 01:03:13,936
Speaker 1: So function parentheses, parameter, parameter.

999
01:03:14,596 --> 01:03:17,398
Speaker 1: And they'll say, yeah, this will return X. Right.

1000
01:03:17,458 --> 01:03:19,219
Speaker 1: With these inputs, it's like, OK, I see it.

1001
01:03:19,780 --> 01:03:19,920
Speaker 1: But.

1002
01:03:20,540 --> 01:03:23,402
Speaker 1: You didn't include where to import the function from.

1003
01:03:23,962 --> 01:03:25,984
Speaker 1: It could be like from library import function.

1004
01:03:26,004 --> 01:03:28,766
Speaker 1: You need to put that import statement in the example.

1005
01:03:29,106 --> 01:03:30,887
Speaker 1: Otherwise, I'm like, OK, I see the function.

1006
01:03:31,187 --> 01:03:32,208
Speaker 1: where the fuck do I import.

1007
01:03:32,228 --> 01:03:32,728
Speaker 0: Oh, my God.

1008
01:03:32,768 --> 01:03:36,631
Speaker 0: So fixed protocol documentation, like a specification for a fixed API.

1009
01:03:37,111 --> 01:03:37,612
Speaker 0: Same thing.

1010
01:03:37,872 --> 01:03:44,476
Speaker 0: They will give you the most excruciating examples possible of the payloads, like full interactions.

1011
01:03:45,037 --> 01:03:45,437
Speaker 0: So cool.

1012
01:03:45,457 --> 01:03:46,137
Speaker 0: You've got all that.

1013
01:03:46,598 --> 01:03:50,100
Speaker 0: They'll often leave out the headers and footers of those messages.

1014
01:03:50,500 --> 01:03:52,301
Speaker 0: Meaning you're looking at a partial message.

1015
01:03:52,562 --> 01:03:53,943
Speaker 0: Now, I'm a I'm a fixed expert.

1016
01:03:54,043 --> 01:03:54,583
Speaker 0: I look at this.

1017
01:03:55,123 --> 01:03:56,764
Speaker 0: I can infer what the headers should look like.

1018
01:03:56,824 --> 01:03:57,645
Speaker 0: Doesn't bother me at all.

1019
01:03:58,185 --> 01:04:05,090
Speaker 0: Ninety nine percent of people who read that documentation are going to send that raw message without the header because the example didn't have it.

1020
01:04:05,490 --> 01:04:11,995
Speaker 0: It's going to fail in ways they don't understand because the fixed protocol breaks down badly without those message, without those headers.

1021
01:04:12,355 --> 01:04:13,876
Speaker 0: And they're not going to understand what happened.

1022
01:04:14,616 --> 01:04:16,137
Speaker 1: So you should whatever you make put it.

1023
01:04:16,157 --> 01:04:17,879
Speaker 1: you should put an example on everything.

1024
01:04:17,959 --> 01:04:19,880
Speaker 0: And also, if you example seems trivial.

1025
01:04:20,200 --> 01:04:27,426
Speaker 1: To you and all of your examples should be one hundred percent complete examples showing you everything is happening.

1026
01:04:27,566 --> 01:04:30,028
Speaker 0: Don't know if you're a fixed message.

1027
01:04:30,108 --> 01:04:30,328
Speaker 0: Right.

1028
01:04:30,648 --> 01:04:46,120
Speaker 1: So now, even though in this section of the documentation, only one part of that complete example is like the relevant part to what you're discussing, include all the parts and then just put the relevant part in bold so that they can see that's the part that's being discussed here.

1029
01:04:46,440 --> 01:04:51,886
Speaker 1: But I can see it in the full context of the full interaction that is happening.

1030
01:04:52,347 --> 01:04:54,789
Speaker 1: And now when I go to use that, I will be successful.

1031
01:04:55,670 --> 01:05:01,036
Speaker 1: If you just give me only that bold part and you leave out everything else, it is not even there that I'm going to try.

1032
01:05:01,056 --> 01:05:01,837
Speaker 1: It's not going to work.

1033
01:05:01,897 --> 01:05:03,879
Speaker 1: And I had to go and figure out everything else.

1034
01:05:03,979 --> 01:05:04,780
Speaker 1: So that's annoying.

1035
01:05:06,500 --> 01:05:12,945
Speaker 0: In terms of user documentation, like not just developer API documentation, I think it bothers me the most.

1036
01:05:13,005 --> 01:05:19,189
Speaker 0: The two things that bother me the most are one, when instead of having a written document, they have a YouTube video like fuck that.

1037
01:05:19,550 --> 01:05:21,051
Speaker 0: That is the least like.

1038
01:05:21,091 --> 01:05:22,872
Speaker 0: videos are good accompaniments.

1039
01:05:23,132 --> 01:05:25,574
Speaker 0: They're good accessories to documentation.

1040
01:05:25,614 --> 01:05:28,656
Speaker 0: They are fantastic ways to augment documentation.

1041
01:05:28,956 --> 01:05:30,798
Speaker 0: They do not replace documentation.

1042
01:05:30,858 --> 01:05:34,300
Speaker 0: Video is a very information sparse medium.

1043
01:05:34,900 --> 01:05:37,883
Speaker 0: That is very difficult to search and scan through.

1044
01:05:38,283 --> 01:05:40,024
Speaker 0: It is very difficult to take things from it.

1045
01:05:40,305 --> 01:05:44,108
Speaker 0: I can't copy paste code out of YouTube video and put it in my editor while I'm.

1046
01:05:44,148 --> 01:05:58,760
Speaker 0: videos are great when you need to teach someone a physical thing because you can show the thing happening, but they still should not replace written documentation that provides more depth, more context and other information.

1047
01:05:58,880 --> 01:06:03,724
Speaker 1: Well, I mean, if you were like how for something like how to assemble an Ikea furniture, right?

1048
01:06:03,804 --> 01:06:12,351
Speaker 1: A video could end up being better than the because the diagram might be confusing, but the video would be like unambiguous to see the pieces going to get.

1049
01:06:12,371 --> 01:06:14,833
Speaker 0: But I don't want by you without the document and vice versa.

1050
01:06:15,233 --> 01:06:24,240
Speaker 1: In that case, I mean, I definitely still want the document, but I'm just saying in some scenarios, as long as the video is accessible to you, video could be the preferable thing.

1051
01:06:24,360 --> 01:06:31,745
Speaker 1: I like if I had the video and the paper, I'd go to the video first, even though it's slower, because for that kind of thing, it's a better.

1052
01:06:32,085 --> 01:06:33,406
Speaker 1: you know, it's better.

1053
01:06:33,626 --> 01:06:37,649
Speaker 0: Though they get to the first for most things, video is not better.

1054
01:06:37,989 --> 01:06:49,757
Speaker 1: Well, even any information thing that's not a physical thing, video is not better or anything that requires is not as good or anything that requires a physical learned skill, like for like, for example, skiing.

1055
01:06:50,037 --> 01:06:54,000
Speaker 0: There's documentation on skiing, like the right way to do specific kinds of turns.

1056
01:06:54,501 --> 01:06:58,680
Speaker 0: That text documentation is very helpful to understand the mechanics involved.

1057
01:06:59,460 --> 01:07:03,080
Speaker 0: But a video is also very valuable because it shows it in action.

1058
01:07:03,300 --> 01:07:07,772
Speaker 0: You see what the result of those motions should be, but you also need.

1059
01:07:08,053 --> 01:07:10,460
Speaker 1: you can also see the person that you're going to be trying to mimic.

1060
01:07:10,600 --> 01:07:20,840
Speaker 0: Right now, it's hard to mimic a drawing of a person, but just trying to mimic the person without the explanation of the mechanics involved is a great way to hurt yourself badly.

1061
01:07:23,481 --> 01:07:37,079
Speaker 0: The other thing that really bothers me is when user documentation goes overboard and documents all the things like it goes, it's dense and documents things that that are honestly 100 percent self-evident.

1062
01:07:38,300 --> 01:07:44,760
Speaker 0: I've seen so many user guides, especially in more commercial software that explain this is a table.

1063
01:07:45,160 --> 01:07:47,920
Speaker 0: A table is a way to see information from our system.

1064
01:07:48,361 --> 01:07:58,540
Speaker 0: You can sort this table by clicking on the top with these error and it'll spend paragraphs explaining how to use a table every time a table appears in the documentation.

1065
01:07:59,060 --> 01:08:00,619
Speaker 1: You don't do it every time a table appears.

1066
01:08:00,761 --> 01:08:02,319
Speaker 1: You do it once in one place.

1067
01:08:03,382 --> 01:08:07,340
Speaker 1: And as long as you organize the documentation, like it's not bad to have those things.

1068
01:08:07,540 --> 01:08:12,840
Speaker 1: Yeah, it's just as long as the documentation is organized to put those things in their own area.

1069
01:08:12,960 --> 01:08:19,319
Speaker 1: So it's like, you know, oh, yes, in this, you know, you go to look up, maybe you're going the index and you're like, all right, how do I do X?

1070
01:08:19,600 --> 01:08:21,660
Speaker 1: You find the section about X. It involves a table.

1071
01:08:22,241 --> 01:08:27,540
Speaker 1: And if you don't understand the table, there'll be a reference like see appendix X, how to use table like you have a section like that is okay.

1072
01:08:27,901 --> 01:08:29,880
Speaker 0: That's like how we use tables.

1073
01:08:30,180 --> 01:08:36,960
Speaker 0: So in all my things, whatever it is I'm selling to you, here is the documentation of just in case you needed it, how tables work.

1074
01:08:37,401 --> 01:08:46,700
Speaker 0: I find the main reason a lot of user guides do this is, one, they hire poorly trained technical writers who are trying to fill space.

1075
01:08:47,581 --> 01:08:48,198
Speaker 0: I see that a lot.

1076
01:08:48,740 --> 01:08:52,067
Speaker 0: And two, you have people where they're documenting it.

1077
01:08:52,147 --> 01:08:58,300
Speaker 0: in every place tables appear because the behavior of the thing is inconsistent, which is a more fundamental problem.

1078
01:08:58,600 --> 01:09:05,880
Speaker 0: If the thing behave more consistently, you could make your documentation way simpler and put all the table documentation in one spot.

1079
01:09:06,341 --> 01:09:10,200
Speaker 1: You also got to consider, even if you get someone to write documentation, are you hiring editors?

1080
01:09:10,640 --> 01:09:11,420
Speaker 1: Are you hiring review?

1081
01:09:11,760 --> 01:09:12,939
Speaker 1: People review the documentation.

1082
01:09:13,340 --> 01:09:19,578
Speaker 0: It's like, you know, I have someone in engineering read the documentation that came out to make sure it's not wrong now.

1083
01:09:20,720 --> 01:09:20,819
Speaker 1: Yeah.

1084
01:09:21,062 --> 01:09:22,138
Speaker 1: So much stuff is going on.

1085
01:09:22,520 --> 01:09:24,459
Speaker 0: But here's how I handle that.

1086
01:09:24,862 --> 01:09:26,620
Speaker 0: We literally like we have a software release.

1087
01:09:26,845 --> 01:09:27,160
Speaker 0: All right.

1088
01:09:27,340 --> 01:09:30,551
Speaker 0: We have tickets and all the stuff to track that all the way out to go to market.

1089
01:09:30,854 --> 01:09:32,520
Speaker 0: QA, UAT, go live.

1090
01:09:32,761 --> 01:09:36,691
Speaker 0: The user guide goes through QAT, UAT and go live.

1091
01:09:36,971 --> 01:09:39,960
Speaker 0: the service run book, QA, UAT, go live.

1092
01:09:41,264 --> 01:09:43,100
Speaker 0: We don't do that with the engineering documentation.

1093
01:09:43,720 --> 01:09:50,660
Speaker 0: I trust the engineers to document their own stuff because if they don't document their own stuff, well, that's their problem when it's undocumented.

1094
01:09:51,901 --> 01:09:58,060
Speaker 1: Well, the thing is, right, is that's often a problem in that you don't as an individual, right?

1095
01:09:58,200 --> 01:10:00,400
Speaker 1: It's not your problem because you understand what you did.

1096
01:10:00,480 --> 01:10:02,260
Speaker 1: It's only you just only cause problems.

1097
01:10:02,500 --> 01:10:03,615
Speaker 0: I guess I'm not an engineer anymore.

1098
01:10:04,202 --> 01:10:06,180
Speaker 0: So I'm speaking from like a product perspective.

1099
01:10:06,320 --> 01:10:10,380
Speaker 1: If an engineer doesn't document for the other engineers, they're only causing other people's problems.

1100
01:10:10,440 --> 01:10:11,560
Speaker 1: It never caused them a problem.

1101
01:10:12,041 --> 01:10:15,680
Speaker 1: Of course, someone else not writing documentation for them causes them a problem.

1102
01:10:16,082 --> 01:10:17,020
Speaker 0: But I'll give you an example.

1103
01:10:17,345 --> 01:10:18,320
Speaker 0: There is a pipeline.

1104
01:10:18,460 --> 01:10:19,199
Speaker 0: This is my current job.

1105
01:10:19,300 --> 01:10:26,360
Speaker 0: There is a pipeline that pulls metrics out of some runtime production systems and via multiple engineering teams.

1106
01:10:26,380 --> 01:10:29,796
Speaker 0: This code makes its way into an analytics platform that then does stuff.

1107
01:10:31,941 --> 01:10:35,580
Speaker 0: Nobody works at the company who designed that pipeline originally.

1108
01:10:36,041 --> 01:10:39,940
Speaker 0: So I wanted to build a thing that used the data at the end of the pipeline.

1109
01:10:40,300 --> 01:10:43,780
Speaker 0: So I have a very clear specification on how I want this thing to work.

1110
01:10:45,241 --> 01:10:49,300
Speaker 0: No engineering team would take ownership of it because the pipeline crossed so many teams.

1111
01:10:50,321 --> 01:10:55,459
Speaker 0: So they assumed things worked the way it worked and they made a thing and it didn't do what they expected at all.

1112
01:10:56,680 --> 01:11:00,340
Speaker 0: Nobody, every engineering team one by one said, oh, well, we didn't write that part.

1113
01:11:00,741 --> 01:11:09,920
Speaker 0: And they passed the buck to the next engineering team down the line until eventually they discovered that at no point had anyone documented how that thing worked.

1114
01:11:10,621 --> 01:11:10,821
Speaker 0: Ever.

1115
01:11:11,223 --> 01:11:16,560
Speaker 0: It was never documented by anyone because everyone in this chain thought this is obvious.

1116
01:11:17,340 --> 01:11:26,160
Speaker 0: Everyone I work with knows how it works and I won't even be here anymore 10 years from now when some jerk like rim tries to change something about this pipeline.

1117
01:11:26,560 --> 01:11:34,080
Speaker 0: So one engineer who didn't write documentation like seven years ago fucked over like 20 other engineers and me.

1118
01:11:35,261 --> 01:11:35,421
Speaker 1: Yep.

1119
01:11:36,704 --> 01:11:42,999
Speaker 1: So I guess that the most relevant thing, I guess, just to consumers is that, you know, this all ties in.

1120
01:11:43,381 --> 01:11:47,540
Speaker 1: You know, we've talked about it mostly from like you're at work, you're an engineer or stuff like that.

1121
01:11:47,640 --> 01:11:53,240
Speaker 1: But for consumers, the big deal is like all right to repair, you know, kind of stuff.

1122
01:11:53,643 --> 01:11:55,800
Speaker 1: It's like, you know, things not being documented.

1123
01:11:56,162 --> 01:11:59,680
Speaker 1: You buy a product and you can't make it do what you want, even though you own it.

1124
01:11:59,840 --> 01:12:02,860
Speaker 0: That RTMP streaming box I bought, that just didn't work at all.

1125
01:12:03,521 --> 01:12:07,560
Speaker 1: You buy a smart TV, it's spying on you, but they didn't document how you can put your own.

1126
01:12:08,340 --> 01:12:12,600
Speaker 1: It's technically possible to completely replace the software on that smart TV with some other.

1127
01:12:12,660 --> 01:12:13,450
Speaker 1: It's running Android.

1128
01:12:13,612 --> 01:12:14,220
Speaker 1: So a lot of them.

1129
01:12:14,380 --> 01:12:14,480
Speaker 1: Right.

1130
01:12:15,163 --> 01:12:20,480
Speaker 1: There's nothing stopping you from installing some other software on there other than it's not documented how to interface with it.

1131
01:12:20,600 --> 01:12:20,781
Speaker 1: Right.

1132
01:12:20,801 --> 01:12:25,080
Speaker 1: If there was full documentation, you could open that shit up, flash it, go to town.

1133
01:12:25,681 --> 01:12:32,800
Speaker 1: So, you know, but we only get that we're only lucky in some cases, like the famous, you know, Linksys WRT router.

1134
01:12:32,901 --> 01:12:33,202
Speaker 1: Right.

1135
01:12:33,222 --> 01:12:36,380
Speaker 1: You could put your own, you know, router software on there.

1136
01:12:36,400 --> 01:12:36,662
Speaker 1: Right.

1137
01:12:37,104 --> 01:12:39,580
Speaker 1: That's the GPL forced them to reveal or whatnot.

1138
01:12:39,680 --> 01:12:43,059
Speaker 1: So that's, you know, nerds out there.

1139
01:12:43,320 --> 01:12:43,481
Speaker 1: Right.

1140
01:12:43,561 --> 01:12:43,882
Speaker 1: Lack.

1141
01:12:43,982 --> 01:12:59,660
Speaker 0: If we could force documentation upon all consumer, you know, especially electronic, but all, you know, but you pick a line and above that line, you require like there's a bunch of tiers of above these lines and set these lines based on common sense and smartness.

1142
01:12:59,740 --> 01:13:02,819
Speaker 1: The whole John Deere tractor right to repair business going on.

1143
01:13:03,080 --> 01:13:03,180
Speaker 1: Right.

1144
01:13:03,281 --> 01:13:09,700
Speaker 1: If they were required to include with all the tractors, a complete, you know, service manual that showed everything in the tractor and how it works.

1145
01:13:09,840 --> 01:13:10,643
Speaker 1: Now it's put together.

1146
01:13:10,723 --> 01:13:14,840
Speaker 1: It's like, well, then you couldn't really prevent farmers from fixing their own tractor, could you?

1147
01:13:14,980 --> 01:13:16,458
Speaker 1: Because they see it right fucking there.

1148
01:13:18,405 --> 01:13:18,586
Speaker 0: Yep.

1149
01:13:18,667 --> 01:13:20,500
Speaker 0: And I could and I think we've gone on long enough.

1150
01:13:20,640 --> 01:13:26,660
Speaker 0: Maybe we'll do a separate episode on technical writing in the future because we didn't even touch on what good documentation looks like in detail.

1151
01:13:26,760 --> 01:13:27,649
Speaker 0: But what I'll link, we did.

1152
01:13:27,689 --> 01:13:28,740
Speaker 1: It looks like good examples.

1153
01:13:29,002 --> 01:13:29,870
Speaker 1: You cover everything.

1154
01:13:29,890 --> 01:13:30,879
Speaker 1: You don't leave anything out.

1155
01:13:31,102 --> 01:13:32,100
Speaker 0: I guess I could go a lot deeper.

1156
01:13:32,420 --> 01:13:34,620
Speaker 1: You know, whatever language it is, you explain it clearly.

1157
01:13:34,981 --> 01:13:38,120
Speaker 0: I guess as a technical writer, I could go a lot deeper into a lot of specifics.

1158
01:13:38,300 --> 01:13:49,340
Speaker 0: But what I'm going to link to is a panel, a lecture that me and Scott gave years ago called Designing Game Rules, where we talked about game board game rules as documentation.

1159
01:13:50,121 --> 01:13:52,600
Speaker 1: And obviously rules are just documentation for a game.

1160
01:13:53,040 --> 01:13:58,020
Speaker 0: And we covered in extensive detail what makes good documentation in that panel.

1161
01:13:58,321 --> 01:14:01,660
Speaker 0: You could take that panel and apply it to your software engineering job.

1162
01:14:07,504 --> 01:14:09,640
Speaker 0: This has been Geek Nights with Rym and Scott.

1163
01:14:09,780 --> 01:14:14,840
Speaker 0: Special thanks to DJ Pretzel for the opening music, Kat Lee for web design and Brando K for the logos.

1164
01:14:15,140 --> 01:14:20,180
Speaker 1: Be sure to visit our website at FrontRowCrew.com for show notes, discussion news and more.

1165
01:14:20,500 --> 01:14:23,120
Speaker 0: Remember, Geek Nights is not one, but four different shows.

1166
01:14:23,380 --> 01:14:27,860
Speaker 0: Sci-Tech Mondays, Gaming Tuesdays, Anime Comic Wednesdays and Indiscriminate Thursdays.

1167
01:14:28,240 --> 01:14:31,376
Speaker 1: Geek Nights is distributed under a Creative Commons Attribution 3.0 license.

1168
01:14:32,680 --> 01:14:35,660
Speaker 1: Geek Nights is recorded live with no studio and no audience.

1169
01:14:35,920 --> 01:14:38,800
Speaker 1: But unlike those other late shows, it's actually recorded at night.

1170
01:14:39,322 --> 01:15:04,140
Speaker 0: And the Patreon patrons for this episode of Geek Nights are Heidi McNicholallen, Joyce Linkeji, Dread Lily, Tanner Bryant, a bunch of people who don't want me to say their names, Chris Rahmer, Clinton Walton, Dex Finn, JustLikeADudeGuy, Shai Aye 85, Rebecca Dunn, ReviewMadBull34, Cowards, Ryan Perrin, Sam Erickson, Shervin Von Hurl, Taylor Brown, The Iron Front, You Hold the Key to My Heart and a whole collection of individuals who do not want me to say their names.

1171
01:15:04,961 --> 01:15:06,159
Speaker 0: Uh, yeah, show went a little long.

1172
01:15:06,581 --> 01:15:11,040
Speaker 0: Uh, Scott and I sometimes just get going because it's been a while since we, uh, hung out.

1173
01:15:11,320 --> 01:15:14,800
Speaker 0: So, uh, Geek Nights, uh, we found the episodes are getting longer.

1174
01:15:14,800 --> 01:15:19,040
Speaker 0: We're actually having to cut ourselves off, even though we don't have great ideas for Monday and Thursday shows lately.

1175
01:15:19,942 --> 01:15:22,780
Speaker 0: Uh, but anyway, for right now, I'm too hungry to do anything else.

1176
01:15:22,861 --> 01:15:24,619
Speaker 0: So, uh, I just simply leave you with...

1177
01:15:24,619 --> 01:15:30,801
Speaker 1: ♪ Have yourself a merry little Christmas ♪ ♪.

1178
01:15:30,801 --> 01:15:35,060
Speaker 1: Let your heart be light ♪ ♪.

1179
01:15:35,060 --> 01:15:44,339
Speaker 1: From now on, our troubles will be out of sight.

1180
01:15:44,339 --> 01:15:50,379
Speaker 1: ♪ ♪ Have yourself a merry little Christmas.

1181
01:15:50,379 --> 01:15:50,901
Speaker 1: ♪ ♪.

1182
01:15:50,901 --> 01:15:54,718
Speaker 0: Make the yuletide gay.

1183
01:15:54,718 --> 01:16:00,095
Speaker 1: ♪ ♪ From now on, our troubles will be miles away.

1184
01:16:00,095 --> 01:16:04,757
Speaker 1: ♪ ♪ Miles away.

1185
01:16:04,757 --> 01:16:05,541
Speaker 1: ♪ ♪.

1186
01:16:05,541 --> 01:16:09,579
Speaker 0: Here we arise in olden days.

1187
01:16:09,579 --> 01:16:14,918
Speaker 0: ♪ ♪ Happy golden days of yore.

1188
01:16:14,918 --> 01:16:19,599
Speaker 0: ♪ ♪ Faithful friends who are dear to us.

1189
01:16:19,599 --> 01:16:20,000
Speaker 1: ♪ ♪.

1190
01:16:20,000 --> 01:16:25,018
Speaker 0: Gather near to us once more.

1191
01:16:25,018 --> 01:16:25,660
Speaker 1: ♪ ♪.

1192
01:16:25,660 --> 01:16:30,576
Speaker 1: Through the years we all will be together.

1193
01:16:30,576 --> 01:16:35,178
Speaker 1: ♪ ♪ If it's allowed ♪.

