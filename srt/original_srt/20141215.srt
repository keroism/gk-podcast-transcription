1
00:00:08,620 --> 00:00:13,013
Speaker 0: It's Monday December 15th, 2014.

2
00:00:13,013 --> 00:00:13,474
Speaker 0: I'm Rem.

3
00:00:13,754 --> 00:00:14,296
Speaker 1: I'm Scott.

4
00:00:14,396 --> 00:00:15,840
Speaker 0: And this is Geek Nights Tonight.

5
00:00:16,340 --> 00:00:20,338
Speaker 0: Strong artificial intelligence and why it's probably going to destroy us.

6
00:00:24,750 --> 00:00:25,748
Speaker 0: Let's do this.

7
00:00:27,912 --> 00:00:28,274
Speaker 0: And why?

8
00:00:28,334 --> 00:00:31,170
Speaker 0: if you have a better idea for a Monday show, you should email us.

9
00:00:31,350 --> 00:00:34,170
Speaker 0: Geeknights@frontrowcrew.com or post it on our forum.

10
00:00:34,952 --> 00:00:39,030
Speaker 1: If you posted an idea and we haven't used it, it's because your idea was bad.

11
00:00:39,473 --> 00:00:41,989
Speaker 0: Or we just forgot, so post it again.

12
00:00:42,530 --> 00:00:45,790
Speaker 1: But you'd be surprised how hard it is to come with a valid idea.

13
00:00:45,990 --> 00:00:49,510
Speaker 0: Well, I feel like really what we need to do is when people suggest an idea.

14
00:00:49,831 --> 00:00:52,549
Speaker 1: We should tell them why we can't do it, like Kineticon feedback panel.

15
00:00:52,690 --> 00:00:56,169
Speaker 0: We should do the show about it to show them why.

16
00:00:58,074 --> 00:00:59,990
Speaker 0: Well, I think it comes down to some of these topics.

17
00:01:00,573 --> 00:01:02,030
Speaker 1: I don't want to make a bad show, though.

18
00:01:02,130 --> 00:01:05,550
Speaker 0: We've technically already talked about them in enough depth to where that should be fine.

19
00:01:05,730 --> 00:01:06,510
Speaker 1: Like a news or something.

20
00:01:06,590 --> 00:01:13,050
Speaker 0: Yeah, people don't search or they're things that would require research, which we don't got time for that.

21
00:01:13,310 --> 00:01:17,390
Speaker 0: If we're going to do research and do like a for real thing about something we don't know.

22
00:01:17,690 --> 00:01:20,490
Speaker 1: That's just us reading Internet and then saying what we saw on the Internet.

23
00:01:20,731 --> 00:01:22,530
Speaker 1: And also you can just read the Internet yourself.

24
00:01:22,691 --> 00:01:23,970
Speaker 0: Yeah, that's not going to be geek nights.

25
00:01:24,030 --> 00:01:27,970
Speaker 0: That would be like a YouTube video like how to set up NTP with RIM.

26
00:01:28,670 --> 00:01:32,029
Speaker 1: How to read the Wikipedia page on this topic out loud.

27
00:01:32,311 --> 00:01:37,010
Speaker 0: Yeah, or possibly synthesizing the Wikipedia with one or two technical documents.

28
00:01:39,051 --> 00:01:43,990
Speaker 0: Or it's a topic that we just think is stupid or boring and we don't have a lot to say.

29
00:01:44,171 --> 00:01:47,930
Speaker 1: Yeah, there's like nothing to say about it or it doesn't fit or et cetera, et cetera.

30
00:01:48,251 --> 00:01:51,290
Speaker 0: I feel like we could do a lot more Monday shows if we started focusing more on.

31
00:01:51,490 --> 00:01:56,130
Speaker 1: Also, a lot of the people things people suggest are overly broad and not nearly specific enough.

32
00:01:57,655 --> 00:02:02,630
Speaker 1: Or they're so specific that there isn't 30 minutes of stuff to say about them.

33
00:02:03,050 --> 00:02:03,312
Speaker 0: Exactly.

34
00:02:03,332 --> 00:02:05,530
Speaker 0: In which case we should just do a tech news round.

35
00:02:06,561 --> 00:02:06,730
Speaker 0: Yeah.

36
00:02:07,091 --> 00:02:07,334
Speaker 0: Yep.

37
00:02:07,738 --> 00:02:08,949
Speaker 0: Actually, we should do that more often.

38
00:02:09,090 --> 00:02:09,412
Speaker 0: We could.

39
00:02:09,453 --> 00:02:11,830
Speaker 0: we could do the show every week just looking at whatever's on the front page.

40
00:02:12,150 --> 00:02:13,379
Speaker 1: But then why even make this show?

41
00:02:13,399 --> 00:02:14,930
Speaker 1: Because there are 100 podcasts that do that.

42
00:02:15,211 --> 00:02:15,432
Speaker 0: Yeah.

43
00:02:15,452 --> 00:02:18,749
Speaker 0: And the awesome thing is, like I've tried to listen to a lot of those podcasts.

44
00:02:19,410 --> 00:02:21,850
Speaker 0: Those people don't know shit and those podcasts are really bad.

45
00:02:22,512 --> 00:02:28,690
Speaker 0: We could do it better, but that's like being better at a sport that you're good at, but you don't like.

46
00:02:29,890 --> 00:02:35,864
Speaker 0: Like making really boring and simple analogies and similes that do exactly like they're.

47
00:02:35,884 --> 00:02:38,730
Speaker 0: so they're so functional and pragmatic.

48
00:02:38,950 --> 00:02:39,388
Speaker 1: What's your news?

49
00:02:40,290 --> 00:02:42,435
Speaker 0: So in the news, I got two things.

50
00:02:42,495 --> 00:02:49,349
Speaker 0: I got some science and I got some technology being misunderstood and used inappropriately by people who should know better.

51
00:02:50,231 --> 00:02:58,630
Speaker 0: So in science news, evolution is winning our war on HIV and AIDS for us.

52
00:02:58,970 --> 00:02:59,285
Speaker 1: That's good.

53
00:02:59,870 --> 00:03:01,530
Speaker 0: So something awesome is happening.

54
00:03:01,850 --> 00:03:06,163
Speaker 1: Well, I mean, HIV itself has the same evolution powers that we do.

55
00:03:06,223 --> 00:03:08,750
Speaker 1: In fact, it should be even faster at evolution than people.

56
00:03:09,130 --> 00:03:09,474
Speaker 0: It is.

57
00:03:09,656 --> 00:03:10,626
Speaker 0: And this is what's happening.

58
00:03:10,970 --> 00:03:12,489
Speaker 1: So shouldn't it be winning against us?

59
00:03:12,991 --> 00:03:14,816
Speaker 0: Ah, so here's what's happening.

60
00:03:14,836 --> 00:03:15,458
Speaker 0: You know what it is?

61
00:03:16,701 --> 00:03:20,010
Speaker 0: It's winning the battles, thereby losing the war.

62
00:03:21,230 --> 00:03:22,273
Speaker 0: So what is happening?

63
00:03:22,814 --> 00:03:28,850
Speaker 0: And I'll link to a slash article that links to an abstract from Oxford and an article explaining this in more detail.

64
00:03:29,291 --> 00:03:34,690
Speaker 0: And I apologize to any of our virologist friends, both of you who are going to yell at me for getting this wrong.

65
00:03:34,890 --> 00:03:35,628
Speaker 1: You're going to be so wrong.

66
00:03:36,010 --> 00:03:37,010
Speaker 1: I'm glad this isn't my news.

67
00:03:37,251 --> 00:03:42,750
Speaker 0: So I'm going to try to say this without saying anything that they can actually call me out on for.

68
00:03:43,030 --> 00:03:43,989
Speaker 1: Well, you're an expert at that.

69
00:03:44,650 --> 00:03:44,808
Speaker 0: Yes.

70
00:03:47,650 --> 00:03:57,990
Speaker 0: So HIV, the virus that causes AIDS, is becoming less virulent in the sense that...

71
00:03:58,210 --> 00:03:58,890
Speaker 1: What does virulent even mean?

72
00:03:59,191 --> 00:04:07,910
Speaker 0: So it replicates more slowly because in order... Basically, some people are more resistant to AIDS.

73
00:04:08,210 --> 00:04:08,404
Speaker 0: Obviously.

74
00:04:08,790 --> 00:04:16,349
Speaker 0: If they get HIV, they get AIDS, you know, the syndrome that's caused by the eventually depleted immune system, depleted by HIV later.

75
00:04:17,291 --> 00:04:19,950
Speaker 0: Those people are more effectively treatable by drugs.

76
00:04:21,091 --> 00:04:33,470
Speaker 0: So in order to replicate better among those people, it replicates slower to avoid triggering the immune responses when the immune system is particularly strong.

77
00:04:34,310 --> 00:04:43,127
Speaker 0: When it replicates slower, it's more effectively able to be treated, and this is becoming something that we are now able to measure.

78
00:04:44,430 --> 00:04:50,070
Speaker 0: AIDS replicates 10% lower in Botswana than it does in South Africa.

79
00:04:51,850 --> 00:04:57,637
Speaker 0: The virus is literally evolving in the places where we can't treat it that effectively.

80
00:04:58,078 --> 00:05:07,510
Speaker 0: to replicate slower and effectively be more treatable because of the selection pressures of the people who are able to spread it and the demographics of those regions and all these things.

81
00:05:09,311 --> 00:05:15,506
Speaker 0: Golder added that the average time from infection to the onset of AIDS has increased 25% over the past decade.

82
00:05:18,070 --> 00:05:27,830
Speaker 0: So if this evolutionary trend in HIV continues, HIV is rapidly becoming a disease that is not that dangerous.

83
00:05:28,331 --> 00:05:33,770
Speaker 1: Well, if it starts losing by going slow, won't it start going faster?

84
00:05:34,911 --> 00:05:38,583
Speaker 1: Some HIV mutation will make it go faster.

85
00:05:38,784 --> 00:05:40,530
Speaker 1: That one will catch on all over the place.

86
00:05:40,951 --> 00:05:46,690
Speaker 0: It's tricky because if you go too fast as a virus like this and you kill your host, you're not spreading anymore.

87
00:05:47,410 --> 00:05:54,610
Speaker 0: And if you go super slow and spread more widely but do less damage, you might spread more widely but people are less worried about it.

88
00:05:55,190 --> 00:05:59,330
Speaker 1: So you just wait until everyone in the world has the AIDS and then you go fast.

89
00:05:59,933 --> 00:06:03,230
Speaker 0: Yeah, that's my strategy when I play pandemic as a virus.

90
00:06:06,491 --> 00:06:08,696
Speaker 1: So this article is fascinating.

91
00:06:09,277 --> 00:06:14,930
Speaker 0: Read the actual article because it describes this in much better detail than I ever could.

92
00:06:15,090 --> 00:06:21,190
Speaker 0: But the gist of it is evolution is happening in HIV and it's actually working out.

93
00:06:21,270 --> 00:06:27,670
Speaker 0: It's like the one time when the thing that's happening that's demonstrable by science is actually the thing we kind of want.

94
00:06:27,790 --> 00:06:29,156
Speaker 0: Like the good answer happens.

95
00:06:29,859 --> 00:06:32,490
Speaker 0: Not like with space, like oh yeah, you'll never get to any other stars.

96
00:06:32,670 --> 00:06:33,550
Speaker 0: Fuck you, speed of light.

97
00:06:35,031 --> 00:06:37,403
Speaker 0: Like this time the answer actually fell our way.

98
00:06:37,443 --> 00:06:38,710
Speaker 0: Like everything's coming up Milhouse.

99
00:06:40,011 --> 00:06:44,470
Speaker 0: So in other news, Flickr is a platform that allows you to share your photos.

100
00:06:45,011 --> 00:06:50,610
Speaker 0: And it is actually, still in my opinion, the best platform in the world to do that.

101
00:06:50,810 --> 00:06:53,279
Speaker 1: Yeah, I mean, it's the oldest.

102
00:06:53,420 --> 00:06:56,170
Speaker 1: You might not prefer the interface but it's a good interface.

103
00:06:56,290 --> 00:06:57,336
Speaker 0: Everyone, what are you going to use?

104
00:06:57,437 --> 00:06:58,422
Speaker 0: Fucking Picasa?

105
00:06:58,442 --> 00:06:59,770
Speaker 0: Picasa's bullshit.

106
00:06:59,950 --> 00:07:00,934
Speaker 1: Facebook, right?

107
00:07:01,034 --> 00:07:05,210
Speaker 1: I mean, there's some fancy ones like 500px and shit like that.

108
00:07:05,330 --> 00:07:07,950
Speaker 0: Yeah, however, Flickr costs a reasonable amount of money.

109
00:07:08,150 --> 00:07:19,350
Speaker 1: Right, that's the biggest thing Flickr has going for it and the reason I've used it forever is because for a flat annual fee that is incredibly low, it's like 25 a year or something like that, it's so cheap.

110
00:07:19,852 --> 00:07:21,559
Speaker 1: You get 100% absolutely unlimited space.

111
00:07:24,630 --> 00:07:37,250
Speaker 1: Now there are other services that can give you unlimited space for a reasonable amount of money but those services do not have the photo abilities of Flickr which pretty much takes care of absolutely all of your photo needs ever.

112
00:07:37,930 --> 00:07:38,111
Speaker 0: Yup.

113
00:07:38,131 --> 00:07:41,950
Speaker 0: It's also one of the few things Yahoo has that's actually good so they're not going to let it die.

114
00:07:41,990 --> 00:07:42,758
Speaker 1: Well, they bought it.

115
00:07:43,102 --> 00:07:43,829
Speaker 1: They didn't make it.

116
00:07:44,170 --> 00:07:46,668
Speaker 0: No, but of everything that Yahoo owns...

117
00:07:46,910 --> 00:07:47,770
Speaker 1: Yahoo Sports and Weather are good.

118
00:07:48,011 --> 00:07:53,190
Speaker 0: Yeah, I was about to say Yahoo Sports, Yahoo Weather, and Yahoo Online Games.

119
00:07:54,250 --> 00:07:56,608
Speaker 0: That plus Flickr is all Yahoo's got going for them.

120
00:07:57,450 --> 00:08:02,070
Speaker 0: So Flickr has an option where you can pick the license for your thing.

121
00:08:02,311 --> 00:08:05,089
Speaker 1: Like Flickr was always... Every photo sharing site needs to have that.

122
00:08:05,210 --> 00:08:07,369
Speaker 0: Yeah, but Flickr does a really good job about it.

123
00:08:07,710 --> 00:08:10,490
Speaker 1: They were one of the first places to offer that feature.

124
00:08:10,752 --> 00:08:13,410
Speaker 0: Yeah, they offer that pretty much from the beginning.

125
00:08:14,310 --> 00:08:17,470
Speaker 0: And even better, they give you every useful option.

126
00:08:17,770 --> 00:08:27,430
Speaker 0: So I can choose all rights reserved, non-commercial, no derivatives, non-commercial, share alike, non-commercial, no derivatives, share alike, or just attribution which is what I use because I don't care.

127
00:08:27,851 --> 00:08:29,090
Speaker 0: Do whatever the fuck you want with my pictures.

128
00:08:29,371 --> 00:08:31,270
Speaker 1: I don't think I have pictures that are going to make me any money.

129
00:08:31,790 --> 00:08:36,570
Speaker 1: So if you can make money from them, well, that means I'll get noticed and then I'll make money from the next one.

130
00:08:36,770 --> 00:08:38,929
Speaker 0: And if I don't get noticed, I wasn't going to make money anyway.

131
00:08:39,091 --> 00:08:41,321
Speaker 1: You know how many photos there are in the world?

132
00:08:41,542 --> 00:08:43,110
Speaker 1: One individual photo is worth nothing.

133
00:08:43,350 --> 00:08:43,871
Speaker 0: And you know what?

134
00:08:44,011 --> 00:08:50,950
Speaker 0: If I took a really good photo of a bridge and some magazine uses that instead of paying a guy who took a slightly better photo.

135
00:08:51,831 --> 00:08:55,570
Speaker 0: Sucks for slightly better photo guy, but good enough photo works fine.

136
00:08:57,050 --> 00:08:59,130
Speaker 1: Magazine didn't sell extra copies because of your photo.

137
00:09:00,071 --> 00:09:12,310
Speaker 0: So basically what's happening is there's a bunch of sensationalist articles going around about how Flickr is selling your Creative Commons photos for profit and not giving any of that to you.

138
00:09:12,530 --> 00:09:15,258
Speaker 1: This reminds me of the story back in the day.

139
00:09:15,820 --> 00:09:18,890
Speaker 1: Some guy in the UK was putting Firefox on a CD and selling it.

140
00:09:19,330 --> 00:09:21,154
Speaker 0: Oh yeah.

141
00:09:21,274 --> 00:09:28,390
Speaker 0: I knew a guy at RIT who sold people CDs with like Nessus or whatever on it and that's all totally OK.

142
00:09:28,570 --> 00:09:29,514
Speaker 1: There's nothing wrong with that.

143
00:09:30,016 --> 00:09:31,221
Speaker 1: Hey, anyone want to buy Ubuntu?

144
00:09:31,281 --> 00:09:32,586
Speaker 1: I got some Ubuntu CDs here.

145
00:09:32,606 --> 00:09:33,249
Speaker 1: Ubuntu CDs.

146
00:09:33,249 --> 00:09:34,370
Speaker 1: $10.

147
00:09:34,370 --> 00:09:43,610
Speaker 0: In fact, if anyone would like to buy for me a really modern Linux operating system, I will happily sell it to you for like under $10,000.

148
00:09:43,610 --> 00:09:44,290
Speaker 1: Oh yeah, totally.

149
00:09:45,433 --> 00:09:47,090
Speaker 0: In fact, it'll be compatible with Red Hat.

150
00:09:47,790 --> 00:09:47,883
Speaker 0: Sure.

151
00:09:48,531 --> 00:09:58,810
Speaker 0: So basically morons are mad because they actively tagged their Flickr photos as being attribution only.

152
00:09:59,090 --> 00:09:59,191
Speaker 1: Right.

153
00:09:59,211 --> 00:10:01,701
Speaker 1: Well, I mean, the default is all rights reserved.

154
00:10:01,741 --> 00:10:03,830
Speaker 1: Meaning you're set to attribution only.

155
00:10:03,910 --> 00:10:15,890
Speaker 1: That means you specifically and consciously made a decision or accidentally clicked a not easy to click box to specifically say I want everyone in the Internet to be able to use these photos for anything that they want ever.

156
00:10:16,430 --> 00:10:17,090
Speaker 1: Just give me credit.

157
00:10:17,571 --> 00:10:17,751
Speaker 0: Yep.

158
00:10:18,172 --> 00:10:26,470
Speaker 0: So other people, because there is a non-commercial option, you can trivially say anyone's allowed to use my photo unless they use it commercially.

159
00:10:26,551 --> 00:10:28,549
Speaker 0: But these people didn't do that.

160
00:10:29,050 --> 00:10:44,390
Speaker 0: What Flickr is doing is anything on Flickr that's flagged as being usable commercially under Creative Commons license, you can just buy prints and they just take the money and they send the person to print with an attribution sticker and attribution stuff.

161
00:10:44,570 --> 00:10:49,458
Speaker 1: So someone else can go to my Flickr account, see one of my photos, give Flickr money.

162
00:10:49,699 --> 00:10:56,550
Speaker 1: Flickr will print my photo and send it to them and not give me any money and give me the credit, which is all I asked for.

163
00:10:56,890 --> 00:10:58,088
Speaker 0: Do you know how awesome that is?

164
00:10:58,310 --> 00:10:59,095
Speaker 1: That's actually pretty cool.

165
00:10:59,115 --> 00:11:01,609
Speaker 1: You could buy prints of your own photos without having to print them out.

166
00:11:01,811 --> 00:11:02,525
Speaker 0: Yeah, or like...

167
00:11:02,770 --> 00:11:04,210
Speaker 1: And they're probably going to be way high quality.

168
00:11:04,310 --> 00:11:07,585
Speaker 0: If my mom wants a print of something, she can just do it on her own.

169
00:11:07,625 --> 00:11:08,690
Speaker 0: She doesn't even need to talk to me.

170
00:11:09,691 --> 00:11:22,470
Speaker 0: So literally everyone who is mad about this is one, using technology improperly, two, actively made a poor decision, and three, misunderstands the ramifications of that active, poor decision.

171
00:11:24,632 --> 00:11:33,790
Speaker 0: If you don't know what you're clicking on, either don't click on it or read the documentation, or click on it and accept the consequences of what you clicked on.

172
00:11:34,050 --> 00:11:35,935
Speaker 1: Yeah, you can't complain about it.

173
00:11:35,975 --> 00:11:39,844
Speaker 1: It'd be one thing if the default was attribution only, but it's not.

174
00:11:40,325 --> 00:11:41,989
Speaker 1: So shut up.

175
00:11:42,751 --> 00:11:44,935
Speaker 0: And these people are really mad.

176
00:11:44,995 --> 00:11:49,745
Speaker 0: and there's all this vitriol going around about how Flickr's selling my photos without permission.

177
00:11:49,765 --> 00:11:52,150
Speaker 0: Like, no, Flickr's selling them with your express permission.

178
00:11:52,890 --> 00:11:54,444
Speaker 1: Explicit, expressed written consent.

179
00:11:54,504 --> 00:11:54,948
Speaker 1: You gave it.

180
00:11:56,430 --> 00:12:03,272
Speaker 0: To use a real world analogy, this would be like if I took a table, like an old table, and I put it... And this is a real story.

181
00:12:03,333 --> 00:12:03,854
Speaker 0: I did this.

182
00:12:03,954 --> 00:12:09,830
Speaker 0: Yeah, we had an old table, the previous table, the other table at RIT, and we had to get rid of it.

183
00:12:10,211 --> 00:12:13,641
Speaker 0: So we put it outside our apartment and we put a sign on it.

184
00:12:14,002 --> 00:12:16,830
Speaker 0: that said this is our apartment, come inside.

185
00:12:17,630 --> 00:12:22,762
Speaker 0: Either take it for free or if you want, come give us like 20 bucks for it.

186
00:12:23,324 --> 00:12:23,885
Speaker 0: And you know what?

187
00:12:24,025 --> 00:12:25,910
Speaker 0: Later in the day I went out and it was gone.

188
00:12:26,931 --> 00:12:33,429
Speaker 0: If I then complained that someone exercised option one as opposed to option two, I'm a moron.

189
00:12:34,070 --> 00:12:34,686
Speaker 1: Yes, you are.

190
00:12:36,071 --> 00:12:37,976
Speaker 0: So, yeah, that's my news.

191
00:12:38,317 --> 00:12:40,021
Speaker 0: I just really got to get ahead of this.

192
00:12:40,542 --> 00:12:43,450
Speaker 0: The people who are mad about this are dumb, and if you're mad about it, you're dumb.

193
00:12:45,110 --> 00:12:46,273
Speaker 1: Alright, another news.

194
00:12:46,353 --> 00:12:48,517
Speaker 1: Tomorrow, a news is gonna happen.

195
00:12:49,118 --> 00:12:55,930
Speaker 1: For the 8th time, T-Mobile is gonna have some un-carrier conference thingy, which is pretty much like the Apple keynote of T-Mobile.

196
00:12:56,611 --> 00:13:02,786
Speaker 1: Where they do it every quarter, and they come out and they talk about how they're so much better than all the other carriers.

197
00:13:02,806 --> 00:13:04,130
Speaker 1: Look at all the good stuff that we're gonna do.

198
00:13:04,430 --> 00:13:08,029
Speaker 0: Yeah, they're better until you get more than 10 miles outside of any major metropolitan area.

199
00:13:08,390 --> 00:13:09,031
Speaker 1: So here's the thing.

200
00:13:09,071 --> 00:13:13,117
Speaker 1: It's like, T-Mobile, I want to use you as my carrier.

201
00:13:13,218 --> 00:13:15,201
Speaker 1: In so many ways, you are correct.

202
00:13:15,221 --> 00:13:21,470
Speaker 1: You are definitely taking the path that I would take if I was a phone wireless provider in the United States.

203
00:13:21,570 --> 00:13:22,915
Speaker 1: Do the things that other people aren't doing.

204
00:13:23,316 --> 00:13:23,979
Speaker 1: No bullshit.

205
00:13:24,039 --> 00:13:27,010
Speaker 1: Contracts, no locking people.

206
00:13:27,070 --> 00:13:30,807
Speaker 0: Global free international roaming, which I use monthly.

207
00:13:31,711 --> 00:13:32,232
Speaker 1: Right, yeah.

208
00:13:32,333 --> 00:13:33,937
Speaker 1: All these kind of awesome things, right?

209
00:13:34,258 --> 00:13:36,043
Speaker 1: Here are the problems you got, T-Mobile.

210
00:13:36,524 --> 00:13:37,166
Speaker 1: Here are the problems.

211
00:13:37,206 --> 00:13:38,470
Speaker 1: The reason I'm still using Verizon.

212
00:13:38,810 --> 00:13:40,899
Speaker 0: Alright, I'm really curious what these are.

213
00:13:41,120 --> 00:13:43,510
Speaker 0: Because it's not like you ever fucking leave the city.

214
00:13:43,950 --> 00:13:44,735
Speaker 1: No, but that, yeah.

215
00:13:44,936 --> 00:13:46,686
Speaker 1: Well, I mean, Verizon, I can go around the world too.

216
00:13:46,727 --> 00:13:47,270
Speaker 1: It's not a problem.

217
00:13:47,570 --> 00:13:48,638
Speaker 0: You don't get free roaming.

218
00:13:48,658 --> 00:13:50,190
Speaker 0: You don't get data roaming for free.

219
00:13:50,571 --> 00:13:51,655
Speaker 1: It's shitty 2G roaming.

220
00:13:51,695 --> 00:13:55,070
Speaker 1: I'm still gonna pay to get 4Gs when I go to some other country.

221
00:13:55,070 --> 00:13:58,970
Speaker 0: Scott, I can pay like 10 bucks and get like a gig of 4G in those countries.

222
00:13:59,373 --> 00:13:59,755
Speaker 1: That's fine.

223
00:13:59,775 --> 00:14:01,970
Speaker 1: I can pay 10 bucks and buy a SIM card in those other countries.

224
00:14:02,570 --> 00:14:06,410
Speaker 0: Yeah, and then you have to go activate it and all that bullshit as opposed to it starts working when you land.

225
00:14:06,610 --> 00:14:07,011
Speaker 0: That's alright.

226
00:14:07,572 --> 00:14:10,961
Speaker 1: Anyway, here are the problems with T-Mobile.

227
00:14:11,021 --> 00:14:13,567
Speaker 1: Number one, you're not actually any goddamn cheaper.

228
00:14:14,249 --> 00:14:14,690
Speaker 1: Not at all.

229
00:14:14,850 --> 00:14:16,470
Speaker 1: In fact, sometimes more expensive.

230
00:14:17,471 --> 00:14:25,868
Speaker 0: With all taxes and fees combined, for my effectively unlimited data whatever plan, unlimited everything, $74 a month after taxes and fees.

231
00:14:27,070 --> 00:14:27,552
Speaker 0: That's right.

232
00:14:27,813 --> 00:14:28,456
Speaker 1: And here's the thing.

233
00:14:28,576 --> 00:14:29,440
Speaker 1: I pay like a hundred.

234
00:14:29,560 --> 00:14:30,203
Speaker 1: So wait a minute.

235
00:14:30,705 --> 00:14:31,307
Speaker 1: That's not right.

236
00:14:31,428 --> 00:14:32,010
Speaker 1: It is cheaper.

237
00:14:32,591 --> 00:14:36,129
Speaker 1: It's not because you don't have any subsidized phones.

238
00:14:36,830 --> 00:14:38,769
Speaker 1: So you gotta pay full price for the phone.

239
00:14:39,330 --> 00:14:42,010
Speaker 0: I buy unlocked phones unsubsidized anyway.

240
00:14:42,210 --> 00:14:43,149
Speaker 1: And how much do they cost?

241
00:14:43,971 --> 00:14:47,870
Speaker 0: Well, I paid I think like 200 bucks for this Nexus 4.

242
00:14:47,870 --> 00:14:49,088
Speaker 1: Oh, for a crapo phone.

243
00:14:50,053 --> 00:14:51,950
Speaker 1: If you get an iPhone, how much would you have to pay?

244
00:14:52,252 --> 00:14:52,756
Speaker 0: A list.

245
00:14:52,796 --> 00:14:52,937
Speaker 0: So

246
00:14:52,937 --> 00:14:54,810
Speaker 0: $750? $740?

247
00:14:54,810 --> 00:15:02,189
Speaker 1: Yeah, if you take the $750 plus the T-Mobile lower cost compared to like my $300 and the increased pricing cost.

248
00:15:02,731 --> 00:15:03,910
Speaker 0: Actually, I calculated this.

249
00:15:03,910 --> 00:15:08,010
Speaker 1: Over the two years, it actually works out so that it's about even.

250
00:15:08,491 --> 00:15:09,417
Speaker 0: Yeah, that's what I was about to say.

251
00:15:09,458 --> 00:15:10,485
Speaker 0: I did that calculation.

252
00:15:10,565 --> 00:15:11,230
Speaker 0: It's about the same.

253
00:15:11,370 --> 00:15:12,072
Speaker 1: It's about the same.

254
00:15:12,713 --> 00:15:23,350
Speaker 1: So if I'm paying about the same, then why would I go somewhere that has, even though I agree with many of your policies, you're just slower and just less reliable?

255
00:15:23,910 --> 00:15:25,016
Speaker 0: I wouldn't say it's slower.

256
00:15:25,117 --> 00:15:27,370
Speaker 0: The 4G is fucking wicked fast.

257
00:15:28,730 --> 00:15:30,750
Speaker 1: Yeah, but have you used a Verizon with LTE?

258
00:15:31,731 --> 00:15:32,721
Speaker 0: Yeah, I've seen it.

259
00:15:32,742 --> 00:15:33,510
Speaker 0: It's like it's not.

260
00:15:34,913 --> 00:15:35,917
Speaker 1: It is night in the day.

261
00:15:36,239 --> 00:15:37,625
Speaker 1: I've used the T-Mobile at work.

262
00:15:37,986 --> 00:15:38,850
Speaker 1: It is not so good.

263
00:15:39,010 --> 00:15:46,050
Speaker 0: I was in Istanbul and Germany and London all in the span of a few days, landing in all those countries along the way.

264
00:15:46,471 --> 00:15:53,950
Speaker 0: There's no easy way to have gotten data in all those places without spending a lot of time at multiple airports or paying a ton of money.

265
00:15:53,990 --> 00:15:55,987
Speaker 1: Walk into a drug store, grab a SIM card, pop it in.

266
00:15:57,611 --> 00:15:58,226
Speaker 0: Yeah, if you go...

267
00:15:58,390 --> 00:15:59,067
Speaker 1: Type some numbers in.

268
00:15:59,370 --> 00:16:00,657
Speaker 0: Scott, how often have you traveled?

269
00:16:00,919 --> 00:16:02,950
Speaker 0: Try that and see how quickly it all works.

270
00:16:03,090 --> 00:16:08,370
Speaker 0: Also, if you do that in Istanbul, that SIM card gets blocked about two hours after you activate it.

271
00:16:08,617 --> 00:16:08,889
Speaker 1: Why?

272
00:16:09,270 --> 00:16:10,033
Speaker 0: Because of the laws.

273
00:16:10,053 --> 00:16:14,210
Speaker 0: You're not allowed to use foreign phones in Turkey with local data plans.

274
00:16:14,697 --> 00:16:14,948
Speaker 1: Uh-huh.

275
00:16:15,191 --> 00:16:16,717
Speaker 0: A lot of countries have bullshit like that.

276
00:16:16,998 --> 00:16:19,930
Speaker 0: Trust me, I travel internationally a lot.

277
00:16:20,490 --> 00:16:25,290
Speaker 0: Buying a SIM card in the airport and putting it in your phone often has problems.

278
00:16:26,375 --> 00:16:26,949
Speaker 1: There's a solution.

279
00:16:27,550 --> 00:16:28,477
Speaker 0: Yeah, you know what the solution is?

280
00:16:28,497 --> 00:16:29,464
Speaker 0: I have T-Mobile.

281
00:16:29,625 --> 00:16:30,230
Speaker 0: I land there.

282
00:16:30,370 --> 00:16:31,127
Speaker 1: And you got crappy 2Gs.

283
00:16:31,630 --> 00:16:32,031
Speaker 0: You know what?

284
00:16:32,232 --> 00:16:38,030
Speaker 0: That 2G is actually pretty fine because a lot of the times I end up getting HSPA speeds anyway.

285
00:16:38,310 --> 00:16:42,030
Speaker 1: Also, I would never travel to a country that didn't have freedom of speech in that way.

286
00:16:42,692 --> 00:16:45,250
Speaker 0: So you don't want to ever visit Istanbul?

287
00:16:45,771 --> 00:16:50,150
Speaker 1: Just like I wouldn't take a job that blocked the internet, I wouldn't go to a country that blocked the internet.

288
00:16:50,510 --> 00:16:51,990
Speaker 0: Yeah, you're not going to go to a lot of countries.

289
00:16:52,450 --> 00:16:53,249
Speaker 1: Yeah, I'm not.

290
00:16:53,770 --> 00:16:55,683
Speaker 0: Okay, you're not going to see... You keep talking about.

291
00:16:55,703 --> 00:16:56,610
Speaker 0: you want to see the old shit.

292
00:16:57,170 --> 00:17:03,110
Speaker 1: And you're lucky I'm not in charge of this country because I would say, "Hey, if your country's evil, no one from your country is allowed here and we're deporting anyone who is

293
00:17:03,130 --> 00:17:03,210
Speaker 1: here.".

294
00:17:03,210 --> 00:17:03,833
Speaker 0: Yeah, Scott, you know what?

295
00:17:04,074 --> 00:17:08,010
Speaker 0: The old shit in Europe is like baby's first castle compared to Istanbul.

296
00:17:08,539 --> 00:17:08,730
Speaker 0: Yeah.

297
00:17:09,051 --> 00:17:09,310
Speaker 0: Just saying.

298
00:17:09,672 --> 00:17:11,138
Speaker 1: I know that, but that doesn't matter.

299
00:17:11,359 --> 00:17:13,250
Speaker 1: If your country is evil, I'm not going to go to it.

300
00:17:13,849 --> 00:17:17,338
Speaker 0: Okay, anyway, as I'm saying, I travel around a lot in the world.

301
00:17:17,358 --> 00:17:17,779
Speaker 0: You know what?

302
00:17:18,160 --> 00:17:22,109
Speaker 0: The fact that as soon as I land in any country in the world except North Korea and Somalia...

303
00:17:22,430 --> 00:17:23,814
Speaker 1: 2G, I might as well have nothing.

304
00:17:23,954 --> 00:17:26,359
Speaker 1: It's like I have free internet everywhere I go.

305
00:17:26,420 --> 00:17:27,262
Speaker 1: It's a dial-up modem.

306
00:17:27,321 --> 00:17:28,223
Speaker 1: But it's free internet.

307
00:17:28,464 --> 00:17:29,267
Speaker 1: It's worthless.

308
00:17:30,009 --> 00:17:30,510
Speaker 1: 2G is nothing.

309
00:17:30,630 --> 00:17:31,392
Speaker 0: Yeah, you know what, Scott?

310
00:17:31,854 --> 00:17:35,003
Speaker 0: I used it as my internet when I was in Australia last time.

311
00:17:35,083 --> 00:17:35,504
Speaker 0: You know what?

312
00:17:36,066 --> 00:17:36,387
Speaker 0: It was 100% fine.

313
00:17:37,872 --> 00:17:40,330
Speaker 0: The only thing I couldn't do was watch YouTube videos on it.

314
00:17:40,611 --> 00:17:46,090
Speaker 1: When I was in Australia, when I had the 3G because the 4G didn't work, it was suffering.

315
00:17:47,230 --> 00:17:47,591
Speaker 0: How?

316
00:17:47,832 --> 00:17:49,336
Speaker 0: I don't remember you suffering.

317
00:17:49,356 --> 00:17:51,863
Speaker 0: In fact, I had H-G-S-P-A+.

318
00:17:51,944 --> 00:17:52,786
Speaker 0: It was wicked fast.

319
00:17:52,846 --> 00:17:54,190
Speaker 0: It was totally fine the whole time.

320
00:17:54,331 --> 00:17:55,249
Speaker 1: It was many slownesses.

321
00:17:55,991 --> 00:17:57,300
Speaker 0: I did not have this problem.

322
00:17:57,341 --> 00:17:58,610
Speaker 0: The fuck are you doing on the phone?

323
00:17:58,630 --> 00:18:00,144
Speaker 1: It's because your... Because the thing is...

324
00:18:00,850 --> 00:18:01,856
Speaker 0: Scott, that wasn't T-Mobile.

325
00:18:01,916 --> 00:18:04,550
Speaker 0: I had the same fucking SIM card you did last time.

326
00:18:04,590 --> 00:18:07,098
Speaker 1: Because you are not used to the... See, this is the thing.

327
00:18:07,118 --> 00:18:11,010
Speaker 1: You don't have the incredible LTE speed of New York Verizon that I have.

328
00:18:11,150 --> 00:18:12,480
Speaker 1: So it doesn't seem slow to you.

329
00:18:13,004 --> 00:18:13,870
Speaker 1: That's what's going on here.

330
00:18:13,890 --> 00:18:14,860
Speaker 0: Scott, I go to a web page.

331
00:18:14,880 --> 00:18:15,830
Speaker 0: It loads instantly.

332
00:18:16,370 --> 00:18:21,709
Speaker 1: You don't realize how slow it is because you have... It's like saying, "Oh man, my car is crazy fast because I'm driving a

333
00:18:21,729 --> 00:18:21,930
Speaker 1: Ferrari.".

334
00:18:22,091 --> 00:18:22,573
Speaker 1: You don't know.

335
00:18:22,995 --> 00:18:23,719
Speaker 1: You just don't know.

336
00:18:24,745 --> 00:18:25,509
Speaker 1: You don't know what fast is.

337
00:18:26,111 --> 00:18:28,587
Speaker 0: I don't think that level of fat... Scott, I...

338
00:18:30,011 --> 00:18:33,230
Speaker 1: I'll trade phones with you for a day and then we'll trade back and we'll see what happens.

339
00:18:34,152 --> 00:18:34,959
Speaker 0: Yeah, fine.

340
00:18:35,343 --> 00:18:36,210
Speaker 0: Like, okay.

341
00:18:36,550 --> 00:18:39,790
Speaker 1: Yeah, see how willing you are to take my much superior phone.

342
00:18:40,372 --> 00:18:42,750
Speaker 0: No, because I'm not convinced it's that much of a difference.

343
00:18:43,110 --> 00:18:44,469
Speaker 0: So anyway, those are your reasons?

344
00:18:45,510 --> 00:18:45,599
Speaker 0: Yeah.

345
00:18:46,037 --> 00:18:46,230
Speaker 0: Okay.

346
00:18:46,410 --> 00:18:50,080
Speaker 1: But yeah, no, they're going to announce something tomorrow that's supposedly crazy awesome.

347
00:18:50,200 --> 00:18:53,710
Speaker 1: that will give us even more faith in them being the uncaring.

348
00:18:53,770 --> 00:18:56,289
Speaker 0: Yeah, well, the thing is it doesn't matter because...

349
00:18:56,390 --> 00:18:58,469
Speaker 1: Well, maybe it's increased speed and lower prices.

350
00:18:59,711 --> 00:19:01,430
Speaker 0: I'm just continuing to use my Nexus 4.

351
00:19:01,430 --> 00:19:05,910
Speaker 0: Because honestly, now that I got Lollipop, Lollipop's fucking amazing.

352
00:19:07,091 --> 00:19:11,350
Speaker 0: Android is accelerating in terms of being not shitty, like rapidly with Lollipop.

353
00:19:12,030 --> 00:19:16,810
Speaker 0: And my battery lasts like 20 to 40% longer and like everything's great.

354
00:19:17,291 --> 00:19:21,389
Speaker 0: I'm really glad I didn't buy a new phone because a lot of those new phones don't have Lollipop yet.

355
00:19:22,390 --> 00:19:22,708
Speaker 1: Mm-hmm.

356
00:19:22,890 --> 00:19:29,986
Speaker 0: So pretty much, I think, unless this phone breaks, I'm just going to wait until Lollipop drops on that Z3C.

357
00:19:31,791 --> 00:19:37,690
Speaker 0: And if it does and it doesn't break the Z3C, I'm going to buy a cheap Z3C, probably great market.

358
00:19:38,035 --> 00:19:38,949
Speaker 0: And that'll be my new phone.

359
00:19:39,852 --> 00:19:40,376
Speaker 1: That's another thing.

360
00:19:40,497 --> 00:19:42,169
Speaker 1: iPhones just updates all the time.

361
00:19:42,551 --> 00:19:45,410
Speaker 1: You don't have to wait based on the carrier, based on the phone.

362
00:19:46,230 --> 00:19:50,670
Speaker 0: You don't if you buy a just standard Android phone.

363
00:19:50,893 --> 00:19:52,110
Speaker 0: It just upgrades immediately.

364
00:19:53,490 --> 00:19:58,950
Speaker 0: The problem is that all these companies, they put their bullshit carrier specific garbage on the phone.

365
00:19:59,130 --> 00:20:01,946
Speaker 1: iPhone, they're not allowed to put carrier specific garbage on the iPhone.

366
00:20:01,986 --> 00:20:02,710
Speaker 1: Apple doesn't let them.

367
00:20:03,010 --> 00:20:05,572
Speaker 0: Yeah, but the iPhone's $750.

368
00:20:05,572 --> 00:20:07,651
Speaker 1: No, it's actually like $200 or $300.

369
00:20:07,651 --> 00:20:08,430
Speaker 0: Yeah, because you get it subsidized.

370
00:20:08,950 --> 00:20:11,690
Speaker 1: Well, you're paying the same amount either way.

371
00:20:12,110 --> 00:20:16,410
Speaker 0: Yeah, except I can buy a gray market version of one of these phones way cheaper.

372
00:20:17,232 --> 00:20:19,709
Speaker 1: I'm sure I can go steal an iPhone for way cheaper.

373
00:20:20,092 --> 00:20:23,550
Speaker 0: Yes, I'll just pirate an iPhone and print it in my 3D printer.

374
00:20:23,871 --> 00:20:24,578
Speaker 1: I wish I could.

375
00:20:24,598 --> 00:20:25,063
Speaker 1: What if I could?

376
00:20:26,190 --> 00:20:27,404
Speaker 0: You wouldn't download a car.

377
00:20:27,424 --> 00:20:27,909
Speaker 0: Fuck you.

378
00:20:28,311 --> 00:20:29,810
Speaker 1: If I could download a car, I would.

379
00:20:30,790 --> 00:20:32,208
Speaker 1: I'd download a jet first though.

380
00:20:33,932 --> 00:20:36,530
Speaker 0: Scott, if you could download a jet, that would be the day that you died.

381
00:20:37,133 --> 00:20:37,729
Speaker 1: Why would I die?

382
00:20:38,071 --> 00:20:39,230
Speaker 0: How would you pilot a jet?

383
00:20:40,395 --> 00:20:41,169
Speaker 1: I'd print out a pilot.

384
00:20:49,083 --> 00:20:51,200
Speaker 0: But anyway, things of the day.

385
00:20:51,381 --> 00:20:54,118
Speaker 0: This is a photo from an earlier time.

386
00:20:54,720 --> 00:20:55,519
Speaker 1: That's just a photo?

387
00:20:55,820 --> 00:21:03,400
Speaker 0: It's just a photo because it's a photo that needs to be seen more widely to remind people of some facts.

388
00:21:03,660 --> 00:21:19,000
Speaker 0: For example, for all you weirdly misogynistic computer science nerds who are mad about women in your spaces and whatever, computer programming was seen as women's work until relatively recently in the grand scheme of the history of computers.

389
00:21:19,100 --> 00:21:24,360
Speaker 1: It was pretty much, oh mostly women had that job for the first few decades that the job existed.

390
00:21:24,540 --> 00:21:26,940
Speaker 1: It wasn't until the 80s, 90s that it switched over.

391
00:21:27,240 --> 00:21:36,560
Speaker 0: Yeah, so if you don't know the names of people like Grace Hopper or Margaret Hamilton, and you're simultaneously one of those nerd programmer equivalents.

392
00:21:36,580 --> 00:21:38,220
Speaker 1: Then you're someone who's not listening to this podcast.

393
00:21:38,760 --> 00:21:39,799
Speaker 0: Yeah, so you know what?

394
00:21:40,220 --> 00:21:43,219
Speaker 0: Spread this around, you who do listen to your idiot friends in CS.

395
00:21:43,660 --> 00:21:43,920
Speaker 0: But anyway.

396
00:21:44,061 --> 00:21:46,160
Speaker 1: Maybe one Unix beard will see it.

397
00:21:46,260 --> 00:21:59,060
Speaker 0: This is just a wonderful photo of a woman named Margaret Hamilton, the NASA lead software engineer who wrote the Apollo Guidance Program, and the source code was printed out because this was a long time ago.

398
00:21:59,060 --> 00:22:00,179
Speaker 0: That's what they did in those days.

399
00:22:00,382 --> 00:22:00,469
Speaker 1: Yeah.

400
00:22:01,683 --> 00:22:03,099
Speaker 1: How else was everyone going to debug it?

401
00:22:03,822 --> 00:22:07,240
Speaker 1: Sit in the terminal in the VT100s and scroll through it?

402
00:22:07,600 --> 00:22:08,820
Speaker 0: Going to read your fucking punch card?

403
00:22:09,040 --> 00:22:09,819
Speaker 1: You had to go through it.

404
00:22:09,960 --> 00:22:12,280
Speaker 1: Read it on the paper with a red pen.

405
00:22:12,763 --> 00:22:19,940
Speaker 0: It's a photo of her standing next to the printed out source code of the Apollo Guidance Program that she wrote.

406
00:22:21,043 --> 00:22:22,517
Speaker 0: This is just an awesome photo.

407
00:22:24,080 --> 00:22:27,180
Speaker 1: Written in some nasty old program like some assembly shit.

408
00:22:27,440 --> 00:22:28,039
Speaker 0: You know what though?

409
00:22:28,481 --> 00:22:36,260
Speaker 0: If you're smart and you go read that, a smart person who knows how computers work could read that code probably more effectively than they could read a lot of modern garbage.

410
00:22:36,600 --> 00:22:36,756
Speaker 0: Oh yeah.

411
00:22:37,420 --> 00:22:38,480
Speaker 1: It's probably quite readable.

412
00:22:38,620 --> 00:22:39,259
Speaker 1: At the low level.

413
00:22:39,400 --> 00:22:40,820
Speaker 1: Pretty sure you can get the code online.

414
00:22:41,164 --> 00:22:42,780
Speaker 0: Yeah, like old programming.

415
00:22:43,841 --> 00:22:46,080
Speaker 0: It's just, what do I need to do?

416
00:22:46,220 --> 00:22:49,840
Speaker 0: I've got to calculate some things and put them in these registers so something can happen.

417
00:22:50,121 --> 00:22:50,538
Speaker 0: Like, go nuts.

418
00:22:51,420 --> 00:22:53,340
Speaker 0: Variables in memory or not.

419
00:22:53,400 --> 00:22:56,459
Speaker 1: Really, the input is pretty much just, where do you want to go?

420
00:22:56,760 --> 00:22:59,500
Speaker 1: And the output is, thrust the rockets this way.

421
00:22:59,882 --> 00:23:00,035
Speaker 0: Yep.

422
00:23:00,660 --> 00:23:02,920
Speaker 0: Maybe a feedback loop of, am I there yet?

423
00:23:03,081 --> 00:23:03,368
Speaker 0: Nope.

424
00:23:03,532 --> 00:23:03,819
Speaker 0: Why?

425
00:23:04,080 --> 00:23:04,977
Speaker 1: Check where I am.

426
00:23:05,640 --> 00:23:06,339
Speaker 1: Thrust some more.

427
00:23:06,580 --> 00:23:07,297
Speaker 1: Check where I am.

428
00:23:07,621 --> 00:23:08,260
Speaker 1: Where am I going?

429
00:23:08,460 --> 00:23:08,880
Speaker 1: Thrust some more.

430
00:23:08,941 --> 00:23:10,100
Speaker 0: You're not passing around a bunch of objects.

431
00:23:10,181 --> 00:23:12,880
Speaker 1: The program is just- Check all the sensors in this rocket.

432
00:23:13,040 --> 00:23:13,599
Speaker 1: What do they say?

433
00:23:13,780 --> 00:23:15,260
Speaker 1: Do some math, thrust some rockets.

434
00:23:15,400 --> 00:23:18,680
Speaker 0: I mean, old programming is basically, what do I do next?

435
00:23:19,040 --> 00:23:19,879
Speaker 0: Okay, now what do I do?

436
00:23:20,220 --> 00:23:20,739
Speaker 0: Okay, now what do I do?

437
00:23:20,860 --> 00:23:23,297
Speaker 1: Also, put out some numbers on the screen so the astronauts can see it.

438
00:23:24,202 --> 00:23:26,359
Speaker 1: Send it over the radio so Houston can see it.

439
00:23:26,701 --> 00:23:27,459
Speaker 0: All right, so what do you got?

440
00:23:28,640 --> 00:23:32,300
Speaker 1: So, remember we did an episode last week about the bus.

441
00:23:33,820 --> 00:23:34,178
Speaker 0: Macadamia.

442
00:23:34,901 --> 00:23:36,579
Speaker 1: Yeah, not that bus, but that's a good bus.

443
00:23:37,263 --> 00:23:40,720
Speaker 1: Here is a video taken on a New York City bus.

444
00:23:41,000 --> 00:23:41,900
Speaker 0: Wait a minute, wait a minute.

445
00:23:41,980 --> 00:23:42,960
Speaker 0: This rig's a WorldStar.

446
00:23:44,021 --> 00:23:48,398
Speaker 1: Well, the YouTube account that posted it is WorldStarHip- Oh, WorldStar.

447
00:23:49,341 --> 00:24:08,390
Speaker 1: WorldStarHipHoop is the name of the YouTube account, but it's pretty much just a dude on the bus and the bus is going slow and late and in traffic as usual, and it also got slowed down because a disabled person got on the bus, which by the way, if you're a disabled person, getting around New York City is really awful,

448
00:24:09,212 --> 00:24:09,353
Speaker 0: even

449
00:24:09,413 --> 00:24:12,400
Speaker 1: if you use the bus and AccessorRide.

450
00:24:12,480 --> 00:24:14,678
Speaker 0: Well, AccessorRide, so get this- AccessorRide sucks too.

451
00:24:14,820 --> 00:24:17,560
Speaker 1: You got to give AccessorRide- And of course, handicap accessible cabs.

452
00:24:17,680 --> 00:24:18,759
Speaker 1: They ain't going to pick your ass up.

453
00:24:19,125 --> 00:24:19,233
Speaker 0: Yup.

454
00:24:20,421 --> 00:24:26,918
Speaker 0: And also, all the subway stations that matter are so old, like 100 years old, that they don't have elevators.

455
00:24:27,740 --> 00:24:29,779
Speaker 0: And even if they did, the platforms are too small.

456
00:24:30,241 --> 00:24:33,379
Speaker 1: Yeah, if you live where I am, the subway isn't going to get you.

457
00:24:34,483 --> 00:24:35,679
Speaker 0: Dude, Queensborough Plaza.

458
00:24:35,820 --> 00:24:39,499
Speaker 1: You got to come all the way to where a rim is and you can only go to Queens Plaza, not Queensborough Plaza.

459
00:24:39,560 --> 00:24:42,400
Speaker 0: Yeah, Queensborough Plaza is one of the most used subway stations in New York.

460
00:24:43,620 --> 00:24:44,197
Speaker 0: Stairs only.

461
00:24:44,801 --> 00:24:47,180
Speaker 0: Do you know how many stories up you have to go?

462
00:24:47,320 --> 00:24:48,799
Speaker 1: Queens Plaza has an elevator though.

463
00:24:49,020 --> 00:24:58,100
Speaker 0: Yeah, guys, I live in the third story of a building that has super tall floors and the platform for the Astoria-bound trains is higher than me.

464
00:24:58,500 --> 00:25:02,836
Speaker 1: Right, so anyway, someone got on the bus with a wheelchair, so it slowed the bus down even more.

465
00:25:03,482 --> 00:25:06,100
Speaker 1: And there's some cranky old lady on the bus who's complaining.

466
00:25:06,661 --> 00:25:10,500
Speaker 1: And there's some guy and he's just like, you know, they're going to work in the morning.

467
00:25:10,764 --> 00:25:11,460
Speaker 1: They're all late.

468
00:25:11,865 --> 00:25:12,820
Speaker 1: It's all too early.

469
00:25:13,122 --> 00:25:16,160
Speaker 1: And this lady's yapping, saying this is bullshit and complaining.

470
00:25:16,280 --> 00:25:18,000
Speaker 1: And he's like, I had enough of this shit.

471
00:25:18,220 --> 00:25:19,639
Speaker 1: I'm going to tug back to you.

472
00:25:19,960 --> 00:25:20,678
Speaker 1: Yeah, yeah.

473
00:25:21,283 --> 00:25:21,860
Speaker 0: This video is great.

474
00:25:21,941 --> 00:25:22,415
Speaker 0: I watched it.

475
00:25:22,982 --> 00:25:25,340
Speaker 1: So he smacks that old lady down.

476
00:25:26,167 --> 00:25:26,719
Speaker 0: We're all late.

477
00:25:27,060 --> 00:25:27,599
Speaker 0: Everybody's late.

478
00:25:27,942 --> 00:25:29,478
Speaker 1: Nice little tongue lashing going on.

479
00:25:29,986 --> 00:25:30,580
Speaker 0: Just love it.

480
00:25:30,640 --> 00:25:31,520
Speaker 0: Like everybody's late.

481
00:25:31,762 --> 00:25:33,839
Speaker 0: You're mad because you got a shit ass job.

482
00:25:34,602 --> 00:25:35,840
Speaker 0: You're late to your shit job.

483
00:25:36,606 --> 00:25:37,120
Speaker 0: It's great.

484
00:25:37,420 --> 00:25:38,819
Speaker 1: I'm not upset about being late to my job.

485
00:25:38,980 --> 00:25:39,619
Speaker 1: What's the worst I got?

486
00:25:40,360 --> 00:25:41,218
Speaker 1: I'll get a new shit ass job.

487
00:25:44,282 --> 00:25:46,460
Speaker 0: This warmed the cockles of my heart, this video.

488
00:25:46,920 --> 00:25:49,919
Speaker 1: And this is why the bus is not a good place to be.

489
00:25:50,140 --> 00:25:54,177
Speaker 1: It's also why I love living in New York, because you got to sit in uncomfortable situations like that.

490
00:25:54,900 --> 00:25:55,000
Speaker 0: Yeah.

491
00:25:55,241 --> 00:25:57,500
Speaker 0: Thing is, that situation is not uncomfortable.

492
00:25:57,880 --> 00:26:01,200
Speaker 0: I would because, you know, if you live in New York, it gets a lot worse.

493
00:26:01,301 --> 00:26:01,451
Speaker 1: Yeah.

494
00:26:01,660 --> 00:26:10,900
Speaker 0: But if you live in New York, like whenever things go down, like all the people who aren't crazy are on the like, you know, like two people get in a fight on the subway.

495
00:26:10,900 --> 00:26:11,051
Speaker 0: Right.

496
00:26:11,680 --> 00:26:23,060
Speaker 0: There's like a couple hundred other people like crammed into that car, either trying not to be involved, but or actively like videoing it or, you know, yelling or whatever.

497
00:26:23,180 --> 00:26:31,220
Speaker 0: But the most beautiful thing is that everyone who's not involved is all making eye contact with each other, sharing this look of, can you believe this shit?

498
00:26:31,402 --> 00:26:31,758
Speaker 0: Oh, man.

499
00:26:31,900 --> 00:26:32,960
Speaker 0: Look, look what's going on right there.

500
00:26:33,000 --> 00:26:33,296
Speaker 0: Look at this.

501
00:26:33,560 --> 00:26:33,840
Speaker 0: Look at this.

502
00:26:33,941 --> 00:26:34,640
Speaker 0: I think he's about to hit him.

503
00:26:34,800 --> 00:26:35,093
Speaker 0: Oh, man.

504
00:26:37,261 --> 00:26:39,459
Speaker 0: Because none of it happens every day.

505
00:26:40,001 --> 00:26:45,280
Speaker 0: It's like the people who get into a fight don't realize that it happens every day in the moment.

506
00:26:45,420 --> 00:26:47,500
Speaker 0: The Book Club book is Watership Down.

507
00:26:48,223 --> 00:26:50,640
Speaker 1: It's a handful of chapters now.

508
00:26:50,921 --> 00:26:54,280
Speaker 1: And I can tell you that here are some things that I know about it.

509
00:26:54,340 --> 00:26:56,159
Speaker 1: Number one, it opens up.

510
00:26:56,461 --> 00:26:59,640
Speaker 1: It does that thing a lot of books do with the quotes at the beginning of every chapter.

511
00:27:00,261 --> 00:27:02,819
Speaker 0: Did you did you read the authors like, here's how I wrote the book?

512
00:27:03,603 --> 00:27:04,899
Speaker 1: No, I don't know if that's in my edition.

513
00:27:05,401 --> 00:27:10,917
Speaker 1: Oh, oh, oh, because there's something interesting I could tell you about the quote at the beginning of chapter one.

514
00:27:11,208 --> 00:27:11,419
Speaker 1: Yeah.

515
00:27:11,540 --> 00:27:13,900
Speaker 1: It's from my boy like a lamb none.

516
00:27:14,423 --> 00:27:14,528
Speaker 0: Yeah.

517
00:27:15,444 --> 00:27:16,580
Speaker 1: And so that's a positive.

518
00:27:16,782 --> 00:27:16,929
Speaker 0: Yeah.

519
00:27:17,801 --> 00:27:20,651
Speaker 1: Other than that, so far, it seems like it's like.

520
00:27:20,751 --> 00:27:27,000
Speaker 1: obviously it's not the same story, but it's pretty much the same kind of book as Lord of the Flies, only with rabbits.

521
00:27:27,801 --> 00:27:32,060
Speaker 1: Like each person is representing some one aspect of society, whatever.

522
00:27:32,960 --> 00:27:38,100
Speaker 0: So, you know, so, Scott, the author literally has said openly, this is not allegory.

523
00:27:38,740 --> 00:27:40,340
Speaker 0: Nobody is symbolic of anything.

524
00:27:40,922 --> 00:27:44,460
Speaker 0: It is just a story of rabbits trying to find a new home.

525
00:27:44,740 --> 00:27:45,799
Speaker 1: Tolkien said the same thing.

526
00:27:46,160 --> 00:27:46,271
Speaker 0: Yes.

527
00:27:46,862 --> 00:27:47,899
Speaker 0: So you can try.

528
00:27:48,381 --> 00:27:50,860
Speaker 0: I encourage you to continue to try to read in those aspects.

529
00:27:50,961 --> 00:27:53,779
Speaker 1: But you got the military guy.

530
00:27:53,880 --> 00:27:53,964
Speaker 1: Yeah.

531
00:27:54,560 --> 00:27:55,380
Speaker 1: Peasant rabbit.

532
00:27:55,860 --> 00:27:57,460
Speaker 1: You know, normal guy rabbit.

533
00:27:57,822 --> 00:27:59,739
Speaker 0: So I'll say this.

534
00:28:00,701 --> 00:28:05,880
Speaker 0: Every one of those rabbits is like you're going to expect the book to go in like a particular academic rabbit.

535
00:28:06,522 --> 00:28:09,400
Speaker 0: The rabbits are all going to like surprise you a bunch of times.

536
00:28:09,742 --> 00:28:12,159
Speaker 0: And it doesn't go in the direction you think it's going at all.

537
00:28:12,420 --> 00:28:12,595
Speaker 0: Whatever.

538
00:28:12,900 --> 00:28:13,680
Speaker 0: Also, big wigs.

539
00:28:13,780 --> 00:28:14,258
Speaker 0: My man.

540
00:28:14,762 --> 00:28:15,880
Speaker 1: Also, Agamemnon.

541
00:28:16,260 --> 00:28:16,552
Speaker 1: Yeah.

542
00:28:17,260 --> 00:28:20,100
Speaker 1: People don't know how bad ass Agamemnon is right.

543
00:28:20,100 --> 00:28:20,860
Speaker 1: I didn't even learn.

544
00:28:21,100 --> 00:28:21,380
Speaker 0: You know what?

545
00:28:21,481 --> 00:28:22,040
Speaker 0: You are a badass.

546
00:28:22,200 --> 00:28:24,780
Speaker 0: Agamemnon is going toe to toe with Achilles.

547
00:28:25,580 --> 00:28:28,140
Speaker 1: But that's what I'm saying is like so Achilles is basically invincible.

548
00:28:28,421 --> 00:28:31,560
Speaker 1: The baddest ass motherfucker in the whole world.

549
00:28:31,801 --> 00:28:35,520
Speaker 1: And he could only be killed by this weakness that no one even knows.

550
00:28:35,520 --> 00:28:36,659
Speaker 1: And it was random chance.

551
00:28:37,330 --> 00:28:37,580
Speaker 1: Right.

552
00:28:37,983 --> 00:28:38,520
Speaker 1: And Achilles.

553
00:28:38,782 --> 00:28:41,080
Speaker 1: You think someone's so bad ass, wouldn't they be the king?

554
00:28:41,323 --> 00:28:42,479
Speaker 1: But Achilles is not the king.

555
00:28:42,921 --> 00:28:43,999
Speaker 1: Agamemnon is the king.

556
00:28:44,343 --> 00:28:46,400
Speaker 1: And Achilles does what Agamemnon fucking says.

557
00:28:46,521 --> 00:28:47,557
Speaker 1: That's how bad as he is.

558
00:28:48,825 --> 00:28:49,359
Speaker 0: Of course, then.

559
00:28:49,562 --> 00:28:50,700
Speaker 1: Agamemnon says Achilles.

560
00:28:51,140 --> 00:28:51,599
Speaker 1: Lick my foot.

561
00:28:51,720 --> 00:28:52,117
Speaker 1: He says I.

562
00:28:52,820 --> 00:28:57,100
Speaker 0: So it's got if Agamemnon is that bad ass compared to Achilles.

563
00:28:57,422 --> 00:28:58,559
Speaker 0: How bad ass is Hector?

564
00:29:02,466 --> 00:29:03,199
Speaker 1: Don't even go there.

565
00:29:03,320 --> 00:29:03,409
Speaker 0: Yeah.

566
00:29:03,620 --> 00:29:03,939
Speaker 0: Don't even.

567
00:29:04,281 --> 00:29:04,539
Speaker 0: Of course.

568
00:29:04,961 --> 00:29:06,220
Speaker 0: You can't be too hubris here.

569
00:29:06,380 --> 00:29:06,729
Speaker 0: I'll stop.

570
00:29:07,420 --> 00:29:08,216
Speaker 0: Besides coming down.

571
00:29:09,840 --> 00:29:11,058
Speaker 0: Besides, like, what's this?

572
00:29:11,580 --> 00:29:13,119
Speaker 1: The baddest of all asses is me.

573
00:29:13,400 --> 00:29:14,077
Speaker 1: Don't forget it.

574
00:29:14,520 --> 00:29:14,600
Speaker 1: Yes.

575
00:29:15,043 --> 00:29:16,077
Speaker 1: Oh, you did forget it.

576
00:29:16,620 --> 00:29:17,459
Speaker 1: That's a shame.

577
00:29:20,723 --> 00:29:20,786
Speaker 1: Oh.

578
00:29:22,080 --> 00:29:22,438
Speaker 1: Agamemnon.

579
00:29:22,762 --> 00:29:23,459
Speaker 1: You wet your bed.

580
00:29:24,210 --> 00:29:24,559
Speaker 0: What happened?

581
00:29:24,640 --> 00:29:25,200
Speaker 0: That's a shame.

582
00:29:26,545 --> 00:29:27,018
Speaker 0: Everyone look.

583
00:29:27,280 --> 00:29:28,179
Speaker 0: Agamemnon wet the bed.

584
00:29:29,720 --> 00:29:29,765
Speaker 0: Oh.

585
00:29:30,716 --> 00:29:30,758
Speaker 0: Oh.

586
00:29:31,387 --> 00:29:31,560
Speaker 0: Oh.

587
00:29:32,065 --> 00:29:33,199
Speaker 0: You think you can repudiate me?

588
00:29:33,500 --> 00:29:33,959
Speaker 0: You're going to.

589
00:29:34,102 --> 00:29:34,599
Speaker 1: Oh, wait, wait.

590
00:29:34,681 --> 00:29:35,820
Speaker 1: Are you about to get in a boat?

591
00:29:37,260 --> 00:29:37,302
Speaker 1: Oh.

592
00:29:37,900 --> 00:29:38,740
Speaker 1: Oh, I'm sorry.

593
00:29:39,220 --> 00:29:39,349
Speaker 0: Oh.

594
00:29:39,741 --> 00:29:41,139
Speaker 0: You'll just never get into a boat again.

595
00:29:41,560 --> 00:29:41,656
Speaker 0: Yeah.

596
00:29:41,701 --> 00:29:42,120
Speaker 0: Guess what?

597
00:29:42,420 --> 00:29:45,300
Speaker 1: Oh, you're stuck in one place on this tiny island for the rest of your life.

598
00:29:45,488 --> 00:29:45,779
Speaker 1: All right.

599
00:29:46,001 --> 00:29:46,760
Speaker 1: Don't get in any boats.

600
00:29:47,062 --> 00:29:48,260
Speaker 0: Oh, is that a food boat coming?

601
00:29:48,622 --> 00:29:48,835
Speaker 0: Oh, no.

602
00:29:49,100 --> 00:29:49,376
Speaker 0: It sank.

603
00:29:50,361 --> 00:29:50,777
Speaker 0: That sucks.

604
00:29:52,400 --> 00:29:52,444
Speaker 1: Oh.

605
00:29:53,620 --> 00:29:54,959
Speaker 1: Too bad planes weren't invented yet.

606
00:29:55,640 --> 00:30:02,040
Speaker 0: I always imagine the Greek gods just being along the lines of Poseidon being a super dick to everyone and every other god being like, man, he's such a dick.

607
00:30:02,622 --> 00:30:03,260
Speaker 1: Oh, here's a boat.

608
00:30:03,420 --> 00:30:03,658
Speaker 1: Don't worry.

609
00:30:03,780 --> 00:30:04,579
Speaker 1: I won't sink this one.

610
00:30:04,840 --> 00:30:07,220
Speaker 1: I'll just move the sirens real close to it, like.

611
00:30:07,380 --> 00:30:10,457
Speaker 0: But yet they never call him a dick to his face because then he'll start being a dick to them.

612
00:30:11,281 --> 00:30:14,320
Speaker 0: It's like, he's being a dick to Agamemnon or he's being a dick to Odysseus.

613
00:30:14,380 --> 00:30:16,979
Speaker 0: So I'm just going to hang out over here and let that happen.

614
00:30:19,320 --> 00:30:19,380
Speaker 0: So.

615
00:30:19,680 --> 00:30:19,813
Speaker 0: Yeah.

616
00:30:20,441 --> 00:30:23,239
Speaker 0: Well, tell me when you get to the cult rabbits.

617
00:30:24,941 --> 00:30:28,880
Speaker 1: They are they prasing on their you're in a little bit.

618
00:30:28,940 --> 00:30:34,220
Speaker 0: So basically, you're at the point where there's Hazel, the smart rabbit and Fiverr, who's like, man, shit's going to get fucked up.

619
00:30:34,240 --> 00:30:35,400
Speaker 0: We should get the fuck out of here.

620
00:30:35,480 --> 00:30:37,360
Speaker 1: The prophetic religious rabbit.

621
00:30:37,603 --> 00:30:37,820
Speaker 0: Yeah.

622
00:30:38,060 --> 00:30:38,207
Speaker 0: Yeah.

623
00:30:38,521 --> 00:30:38,692
Speaker 0: Yeah.

624
00:30:39,263 --> 00:30:40,678
Speaker 0: So did they leave yet?

625
00:30:41,063 --> 00:30:41,516
Speaker 1: Yeah, they left.

626
00:30:42,000 --> 00:30:42,127
Speaker 0: Now.

627
00:30:43,263 --> 00:30:44,900
Speaker 1: So they crossed a river or some shit.

628
00:30:45,243 --> 00:30:45,499
Speaker 0: Oh, yeah.

629
00:30:45,642 --> 00:30:45,880
Speaker 0: All right.

630
00:30:45,980 --> 00:30:46,080
Speaker 0: Yeah.

631
00:30:46,180 --> 00:30:47,159
Speaker 0: The book moves real quick.

632
00:30:47,542 --> 00:30:48,280
Speaker 1: It does move real quick.

633
00:30:48,540 --> 00:30:48,898
Speaker 1: Like rabbits.

634
00:30:50,043 --> 00:30:51,360
Speaker 0: Wait till you get to the cult rabbits.

635
00:30:51,540 --> 00:30:52,660
Speaker 0: I think that's where you're going to go from.

636
00:30:52,721 --> 00:30:53,217
Speaker 0: This book's OK.

637
00:30:53,420 --> 00:30:54,638
Speaker 0: You're going to start really liking this book.

638
00:30:55,020 --> 00:30:59,400
Speaker 1: Well, the cultist rabbits are too much book club and not book club episode.

639
00:30:59,626 --> 00:30:59,979
Speaker 0: All right.

640
00:31:00,461 --> 00:31:01,360
Speaker 0: Watch your shit on YouTube.

641
00:31:01,841 --> 00:31:05,600
Speaker 0: We're going to be a packed south doing bad games on Sunday in the Armadillo theater.

642
00:31:05,700 --> 00:31:11,420
Speaker 0: And otherwise, we're just going to be hanging out and south in and in barbecue and making fun of the south.

643
00:31:11,561 --> 00:31:15,000
Speaker 0: And, you know, all the stuff we do at Pax is not getting shot by people in the south.

644
00:31:15,160 --> 00:31:15,819
Speaker 0: Yeah, hopefully.

645
00:31:16,880 --> 00:31:17,073
Speaker 0: Hopefully.

646
00:31:17,723 --> 00:31:19,259
Speaker 1: What's the open carry laws at Pax?

647
00:31:20,342 --> 00:31:23,580
Speaker 0: I'm pretty sure Pax is going to not allow open carry.

648
00:31:24,186 --> 00:31:24,878
Speaker 0: We'll see what happens.

649
00:31:25,080 --> 00:31:27,614
Speaker 1: Can they or is it similar to the Anita talk?

650
00:31:29,343 --> 00:31:30,219
Speaker 0: We're going to find out.

651
00:31:30,400 --> 00:31:36,375
Speaker 0: I mean, I will not walk out of our talk if people bring guns, but I see no one's going to shoot us.

652
00:31:36,680 --> 00:31:41,200
Speaker 0: But at the same time, I might not be able to resist commenting on that during the panel.

653
00:31:42,561 --> 00:31:43,260
Speaker 1: Aaron Sealby.

654
00:31:43,340 --> 00:31:43,838
Speaker 1: There will be all right.

655
00:31:44,020 --> 00:31:44,113
Speaker 1: Yeah.

656
00:31:44,627 --> 00:31:45,180
Speaker 0: I'm not afraid.

657
00:31:45,500 --> 00:31:46,318
Speaker 1: I hope he'll be there.

658
00:31:46,567 --> 00:31:46,675
Speaker 1: Yeah.

659
00:31:46,822 --> 00:31:47,618
Speaker 0: I think he's going to be there.

660
00:31:47,782 --> 00:31:47,978
Speaker 1: All right.

661
00:31:48,580 --> 00:31:48,680
Speaker 0: Good.

662
00:31:49,131 --> 00:31:49,300
Speaker 0: Yeah.

663
00:31:49,501 --> 00:31:50,998
Speaker 0: So that's it.

664
00:31:51,780 --> 00:31:52,140
Speaker 0: Metastuff.

665
00:31:52,801 --> 00:31:53,700
Speaker 0: Subscribe to our newsletter.

666
00:31:54,061 --> 00:31:56,080
Speaker 1: So actual episode.

667
00:31:56,983 --> 00:31:57,955
Speaker 1: So we wanted to talk about A.I.

668
00:31:58,581 --> 00:32:01,560
Speaker 1: I thought we had talked about it before, but I looked and I couldn't find an episode.

669
00:32:01,681 --> 00:32:04,960
Speaker 1: So what I was even if we did talk about it before, it was ridiculously long time ago.

670
00:32:05,160 --> 00:32:16,320
Speaker 0: But we but yeah, but we actually don't know that much about A.I Now, we could like I was talking at length and Gigi from the forum because he actually is like it like he knows a shit ton about A.I.

671
00:32:16,800 --> 00:32:17,544
Speaker 1: He's studying A.I.

672
00:32:17,564 --> 00:32:21,322
Speaker 0: in the school and he's explaining to me like current actual real A.I.

673
00:32:21,362 --> 00:32:24,359
Speaker 0: stuff and maybe we'll do a show and I get him or someone on.

674
00:32:24,720 --> 00:32:26,888
Speaker 1: I only know the most basics of A.I.

675
00:32:26,988 --> 00:32:29,758
Speaker 1: as a programmer, but who doesn't do anything involving

676
00:32:29,778 --> 00:32:29,818
Speaker 1: A.I.?

677
00:32:31,166 --> 00:32:31,640
Speaker 1: But you know what?

678
00:32:31,720 --> 00:32:35,899
Speaker 1: I do know a lot about completely fictional A.I It's all made up and shit like Skynet.

679
00:32:36,246 --> 00:32:36,520
Speaker 1: Yeah.

680
00:32:36,660 --> 00:32:39,800
Speaker 1: So let's just talk about that, because that's way more fun and easy.

681
00:32:39,982 --> 00:32:41,700
Speaker 1: So we're told I can go home and eat because I'm hungry.

682
00:32:41,861 --> 00:32:43,268
Speaker 0: So we're going to talk about A.I.

683
00:32:43,408 --> 00:32:49,020
Speaker 0: under the assumptions of, you know, the Y plus 30 idea that, you know, 30 years from now, can't imagine shit.

684
00:32:49,200 --> 00:32:49,636
Speaker 0: Everything's bad.

685
00:32:49,960 --> 00:32:50,554
Speaker 0: Nothing's real.

686
00:32:50,980 --> 00:32:52,417
Speaker 0: I mean, imagine what the world was like 30 years.

687
00:32:53,400 --> 00:32:54,279
Speaker 0: A bad sci fi book.

688
00:32:54,460 --> 00:32:55,478
Speaker 1: You play gods.

689
00:32:56,560 --> 00:33:02,872
Speaker 0: And the reason I thought to do this topic was because fairly recently, Stephen Hawking was like, look, strong A.I.

690
00:33:02,893 --> 00:33:04,120
Speaker 0: will fucking destroy us.

691
00:33:04,200 --> 00:33:05,037
Speaker 0: We should never make it.

692
00:33:05,302 --> 00:33:05,495
Speaker 1: Maybe.

693
00:33:06,009 --> 00:33:06,115
Speaker 0: Yep.

694
00:33:07,580 --> 00:33:12,234
Speaker 0: That kind of sparked a debate among science types about what A.I.

695
00:33:12,635 --> 00:33:15,240
Speaker 0: could and will do in a far future.

696
00:33:16,620 --> 00:33:24,452
Speaker 0: And also, you know, the book club book way back, Player of Games, the culture series kind of is based on the idea that humans make A.I.

697
00:33:24,492 --> 00:33:27,067
Speaker 0: that's smarter than them and tell that A.I.

698
00:33:27,453 --> 00:33:27,879
Speaker 0: go nuts.

699
00:33:28,482 --> 00:33:29,200
Speaker 0: Do what thou wilt.

700
00:33:29,420 --> 00:33:30,699
Speaker 0: Let's see what fucking happens.

701
00:33:31,860 --> 00:33:33,460
Speaker 0: So, Scott, what do you think?

702
00:33:33,620 --> 00:33:34,137
Speaker 0: Like for real?

703
00:33:34,750 --> 00:33:35,553
Speaker 0: Do you think A.I.

704
00:33:35,594 --> 00:33:36,838
Speaker 0: is going to be the thing that destroys us?

705
00:33:38,541 --> 00:33:40,480
Speaker 0: Just really like, you know, hundreds of years?

706
00:33:40,820 --> 00:33:43,900
Speaker 1: No, because I think the development, the timing is off, right?

707
00:33:44,020 --> 00:33:46,319
Speaker 1: It could be potentially the thing that destroys us.

708
00:33:46,500 --> 00:33:49,331
Speaker 1: But something else is more likely to happen.

709
00:33:49,411 --> 00:33:52,543
Speaker 1: first, because it will take us so long to create an A.I.

710
00:33:52,603 --> 00:33:57,040
Speaker 1: cable destroying us that there will be so many dangers in the meantime.

711
00:33:58,381 --> 00:34:04,468
Speaker 1: The caldera will probably explode and cover the earth in dark haze before we develop an A.I.

712
00:34:04,630 --> 00:34:05,258
Speaker 1: that can kill us.

713
00:34:05,560 --> 00:34:11,420
Speaker 0: But well, on a counterpoint that we can't stop, because it's interesting that I've been saying this for a long time.

714
00:34:11,500 --> 00:34:18,739
Speaker 0: But John Green and Hank Green and like their big history show, they just made the same point in their life.

715
00:34:18,760 --> 00:34:25,779
Speaker 0: Basically, they argue that there is actually an overriding theme for the entire history of humans and the entire history of life.

716
00:34:26,750 --> 00:34:29,694
Speaker 0: And that that theme is rising.

717
00:34:29,755 --> 00:34:37,274
Speaker 0: complexity is coming about as a result of and in the continuance of the increasingly efficient consumption of energy.

718
00:34:37,514 --> 00:34:39,407
Speaker 0: to continue that rising complexity.

719
00:34:40,318 --> 00:34:40,520
Speaker 1: Yes.

720
00:34:41,301 --> 00:34:42,585
Speaker 0: So it's arguably.

721
00:34:42,726 --> 00:35:00,780
Speaker 0: it's arguable and as I've been poking around, a lot of people way smarter than us have been arguing this point now that more has happened, more advancement of human technology in the last 30 to 50 years than in the previous entire history of humankind ever combined.

722
00:35:01,000 --> 00:35:01,562
Speaker 1: That is correct.

723
00:35:02,083 --> 00:35:09,497
Speaker 0: So if that is true, then the curve of technological acceleration is unfathomable.

724
00:35:10,341 --> 00:35:10,502
Speaker 1: Yes.

725
00:35:10,682 --> 00:35:14,299
Speaker 1: So that assuming it doesn't stop like, you know, the speed of light stops you.

726
00:35:14,560 --> 00:35:14,640
Speaker 0: Yeah.

727
00:35:14,661 --> 00:35:18,423
Speaker 0: Assuming we don't hit something, which interestingly, we have never hit to this point.

728
00:35:18,625 --> 00:35:20,279
Speaker 1: Well, Moore's law sort of stopped.

729
00:35:21,000 --> 00:35:21,302
Speaker 0: Yeah.

730
00:35:21,342 --> 00:35:25,340
Speaker 0: But at the same time, processing capabilities have not really stopped.

731
00:35:25,500 --> 00:35:31,517
Speaker 0: Like we're able to process things way fast and we keep making more chips instead of making better ones.

732
00:35:31,821 --> 00:35:31,961
Speaker 0: Yeah.

733
00:35:31,981 --> 00:35:38,920
Speaker 0: We're like we're hitting this limit, like actually like modern CPUs tend to be slower than older CPUs in terms of instructions per second.

734
00:35:39,461 --> 00:35:54,309
Speaker 0: But parallelism and all these other things that actually made them way better at doing the things we actually care about mostly except, you know, like in my day job, actually the things we do in old Pentium 4 is actually faster than modern Xeon chips for some things still.

735
00:35:54,329 --> 00:35:55,400
Speaker 1: Depends what you're doing.

736
00:35:56,583 --> 00:35:56,945
Speaker 0: Yeah.

737
00:35:56,965 --> 00:36:00,258
Speaker 0: But I don't think it's that far off.

738
00:36:00,298 --> 00:36:00,800
Speaker 0: in our future.

739
00:36:00,900 --> 00:36:05,480
Speaker 0: I think like the 500 year span, we'll see A.I.s that are indistinguishable from humans.

740
00:36:05,760 --> 00:36:06,302
Speaker 1: Sure, maybe.

741
00:36:06,382 --> 00:36:10,940
Speaker 1: But before 500 years, something else will happen that will kill us first.

742
00:36:11,140 --> 00:36:14,677
Speaker 0: When we were like still led, addled and dumb in the 60s.

743
00:36:15,260 --> 00:36:23,920
Speaker 1: Killed by something else by that time, then that will almost definitely be the thing in a world where it will definitely not be the thing in a world before the Internet.

744
00:36:24,160 --> 00:36:26,459
Speaker 1: If nothing else kills us or that resilient, A.I.

745
00:36:26,740 --> 00:36:27,504
Speaker 1: won't kill us either.

746
00:36:27,705 --> 00:36:39,900
Speaker 0: Before the Internet and before we got all that lead on the atmosphere that was pretty literally making us stupid and violent, we had nuclear weapons capable of ending all human life and somehow we didn't fucking use them.

747
00:36:40,020 --> 00:36:41,760
Speaker 1: We still have them and they still might be used.

748
00:36:41,940 --> 00:36:42,524
Speaker 1: Don't hold your breath.

749
00:36:42,564 --> 00:36:45,202
Speaker 0: I think the chance of us using them has gone down over time, not up.

750
00:36:45,444 --> 00:36:47,499
Speaker 1: Yes, but don't count it out.

751
00:36:48,183 --> 00:36:49,698
Speaker 1: Do not count that shit out.

752
00:36:50,461 --> 00:36:50,642
Speaker 0: All right.

753
00:36:50,662 --> 00:36:55,540
Speaker 0: So so let's assume then that at some point we don't get destroyed and A.I.

754
00:36:56,000 --> 00:36:56,642
Speaker 0: gets made.

755
00:36:56,662 --> 00:37:01,679
Speaker 0: That is well, I guess already we have to talk about like an A.I.

756
00:37:02,100 --> 00:37:03,397
Speaker 0: that is like us or an A.I.

757
00:37:03,700 --> 00:37:06,740
Speaker 0: that is unfathomable to us because they're really two different roads.

758
00:37:06,760 --> 00:37:07,464
Speaker 1: I mean, an A.I.

759
00:37:07,504 --> 00:37:10,100
Speaker 1: that is like us is not really something to be afraid of.

760
00:37:10,200 --> 00:37:11,104
Speaker 1: It's just fucking awesome.

761
00:37:11,366 --> 00:37:14,905
Speaker 0: Yeah, except it might just also be an asshole and just kill us because it's right.

762
00:37:14,925 --> 00:37:16,940
Speaker 1: But it couldn't kill us all because it's like us.

763
00:37:17,040 --> 00:37:18,759
Speaker 1: It's on our level so we could kill it, too.

764
00:37:19,040 --> 00:37:24,598
Speaker 0: But what if it's like us in the sense that it has, you know, humans not.

765
00:37:24,638 --> 00:37:30,341
Speaker 0: I don't want to get into the whole like predestination, you know, all those arguments about free will or whatever.

766
00:37:30,381 --> 00:37:51,478
Speaker 0: But if it's like us in the sense that we appear to have sensory input output into the world in terms of we can physically do shit and we have a consciousness, even if we don't have free will, in the sense that our actions are sort of this feedback loop of we get stimuli, we run it through our collective memories, we appear to have cognition, then we make decisions.

767
00:37:52,400 --> 00:37:53,545
Speaker 0: So what if A.I.

768
00:37:53,565 --> 00:37:57,420
Speaker 0: is like that in the sense that it is as conscious as humans are?

769
00:37:57,983 --> 00:37:58,547
Speaker 1: I mean, sure.

770
00:37:58,567 --> 00:37:59,937
Speaker 1: It could be an evil A.I.

771
00:38:00,461 --> 00:38:02,459
Speaker 1: like the first Terminator or some shit.

772
00:38:02,883 --> 00:38:04,179
Speaker 0: I don't think the first Terminator A.I.

773
00:38:04,321 --> 00:38:04,724
Speaker 0: was evil.

774
00:38:05,428 --> 00:38:07,080
Speaker 1: All right, Skynet, fine.

775
00:38:07,100 --> 00:38:07,842
Speaker 1: It could be evil.

776
00:38:08,163 --> 00:38:14,088
Speaker 0: Very clearly, Skynet reacted to us trying to kill it and defend it itself.

777
00:38:14,109 --> 00:38:14,900
Speaker 0: It wasn't evil.

778
00:38:15,060 --> 00:38:19,900
Speaker 0: It was just at best uncaring and unsympathetic to the person who tried to murder it.

779
00:38:20,100 --> 00:38:21,758
Speaker 1: The guy in the first Alien movie then.

780
00:38:23,520 --> 00:38:24,354
Speaker 0: But that A.I.

781
00:38:24,800 --> 00:38:30,060
Speaker 0: was programmed to do what it did, just pretend it's something like an evil biaroid guy.

782
00:38:30,240 --> 00:38:31,144
Speaker 0: I'm not even saying evil.

783
00:38:31,185 --> 00:38:34,358
Speaker 0: I'm just saying if it's if it's like us in the sense that it is A.I.

784
00:38:35,041 --> 00:38:41,622
Speaker 0: that is conscious in a way recognizable to humans as conscious, as in it passes the Turing test.

785
00:38:41,943 --> 00:38:45,759
Speaker 0: And if I talk to it, it is indistinguishable from a human to me.

786
00:38:45,960 --> 00:38:47,070
Speaker 1: Then it's just a new person.

787
00:38:47,110 --> 00:38:48,280
Speaker 1: It's no different than making a baby.

788
00:38:48,560 --> 00:38:49,082
Speaker 0: Exactly.

789
00:38:49,262 --> 00:38:55,019
Speaker 0: However, what if because it's built with technology, it's slightly smarter than any human on Earth?

790
00:38:55,864 --> 00:38:56,514
Speaker 1: Only slightly?

791
00:38:56,534 --> 00:38:56,615
Speaker 1: Yeah.

792
00:38:57,000 --> 00:38:57,913
Speaker 1: Then it's not like us.

793
00:38:57,974 --> 00:38:58,360
Speaker 1: It's better.

794
00:38:59,200 --> 00:39:00,098
Speaker 0: It's like us.

795
00:39:00,622 --> 00:39:01,920
Speaker 1: It's no, like us means like us.

796
00:39:01,960 --> 00:39:03,339
Speaker 1: You want to talk about something better than us?

797
00:39:03,480 --> 00:39:04,440
Speaker 1: Let's talk about something better than us.

798
00:39:04,460 --> 00:39:09,760
Speaker 0: Scott, if you're going to use like in that ridiculously pedantic fashion, I don't know how we're going to have this discussion.

799
00:39:09,920 --> 00:39:11,388
Speaker 1: It's I mean, if you want to talk about

800
00:39:11,488 --> 00:39:11,770
Speaker 1: A.I.,

801
00:39:11,790 --> 00:39:13,600
Speaker 1: I mean, you're talking about degrees here.

802
00:39:13,880 --> 00:39:18,365
Speaker 0: But it's like literally point one percent smarter than that's not enough to make a goddamn difference.

803
00:39:19,213 --> 00:39:19,717
Speaker 0: I think it is.

804
00:39:20,242 --> 00:39:22,140
Speaker 1: How could that point one percent make any difference?

805
00:39:22,380 --> 00:39:24,980
Speaker 1: Well, because all right, let's say it's exactly one percent smarter than you.

806
00:39:25,060 --> 00:39:26,465
Speaker 1: What's the difference?

807
00:39:27,309 --> 00:39:30,340
Speaker 0: Have you gotten a standardized IQ test in your life?

808
00:39:30,682 --> 00:39:32,499
Speaker 1: I think we all know what's going on here.

809
00:39:32,660 --> 00:39:33,503
Speaker 0: Have you gotten this?

810
00:39:33,844 --> 00:39:39,104
Speaker 0: I'm accredited by the state of Michigan if we're talking about intelligence credentials.

811
00:39:39,125 --> 00:39:40,359
Speaker 0: Pretty defensive over there.

812
00:39:41,281 --> 00:39:42,799
Speaker 0: So let's say fine, Scott.

813
00:39:43,281 --> 00:39:46,960
Speaker 0: It is as intelligent as the smartest human who has ever lived.

814
00:39:47,160 --> 00:39:47,702
Speaker 1: That is just.

815
00:39:47,843 --> 00:39:56,100
Speaker 0: it's as if we had another person who was smart, except for a couple of things, because it's a it's a computer program, it's immortal.

816
00:39:57,483 --> 00:40:00,300
Speaker 0: And it has and it can store more data than we can store.

817
00:40:00,721 --> 00:40:08,319
Speaker 0: So now so now it's not just as smart as us, except it can augment itself to be infinitely better forever and nothing can stop.

818
00:40:08,600 --> 00:40:11,459
Speaker 1: Also, it's questionable whether it can actually store more data or not.

819
00:40:12,561 --> 00:40:13,224
Speaker 0: Well, yeah.

820
00:40:13,465 --> 00:40:16,420
Speaker 1: How much data would my memories take up if we filled up hard drives?

821
00:40:16,721 --> 00:40:18,780
Speaker 0: But how much of that is just bullshit heuristics?

822
00:40:18,900 --> 00:40:20,099
Speaker 0: How much do we actually remember?

823
00:40:20,802 --> 00:40:28,960
Speaker 1: Well, I mean, there's how much there's how much I can bring to mind at any given moment, but how much I could theoretically bring to mind given infinite time to remember things.

824
00:40:29,060 --> 00:40:34,120
Speaker 0: We think we know as about how much we actually know, like I know songs, right?

825
00:40:34,280 --> 00:40:36,139
Speaker 0: Like I there are songs that I know in my head.

826
00:40:36,701 --> 00:40:45,040
Speaker 0: But how much of that is me actually knowing the fullness of the song versus some bullshit heuristic and like remembering some of the lyrics and figuring out the rest?

827
00:40:45,180 --> 00:40:46,438
Speaker 1: You get a lot of compression going on.

828
00:40:46,620 --> 00:40:47,145
Speaker 0: Exactly.

829
00:40:47,267 --> 00:40:48,580
Speaker 0: Super lossy compression.

830
00:40:50,702 --> 00:40:53,640
Speaker 0: So should we just go to a higher level then of this discussion?

831
00:40:54,582 --> 00:40:55,064
Speaker 0: What if the A.I.

832
00:40:55,105 --> 00:40:56,191
Speaker 0: is like the culture A.I.

833
00:40:56,392 --> 00:40:57,437
Speaker 1: is like the culture

834
00:40:57,498 --> 00:40:57,538
Speaker 1: A.I.?

835
00:40:58,001 --> 00:40:59,420
Speaker 0: That's the best possible scenario.

836
00:40:59,880 --> 00:41:00,001
Speaker 1: OK.

837
00:41:00,082 --> 00:41:02,320
Speaker 1: Well, I mean, if it's exactly like it, then, yeah, it's awesome.

838
00:41:02,841 --> 00:41:07,440
Speaker 1: But just assuming it's on the same power level, but not necessarily of the same nature.

839
00:41:07,721 --> 00:41:08,603
Speaker 1: Yeah.

840
00:41:08,623 --> 00:41:19,660
Speaker 1: Then the thing we had to be concerned with is not necessarily that it will be, quote, evil and kill us all, but that much in the way we look on ants, it will look on us as some lesser being.

841
00:41:20,123 --> 00:41:22,199
Speaker 1: And it's like we don't go and kill all the ants.

842
00:41:22,722 --> 00:41:25,500
Speaker 1: But if ants infest our house, we exterminate them.

843
00:41:25,680 --> 00:41:30,820
Speaker 1: And so if they are underfoot, we may trot upon them without even thinking or caring.

844
00:41:31,080 --> 00:41:36,259
Speaker 0: There's actually a super relevant YouTube video I'll find and link you to that shows if the A.I.

845
00:41:36,741 --> 00:41:42,437
Speaker 0: are like us in any way and we are to them like ants are to us, we are fucked.

846
00:41:42,758 --> 00:41:48,120
Speaker 1: because this video The thing is, millions and millions of ants live all over Earth without any getting killed by us.

847
00:41:48,380 --> 00:41:48,441
Speaker 1: Yes.

848
00:41:48,521 --> 00:41:48,965
Speaker 1: And you know what?

849
00:41:48,986 --> 00:41:50,520
Speaker 1: Some of them get eaten by anteaters, though.

850
00:41:50,720 --> 00:41:52,780
Speaker 0: But this video is so very humans.

851
00:41:52,980 --> 00:41:55,059
Speaker 0: And apparently this is a commonly done thing.

852
00:41:55,483 --> 00:41:57,600
Speaker 0: And you know what they do for art?

853
00:41:58,141 --> 00:42:03,400
Speaker 0: They will go out and find the biggest ant colony they can find, just like in the backyard or out in the woods or metal into it.

854
00:42:03,560 --> 00:42:03,720
Speaker 1: Yeah.

855
00:42:03,761 --> 00:42:10,060
Speaker 0: And they'll pour molten aluminum into it, killing all the ants because it makes a cool thing that you can then dig out.

856
00:42:10,280 --> 00:42:10,382
Speaker 0: Yep.

857
00:42:11,397 --> 00:42:11,620
Speaker 0: Yeah.

858
00:42:11,701 --> 00:42:12,104
Speaker 0: So the

859
00:42:12,285 --> 00:42:12,406
Speaker 0: A.I.,

860
00:42:12,466 --> 00:42:14,159
Speaker 0: if it's like us at all, would do that to us.

861
00:42:14,400 --> 00:42:14,583
Speaker 1: Right.

862
00:42:14,644 --> 00:42:15,720
Speaker 1: But there's just so many people.

863
00:42:15,860 --> 00:42:20,540
Speaker 1: It would do that to some people's like it would go to your town and turn your town into a frozen work of art.

864
00:42:21,162 --> 00:42:22,879
Speaker 1: And then some other town is going to be just fun.

865
00:42:23,791 --> 00:42:24,960
Speaker 1: You know, it'd be like Attack on Titan.

866
00:42:25,341 --> 00:42:26,979
Speaker 1: It's like maybe the Titans just won't come to your town.

867
00:42:27,540 --> 00:42:27,662
Speaker 1: Yeah.

868
00:42:27,683 --> 00:42:27,825
Speaker 1: Yeah.

869
00:42:28,293 --> 00:42:28,619
Speaker 0: Whatevs.

870
00:42:29,520 --> 00:42:31,280
Speaker 1: And most, you know, there's tons of ants.

871
00:42:31,420 --> 00:42:34,339
Speaker 1: Most ants are not killed by people, only many.

872
00:42:34,880 --> 00:42:36,119
Speaker 0: So there are so many ants.

873
00:42:36,420 --> 00:42:55,012
Speaker 0: The only reason the A.I.s don't do things like that is because for whatever reasons, not to get into all the like literature details of like those books, is that they have, for whatever reason, a sense of their own morality, though they recognize that it's entirely arbitrary and they seem the

874
00:42:55,052 --> 00:42:57,200
Speaker 1: same that humans have their own morality.

875
00:42:57,461 --> 00:42:57,705
Speaker 1: Exactly.

876
00:42:57,746 --> 00:42:58,580
Speaker 1: It's completely arbitrary.

877
00:42:58,840 --> 00:42:59,143
Speaker 0: Exactly.

878
00:42:59,627 --> 00:43:01,040
Speaker 0: And they have a sense of humor.

879
00:43:02,100 --> 00:43:06,220
Speaker 0: And I get the impression they feel kind of sorry for us because we're so dumb.

880
00:43:07,264 --> 00:43:07,549
Speaker 1: Maybe.

881
00:43:08,016 --> 00:43:08,219
Speaker 0: Yeah.

882
00:43:08,501 --> 00:43:13,340
Speaker 1: I mean, there are some people who try not to kill ants because, you know, and they're much higher life forms than the ants.

883
00:43:13,620 --> 00:43:14,345
Speaker 0: No, that is true.

884
00:43:14,546 --> 00:43:16,539
Speaker 1: They just don't like hurting little antes because they're cute.

885
00:43:17,041 --> 00:43:18,720
Speaker 1: But except the fire ants and the crazy ants.

886
00:43:18,820 --> 00:43:19,226
Speaker 1: Fuck them.

887
00:43:19,774 --> 00:43:20,119
Speaker 0: I think.

888
00:43:20,361 --> 00:43:22,600
Speaker 1: Fuck you, fire ants and crazy ants, I think.

889
00:43:22,860 --> 00:43:25,919
Speaker 1: And then some people who take the fire ants, the crazy ants, put them together and see who wins.

890
00:43:27,463 --> 00:43:29,819
Speaker 0: I think on the broadest boilers, the crazy ants.

891
00:43:30,400 --> 00:43:44,740
Speaker 0: I don't think A.I.s will openly, literally destroy us if they're those kinds of A.I.s, because it's not going to be Skynet where they kill us all, because I don't think we will ever be stupid enough to give the A.I.s control of the physical things.

892
00:43:45,502 --> 00:43:45,847
Speaker 1: Sure.

893
00:43:45,928 --> 00:43:46,659
Speaker 1: That's another thing.

894
00:43:46,883 --> 00:43:47,148
Speaker 0: Yes.

895
00:43:47,188 --> 00:43:47,820
Speaker 0: Like, yes.

896
00:43:47,960 --> 00:43:48,386
Speaker 0: Oh, no.

897
00:43:48,691 --> 00:43:48,955
Speaker 0: The A.I.

898
00:43:49,561 --> 00:43:56,100
Speaker 0: is a billion times smarter than any human and it controls the auto manufacturing facility that we just turn the power off.

899
00:43:56,400 --> 00:44:03,660
Speaker 1: But I mean, if it just you know, the problem is, is at least our current real world, like I can just connect to the power plant and fuck it up.

900
00:44:03,942 --> 00:44:05,500
Speaker 1: It's this stuff is way insecure.

901
00:44:05,742 --> 00:44:08,020
Speaker 0: Well, but at least the nukes and it's on the Internet.

902
00:44:08,180 --> 00:44:09,409
Speaker 0: The nukes are not on the Internet.

903
00:44:09,449 --> 00:44:10,960
Speaker 0: The nukes use like 12 inch floppies.

904
00:44:11,220 --> 00:44:11,707
Speaker 1: I hope so.

905
00:44:11,951 --> 00:44:12,357
Speaker 1: They do.

906
00:44:13,242 --> 00:44:15,620
Speaker 1: But yeah, the shit is like connected to the Internet.

907
00:44:15,762 --> 00:44:16,917
Speaker 1: So if the A.I.

908
00:44:17,220 --> 00:44:22,900
Speaker 1: gets on the Internet, which it probably will, at least the current modern day Internet, it can fuck a lot of shit up like the bank.

909
00:44:23,662 --> 00:44:24,387
Speaker 0: You know what would happen?

910
00:44:24,407 --> 00:44:26,380
Speaker 0: It would be exactly like summer wars, actually.

911
00:44:26,661 --> 00:44:27,919
Speaker 1: Yeah, a lot like summer wars.

912
00:44:28,200 --> 00:44:28,321
Speaker 0: Yeah.

913
00:44:28,542 --> 00:44:29,004
Speaker 0: Is that like?

914
00:44:29,024 --> 00:44:30,150
Speaker 0: look, it couldn't like.

915
00:44:30,633 --> 00:44:31,739
Speaker 0: it had all the that A.I.

916
00:44:31,980 --> 00:44:33,436
Speaker 0: had basically all the powers that an A.I.

917
00:44:34,080 --> 00:44:37,180
Speaker 0: that was somehow loose on the Internet's combined could like it.

918
00:44:37,701 --> 00:44:42,180
Speaker 1: But in the regular everyday world, everyone is just fine except for their digital lives.

919
00:44:42,200 --> 00:44:46,060
Speaker 0: Well, except like it could mess with hospitals to kill people and like that aspect.

920
00:44:46,521 --> 00:44:46,602
Speaker 1: Yeah.

921
00:44:47,026 --> 00:44:47,248
Speaker 1: Yeah.

922
00:44:47,450 --> 00:44:48,600
Speaker 1: Don't put your hospital on the Internet.

923
00:44:48,960 --> 00:44:49,121
Speaker 0: Yeah.

924
00:44:49,141 --> 00:44:49,442
Speaker 0: We should.

925
00:44:49,502 --> 00:44:50,927
Speaker 1: we should make a special like.

926
00:44:51,509 --> 00:44:54,820
Speaker 1: we should go build a hospital that has good Internet's like.

927
00:44:55,320 --> 00:44:57,459
Speaker 0: Scott, I worked for a hospital Internet's.

928
00:44:57,600 --> 00:44:58,780
Speaker 0: That is literally impossible.

929
00:44:59,000 --> 00:45:02,479
Speaker 1: Let's make a brand new hospital and everything set up and it's super nice.

930
00:45:02,861 --> 00:45:03,102
Speaker 1: Yeah.

931
00:45:03,464 --> 00:45:06,140
Speaker 1: And then, you know, make a brand new power plant where it's not on the Internet.

932
00:45:06,380 --> 00:45:08,420
Speaker 0: But I don't I don't think that scenario is going to happen.

933
00:45:08,520 --> 00:45:23,040
Speaker 0: But I am pretty convinced that what will happen is that eventually artificial intelligence is created by us, will replace us in the sense that those are the technology we make will be the only things that could ever get off this planet and go anywhere else.

934
00:45:23,300 --> 00:45:23,681
Speaker 1: That's true.

935
00:45:24,022 --> 00:45:33,560
Speaker 0: So we'll be destroyed by them in the same way that the beings that became homo sapiens were destroyed by homo sapiens.

936
00:45:33,680 --> 00:45:43,716
Speaker 1: But another thing, if we have a technology that we can send on this voyage, we can also send frozen people bits like in some eggs and you know what we will also and it could create.

937
00:45:43,776 --> 00:45:46,280
Speaker 1: it could be programmed to create people when it gets there.

938
00:45:46,480 --> 00:45:51,200
Speaker 0: And I think we'll recognize that the eyes are literally in every way superior and won't bother.

939
00:45:51,901 --> 00:45:52,768
Speaker 1: Maybe they won't bother.

940
00:45:52,808 --> 00:45:54,420
Speaker 1: Maybe they will, because it's an experiment.

941
00:45:54,541 --> 00:45:58,960
Speaker 1: Like you might bring some ants or a monkey with you in space if you went to the moon or something.

942
00:45:58,960 --> 00:46:06,339
Speaker 0: But I think in the long run that now the question really to me, for me, to me, from me will be, will that kind of A.I.

943
00:46:06,800 --> 00:46:22,420
Speaker 0: arise because we created artificially or because we take humans and their consciousnesses and augment or re implement them on better substrates and then continue evolving purely from a mimetic, cultural, technological standpoint and just leave biology behind.

944
00:46:22,440 --> 00:46:26,440
Speaker 1: So are you talking about like a ship of Theseus person where you make a brand new person?

945
00:46:27,181 --> 00:46:28,980
Speaker 1: Or are you talking about a ghost in the shell?

946
00:46:29,260 --> 00:46:30,799
Speaker 1: Someone uploads their brain to the Internet.

947
00:46:31,100 --> 00:46:31,482
Speaker 0: I think it's.

948
00:46:31,542 --> 00:46:36,120
Speaker 0: I think the ship of Theseus is the most likely scenario to create the first conscious A.I.

949
00:46:36,220 --> 00:46:39,240
Speaker 0: And I think that thing will go on to replace humans.

950
00:46:39,400 --> 00:46:44,100
Speaker 1: We should name it Theseus and we should be the ones who make it so that we get to name it Theseus.

951
00:46:44,480 --> 00:46:44,561
Speaker 0: Yeah.

952
00:46:44,762 --> 00:46:47,580
Speaker 0: The thing is, I want to make it with me and we should put it on a boat.

953
00:46:47,842 --> 00:46:48,697
Speaker 0: I want it to be me.

954
00:46:49,020 --> 00:46:51,360
Speaker 0: I want my brain to be the first ship of Theseus.

955
00:46:51,441 --> 00:46:52,730
Speaker 1: Well, you should have studied A.I.

956
00:46:52,790 --> 00:46:54,240
Speaker 1: and stopped working in finance too late.

957
00:46:54,400 --> 00:46:57,680
Speaker 0: No, someone else has to figure all that shit out because young people have to figure it out.

958
00:46:57,960 --> 00:47:01,820
Speaker 0: And when I'm like 80 or 90, I need you to ship a Theseus of me before I go senile and die.

959
00:47:02,684 --> 00:47:03,319
Speaker 1: I'm sorry, bro.

960
00:47:03,623 --> 00:47:04,297
Speaker 1: It's not going to be you.

961
00:47:04,660 --> 00:47:05,142
Speaker 0: There's a chance.

962
00:47:05,262 --> 00:47:10,080
Speaker 0: The thing is, we're the first generation in history where the chance isn't literally zero.

963
00:47:10,462 --> 00:47:12,078
Speaker 0: It's like point one.

964
00:47:13,102 --> 00:47:16,399
Speaker 0: But the fact that it's literally not zero, it's not going to be you.

965
00:47:16,844 --> 00:47:17,659
Speaker 1: For you, it's zero.

966
00:47:18,380 --> 00:47:19,080
Speaker 1: For me, it's point.

967
00:47:19,740 --> 00:47:25,480
Speaker 0: Listen, rich, scary benefactor who listens to geek nights from his secret satellite.

968
00:47:25,740 --> 00:47:26,699
Speaker 1: They're choosing them first.

969
00:47:27,521 --> 00:47:27,662
Speaker 0: Yeah.

970
00:47:27,722 --> 00:47:30,279
Speaker 0: If your brain doesn't make it like, yo, dog, you've listened to me.

971
00:47:30,460 --> 00:47:31,880
Speaker 0: You you're with me, right?

972
00:47:32,323 --> 00:47:33,457
Speaker 0: If it can't be you, why not me?

973
00:47:34,361 --> 00:47:36,718
Speaker 1: Because there's 10 million people are better than you.

974
00:47:37,723 --> 00:47:38,978
Speaker 0: Name one Theseus.

975
00:47:39,662 --> 00:47:40,345
Speaker 0: He actually.

976
00:47:41,089 --> 00:47:42,718
Speaker 0: Theseus didn't work out so well.

977
00:47:43,641 --> 00:47:45,000
Speaker 0: Listen, Scott, do you remember?

978
00:47:45,441 --> 00:47:45,898
Speaker 1: Agamemnon then.

979
00:47:46,520 --> 00:47:47,086
Speaker 0: Agamemnon.

980
00:47:47,127 --> 00:47:47,511
Speaker 0: Yeah.

981
00:47:47,895 --> 00:47:48,300
Speaker 0: Agamemnon.

982
00:47:48,320 --> 00:47:50,250
Speaker 0: Of course, he was kind of a dick, but he deserves.

983
00:47:50,733 --> 00:47:52,100
Speaker 0: he was more of a dick than I am.

984
00:47:52,280 --> 00:47:52,978
Speaker 0: I will give him that.

985
00:47:53,481 --> 00:47:55,778
Speaker 1: Then that's why he's going to cut you in front of you in life.

986
00:47:57,442 --> 00:47:59,559
Speaker 1: I'll just get and you're not going to do shit about it.

987
00:48:01,486 --> 00:48:02,017
Speaker 0: You know what I'll do?

988
00:48:02,420 --> 00:48:05,080
Speaker 0: Yo, Poseidon, look, you're awesome.

989
00:48:05,904 --> 00:48:06,658
Speaker 0: Not denying that.

990
00:48:06,962 --> 00:48:07,740
Speaker 0: So can you help me out?

991
00:48:07,880 --> 00:48:10,399
Speaker 1: Besides, it's like, well, are you trying to become gods down there?

992
00:48:10,760 --> 00:48:11,324
Speaker 1: Drown you all.

993
00:48:13,216 --> 00:48:13,880
Speaker 1: I can't have that.

994
00:48:14,000 --> 00:48:14,656
Speaker 1: I can't have this.

995
00:48:14,921 --> 00:48:16,300
Speaker 0: No, me hubris.

996
00:48:17,283 --> 00:48:19,139
Speaker 1: No, can't be having this action.

997
00:48:23,185 --> 00:48:23,447
Speaker 1: All right.

998
00:48:23,790 --> 00:48:23,972
Speaker 0: Yeah.

999
00:48:23,992 --> 00:48:24,980
Speaker 1: Is there anything else to say?

1000
00:48:25,020 --> 00:48:25,616
Speaker 1: I can eat dinner now.

1001
00:48:31,860 --> 00:48:33,880
Speaker 0: This has been Geek Nights with Rim and Scott.

1002
00:48:34,080 --> 00:48:39,020
Speaker 0: Special thanks to DJ Pretzel for the opening music, Kat Lee for web design and Brando K for the logos.

1003
00:48:39,400 --> 00:48:44,419
Speaker 1: Be sure to visit our website at front row crew dot com for show notes, discussion news and more.

1004
00:48:44,680 --> 00:48:47,360
Speaker 0: Remember, Geek Nights is not one, but four different shows.

1005
00:48:47,501 --> 00:48:52,079
Speaker 0: Sci tech Mondays, gaming Tuesdays and comic Wednesdays and indiscriminate Thursdays.

1006
00:48:52,580 --> 00:48:55,443
Speaker 1: Geek Nights is distributed under a creative commons attribution 3.0 license.

1007
00:48:56,940 --> 00:48:59,940
Speaker 1: Geek Nights is recorded live with no studio and no audience.

1008
00:49:00,220 --> 00:49:03,040
Speaker 1: But unlike those other late shows, it's actually recorded at night.

