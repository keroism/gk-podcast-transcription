1
00:00:09,580 --> 00:00:11,486
Speaker 1: It's Monday October 26 2020.

2
00:00:11,486 --> 00:00:13,312
Speaker 1: I'm rim.

3
00:00:14,034 --> 00:00:15,979
Speaker 1: I'm Scott and this is geek nights.

4
00:00:15,999 --> 00:00:19,399
Speaker 1: tonight We are talking about facial recognition technology.

5
00:00:20,881 --> 00:00:21,916
Speaker 0: Let's do this.

6
00:00:23,461 --> 00:00:23,963
Speaker 1: Tried to vote.

7
00:00:24,364 --> 00:00:26,551
Speaker 1: I do you know voting's big this year.

8
00:00:26,591 --> 00:00:30,204
Speaker 1: like a lot of people are voting who have never voted before a lot Of people are trying to vote early.

9
00:00:30,225 --> 00:00:37,952
Speaker 1: a lot of people who were gonna mail a ballot aren't mailing a ballot because the post office I hope these people vote You know next year and the year after.

10
00:00:37,972 --> 00:00:39,560
Speaker 0: yeah, well you're after that.

11
00:00:39,701 --> 00:00:48,660
Speaker 1: It's tricky though because some of them have never voted before but a lot of them are also voting early and avoiding You know post office problems, but I figured New York City the first day of early voting.

12
00:00:48,741 --> 00:00:51,558
Speaker 1: I wasn't going anywhere near that because I got a whole week where I can vote.

13
00:00:51,699 --> 00:00:54,318
Speaker 0: well election is the thing about the early voting right is I?

14
00:00:55,120 --> 00:01:07,386
Speaker 0: Finally, this is the first time during I think a major election that New York City State has had early voting because we passed it Thanks to our previous elections that where we didn't early vote we regular voted And we voted for people who may vote

15
00:01:07,427 --> 00:01:10,057
Speaker 1: in election years that aren't leap years guys like come on.

16
00:01:10,258 --> 00:01:11,924
Speaker 0: Yeah We voted that.

17
00:01:11,945 --> 00:01:15,800
Speaker 0: those years we were able to get you know the early voting to happen in the first place.

18
00:01:15,860 --> 00:01:18,280
Speaker 0: So normally the election day is a Tuesday, which is really stupid.

19
00:01:19,721 --> 00:01:21,436
Speaker 0: That's what they wrote in the freaking Constitution.

20
00:01:21,456 --> 00:01:21,779
Speaker 0: I don't know.

21
00:01:23,501 --> 00:01:29,220
Speaker 0: So then right people are fucking working on Tuesday, so as soon as early voting now you can vote on Saturday and Sunday.

22
00:01:29,401 --> 00:01:30,871
Speaker 0: That's how people can turn out.

23
00:01:30,912 --> 00:01:31,939
Speaker 0: they don't to go to fucking work.

24
00:01:32,282 --> 00:01:33,155
Speaker 0: Yeah, I can vote right.

25
00:01:33,175 --> 00:01:36,266
Speaker 1: especially Silver link to that talked about.

26
00:01:36,567 --> 00:01:38,574
Speaker 1: what are the specific reasons Americans don't vote?

27
00:01:38,634 --> 00:01:44,474
Speaker 1: and the number one reason across all Demographics was the wait in line to vote was more than an hour.

28
00:01:45,237 --> 00:01:52,100
Speaker 0: Yeah, also People you know there is technically a law that your boss is like let you go vote on Tuesday.

29
00:01:52,241 --> 00:01:53,855
Speaker 0: I don't do that work.

30
00:01:54,298 --> 00:01:58,393
Speaker 0: well You know people don't have the balls to tell their boss that and then fight.

31
00:01:58,614 --> 00:01:59,236
Speaker 0: and you're right.

32
00:01:59,256 --> 00:02:00,260
Speaker 0: whatever they just go to work.

33
00:02:00,532 --> 00:02:02,267
Speaker 0: Yeah More important than going to vote for them.

34
00:02:02,347 --> 00:02:09,493
Speaker 0: so but I figured I wouldn't go on the first day because I figured that's usually the day When people I knew Saturday and Sunday were gonna be a complete trash fire.

35
00:02:09,553 --> 00:02:14,908
Speaker 0: because you know for early voting Not only is this the only time you can vote on the weekend.

36
00:02:15,029 --> 00:02:21,216
Speaker 0: Yeah, right the early voting in New York has fewer locations than actual election day.

37
00:02:21,256 --> 00:02:31,480
Speaker 0: election day There's like tons of locations open because all these people who don't work on election day go to become poll workers Yeah, and that that way that enables them to open up more locations during the early voting.

38
00:02:31,901 --> 00:02:36,834
Speaker 0: It's only the locations that the BOE has staff for that can work all those days.

39
00:02:37,155 --> 00:02:38,919
Speaker 0: You know the old retired people and such.

40
00:02:40,242 --> 00:02:43,013
Speaker 0: And so there's a lot less locations with a lot less hours.

41
00:02:43,576 --> 00:02:47,149
Speaker 0: and because it's weekend This is the only time on the weekend less locations.

42
00:02:47,169 --> 00:02:48,574
Speaker 0: Yeah, it's gonna be a long line.

43
00:02:48,835 --> 00:02:51,063
Speaker 1: Yep, so Saturday We got there.

44
00:02:51,083 --> 00:02:56,707
Speaker 1: All right, right when it opened and the line we see a line like, okay, it doesn't look so bad It's socially distanced line.

45
00:02:56,888 --> 00:02:58,918
Speaker 0: people really really really want to vote right now.

46
00:02:59,320 --> 00:03:04,139
Speaker 1: So we walk down the line walk down the line keep walking down the line and then oh, there's the end of it.

47
00:03:04,603 --> 00:03:06,080
Speaker 1: Nope, it's like a PAX line.

48
00:03:06,140 --> 00:03:14,963
Speaker 1: It's around the corner and then around another corner and then and I then we realized it was like a four or five hour line And we're gonna vote a different day Mm-hmm.

49
00:03:15,003 --> 00:03:17,712
Speaker 0: So I what I did is I say I'm gonna vote early and often.

50
00:03:17,792 --> 00:03:23,800
Speaker 0: I sent in the mail-in ballot long ago But oh, I'm gonna go a New York, right?

51
00:03:24,000 --> 00:03:42,559
Speaker 0: So there were other people from other states who are my friends who were saying Oh New York is so dumb they don't open the absentee ballots till after and They talked about how someone showed up to vote in person who had received an absentee ballot in the mail At least according to the records, but they had not brought it with them to prove that they had not used it.

52
00:03:43,242 --> 00:03:54,806
Speaker 0: Therefore they had to fill out a provisional, you know affidavit Ballot because you know as soon as that state receives ballots in the mail from voters They open them right away.

53
00:03:55,308 --> 00:04:01,111
Speaker 1: New York doesn't open them till after the election Although no states open them in advance, which is a big problem.

54
00:04:01,673 --> 00:04:03,660
Speaker 0: Well, some states do they're including my friends.

55
00:04:03,700 --> 00:04:05,540
Speaker 1: Yeah, those states don't right.

56
00:04:05,640 --> 00:04:09,493
Speaker 0: So not voting them till after is actually kind of good because it.

57
00:04:09,974 --> 00:04:18,197
Speaker 0: the advantage is that I can now Vote by mail and then if I feel like voting in person, maybe I just changed my fucking mind.

58
00:04:18,216 --> 00:04:19,160
Speaker 0: I don't know, right?

59
00:04:19,360 --> 00:04:20,747
Speaker 0: Maybe I just want to make sure.

60
00:04:20,928 --> 00:04:25,509
Speaker 0: maybe I not sure if I'll be able to vote in person So I send one in in case I can't.

61
00:04:25,890 --> 00:04:28,300
Speaker 0: but then if I can I go and do that anyway.

62
00:04:28,842 --> 00:04:36,807
Speaker 0: and what happens is when they go on the day after the election or the night or whatever they start opening the absentee ballots once The polls close and they see wait a minute.

63
00:04:36,827 --> 00:04:37,529
Speaker 0: This is from Scott.

64
00:04:37,790 --> 00:04:40,980
Speaker 0: Scott voted in person near his town near his house.

65
00:04:41,421 --> 00:04:42,886
Speaker 0: We'll just throw this one in the trash.

66
00:04:42,987 --> 00:04:44,773
Speaker 1: Yeah, but some states can't even do that.

67
00:04:45,515 --> 00:04:59,560
Speaker 0: And maybe I'll get lucky and They won't throw it in the trash and I'll get to vote twice Once they crime I. They can't prove it because once they open that second envelope they because what they do is there's the there's the outside envelope.

68
00:04:59,600 --> 00:05:01,699
Speaker 1: They got an admission right here live on geek nights.

69
00:05:02,621 --> 00:05:10,492
Speaker 0: Yeah, they open that first day open the first envelope right and now now they're sitting at the second envelope Which is my name on it, right and those envelopes they've.

70
00:05:10,512 --> 00:05:12,520
Speaker 0: don't open them, but they verify each one.

71
00:05:12,601 --> 00:05:17,736
Speaker 0: So this is the part where they would say Oh this envelope says Scott on it and he voted at the school or whatever.

72
00:05:17,856 --> 00:05:20,619
Speaker 0: or the old folks home Throw this out and they throw out the whole thing.

73
00:05:20,860 --> 00:05:25,395
Speaker 0: Right if they open if they mess that up and they open that envelope and they throw up.

74
00:05:25,616 --> 00:05:26,760
Speaker 0: then they throw out the envelope.

75
00:05:26,780 --> 00:05:27,664
Speaker 0: This is Scott on it.

76
00:05:28,246 --> 00:05:29,170
Speaker 0: Right now.

77
00:05:29,210 --> 00:05:30,696
Speaker 0: They're left with an anonymous ballot.

78
00:05:30,736 --> 00:05:31,418
Speaker 0: They gotta count it.

79
00:05:32,343 --> 00:05:33,652
Speaker 0: Yeah, right GG.

80
00:05:33,712 --> 00:05:34,578
Speaker 0: I got to vote twice.

81
00:05:34,658 --> 00:05:41,217
Speaker 0: maybe If they fuck up but legally I am allowed to both mail it in and then vote in person.

82
00:05:41,257 --> 00:05:42,180
Speaker 1: so check your state laws.

83
00:05:42,221 --> 00:05:42,695
Speaker 1: I'll just go.

84
00:05:43,903 --> 00:05:45,080
Speaker 1: What check your state laws?

85
00:05:46,422 --> 00:05:48,250
Speaker 1: Different cuz America is a really stupid country.

86
00:05:48,912 --> 00:05:50,840
Speaker 0: Yeah, they explicitly allowed that in New York.

87
00:05:50,940 --> 00:05:57,965
Speaker 0: So if I see that there's an opportunity to go vote in person and it's not crowded or anything and there's no line I'll go over and do it.

88
00:05:58,125 --> 00:06:01,240
Speaker 0: And if no such opportunity presents itself, I already voted.

89
00:06:01,922 --> 00:06:13,460
Speaker 1: The importance of this mode is not so much in New York cuz New York's gonna go blue no matter what but Kind of elections that got us early voting in the first place.

90
00:06:13,500 --> 00:06:19,240
Speaker 1: Oh, but we have a sitting president who won't commit to the peaceful transfer of power and the Supreme Court has just been destroyed.

91
00:06:19,683 --> 00:06:36,323
Speaker 1: so we're in a situation where if election night and the day after aren't a clear Biden victory then court cases and nonsense could actually overturn the election and cause a lot of Drama, the more votes get counted the day of the election the better Mmm, like.

92
00:06:36,363 --> 00:06:39,754
Speaker 1: that's a really important thing, especially in certain states like Pennsylvania.

93
00:06:39,814 --> 00:06:42,543
Speaker 1: that could determine the entire election Mm-hmm.

94
00:06:43,386 --> 00:06:52,185
Speaker 1: But yeah, so we're gonna go either like in the middle of a work day Like tomorrow like one day this week or they alternate the days like some days It opens at like 7 a.m.

95
00:06:52,225 --> 00:06:54,875
Speaker 1: And closes at 4 and some days that opens is like 11 a.m.

96
00:06:55,156 --> 00:06:55,858
Speaker 1: And closes like 8 p.m.

97
00:06:57,240 --> 00:07:04,896
Speaker 0: Is that the poll workers can't work from 7 to 8 But they want to give some people who can go late an opportunity and some people can go early.

98
00:07:04,916 --> 00:07:10,476
Speaker 1: and oh from what I heard The people have been stuck at the polling places every night for like four or five extra hours.

99
00:07:10,517 --> 00:07:12,797
Speaker 1: because of how many people get in Line, they can't turn away.

100
00:07:13,442 --> 00:07:18,601
Speaker 0: But if you went from 7 to 8 they'd be there from 7 to midnight instead of 10 to midnight or 7.

101
00:07:18,601 --> 00:07:20,780
Speaker 1: You know that was always my strategy when I was a kid.

102
00:07:20,860 --> 00:07:24,941
Speaker 1: We'd go to Cedar Point the roller coaster park and the park would close at 11.

103
00:07:24,941 --> 00:07:30,960
Speaker 1: But all that meant was all the restaurants closed and all the lines don't let anyone else get in line.

104
00:07:31,080 --> 00:07:40,266
Speaker 0: So what we would always do that was that mean that was that's you know, you were taking advantage of their policy But what they should what they should do is have the packs policy Which is it's six o'clock.

105
00:07:40,327 --> 00:07:42,858
Speaker 0: a big wall of enforces just pushes everyone out immediately.

106
00:07:42,898 --> 00:07:46,852
Speaker 1: the end Yeah, you gotta let people vote and if they're too slow to process voting.

107
00:07:46,872 --> 00:07:47,474
Speaker 0: Yeah voting.

108
00:07:47,514 --> 00:07:48,960
Speaker 0: Yes, but amusement park?

109
00:07:49,341 --> 00:07:53,811
Speaker 0: Yeah, no, but if I ran the amusement park, I'd push you out But if I ran the voting I would not.

110
00:07:53,892 --> 00:08:04,734
Speaker 1: it was so great though like me and all my friends would literally like get into a multi-hour line at 1059 with a pile of like chicken fingers and Mountain Dew and we would be the lat.

111
00:08:04,754 --> 00:08:11,035
Speaker 1: we would try to be the last people in That line on purpose so we would be the last people walking out of the park at like whatever.

112
00:08:11,055 --> 00:08:11,938
Speaker 0: just let everyone cut you.

113
00:08:13,222 --> 00:08:14,614
Speaker 1: We would love anyone who came in after us.

114
00:08:14,635 --> 00:08:15,240
Speaker 1: We'd always like cut.

115
00:08:15,320 --> 00:08:17,715
Speaker 1: We were the last people in line having a chicken finger party.

116
00:08:17,775 --> 00:08:18,540
Speaker 1: That was like a thing.

117
00:08:20,302 --> 00:08:21,667
Speaker 0: I go for some chicken fingers right now.

118
00:08:21,888 --> 00:08:22,551
Speaker 1: I actually could too.

119
00:08:22,571 --> 00:08:23,133
Speaker 1: I'm pretty hungry.

120
00:08:23,474 --> 00:08:25,040
Speaker 1: So there is actually some big news.

121
00:08:25,402 --> 00:08:28,660
Speaker 1: It's not as big a news as some of you nerds are thinking but it is really big news.

122
00:08:28,981 --> 00:08:32,452
Speaker 1: They have confirmed that there is water on the moon.

123
00:08:33,034 --> 00:08:34,820
Speaker 1: We've suspected it for a long time.

124
00:08:35,842 --> 00:08:42,340
Speaker 1: there's a lot of evidence of it, but they basically have Absolutely confirmed that there is significant amounts of water on the moon.

125
00:08:43,082 --> 00:08:46,195
Speaker 0: If you collected all the water on the moon in a cup how big would the cup be?

126
00:08:46,295 --> 00:08:47,580
Speaker 1: it'd be bigger than a cup now.

127
00:08:47,741 --> 00:08:49,128
Speaker 0: It's like a swimming pool.

128
00:08:49,791 --> 00:08:51,540
Speaker 1: Well, you can read the article to see all the details.

129
00:08:51,580 --> 00:08:55,092
Speaker 1: But basically they found water in places.

130
00:08:55,132 --> 00:08:57,700
Speaker 1: They didn't expect to find water and it seems to be protected.

131
00:08:57,782 --> 00:09:08,080
Speaker 1: It's not a lot of water I think they said in those parts like on the sunny side of the moon like not on the poles There's the equivalent of a 12 ounce bottle of water per cubic meter of soil.

132
00:09:09,222 --> 00:09:10,351
Speaker 0: not a ton big meter.

133
00:09:10,351 --> 00:09:13,526
Speaker 1: 12 a Cubic yard is close enough.

134
00:09:13,566 --> 00:09:14,088
Speaker 1: you can think of it.

135
00:09:14,128 --> 00:09:18,919
Speaker 0: that and that's if you that's if you collect literally all the water which Is really hard to do even on earth right yeah?

136
00:09:19,040 --> 00:09:29,480
Speaker 0: The water is basically trapped in like either dust particles or maybe like if I if I pull a plant out of the ground or Something that's got water in it right, but like can I extract a hundred percent of the water from that plant?

137
00:09:29,780 --> 00:09:30,565
Speaker 0: It's pretty difficult.

138
00:09:30,585 --> 00:09:31,269
Speaker 0: Well.

139
00:09:31,289 --> 00:09:32,335
Speaker 0: Yeah, you can get pretty close.

140
00:09:32,596 --> 00:09:33,200
Speaker 0: I can get a lot.

141
00:09:33,523 --> 00:09:35,120
Speaker 0: I can get a lot, but you can't get all of it.

142
00:09:35,120 --> 00:09:48,254
Speaker 1: You're not gonna get the full 12 ounces, but basically they found water in a lot of places around the moon and it implies that there might be a larger source of water near the poles and It's just cool that they actually like this was.

143
00:09:48,636 --> 00:09:59,234
Speaker 1: this was saw I don't know decades and decades coming like the Soviets detected what they thought was water there in the late 70s But no one could ever prove it definitively until now.

144
00:09:59,635 --> 00:10:00,800
Speaker 0: so it's hard to prove something.

145
00:10:00,840 --> 00:10:03,490
Speaker 0: That's small and clear and you know yeah, also.

146
00:10:03,510 --> 00:10:06,380
Speaker 1: It's not like we've sent anyone back to the moon to do more science.

147
00:10:07,561 --> 00:10:10,812
Speaker 1: Yeah, we can only science it from with our more with our fancier technology.

148
00:10:10,852 --> 00:10:11,414
Speaker 0: right the only.

149
00:10:11,454 --> 00:10:14,303
Speaker 0: we've only explored the moon with you know 70s technology.

150
00:10:14,403 --> 00:10:20,840
Speaker 1: yeah, basically and now we're scanning and analyzing it with 2000s technology, maybe some 2010s technology.

151
00:10:22,002 --> 00:10:25,277
Speaker 1: Yep, so yeah, you can read the story like I'm not gonna like.

152
00:10:25,317 --> 00:10:25,960
Speaker 1: that's the news.

153
00:10:26,241 --> 00:10:28,186
Speaker 1: There's literally for real 100%.

154
00:10:28,186 --> 00:10:33,659
Speaker 1: there is significant amounts of water on the moon, and that's actually pretty cool.

155
00:10:34,863 --> 00:10:36,233
Speaker 0: Partly doesn't live up there that easily.

156
00:10:37,401 --> 00:10:45,615
Speaker 1: There's a lot of things about space like things about black holes things about other Galaxies that are like quasars and things that we just think of.

157
00:10:45,676 --> 00:11:02,460
Speaker 1: it's like common knowledge like oh, yeah We know that but a lot of things that are commonly understood and obvious about space We're only actually discovered in the last 30 or 40 years Like textbooks I had in elementary school did not have this information Mmm.

158
00:11:02,982 --> 00:11:03,083
Speaker 0: Well.

159
00:11:03,103 --> 00:11:04,368
Speaker 0: There's also just a lot of things.

160
00:11:04,408 --> 00:11:16,680
Speaker 0: It's like you know we mostly know We're really really sure and then there'll be a news That's like it's a hundred percent or at least as not a hundred percent But ninety nine point nine percent confirmed like us confirmed as can be now.

161
00:11:16,800 --> 00:11:23,896
Speaker 0: It's like aha, and it's like you know it's sort of a news But it sort of lacks its impact because you sort of already kind of knew.

162
00:11:23,916 --> 00:11:28,875
Speaker 1: you know yeah But there's also a hindsight bias where you feel like you always knew it's like it didn't.

163
00:11:28,895 --> 00:11:33,379
Speaker 1: our brains are real like And saying yeah, I always knew that.

164
00:11:34,183 --> 00:11:40,027
Speaker 0: But if you asked me yesterday all right, let's take bets if you Is there water on the moon?

165
00:11:40,087 --> 00:11:41,091
Speaker 0: I'd say water on the moon.

166
00:11:41,212 --> 00:11:41,453
Speaker 0: Yes.

167
00:11:41,553 --> 00:11:41,774
Speaker 0: Yes.

168
00:11:41,855 --> 00:11:43,887
Speaker 0: No you got a bet on yes, or no You got no choice.

169
00:11:44,089 --> 00:11:49,479
Speaker 0: I would have bet on yes I would have bet on yes, cuz I'd be like how many molecules counts one molecule of water?

170
00:11:49,519 --> 00:11:59,651
Speaker 0: yes Yeah, Neil Armstrong like you know like a drop of water from his sweat got out or so if you ask me Say a decade ago are there white holes alongside black holes.

171
00:11:59,691 --> 00:12:02,940
Speaker 1: I probably would have bet on yes, but we're increasingly leaning toward no.

172
00:12:03,822 --> 00:12:05,050
Speaker 0: Yeah, all right.

173
00:12:05,594 --> 00:12:08,048
Speaker 0: so a News I got here.

174
00:12:08,310 --> 00:12:15,595
Speaker 0: That's pretty big going around the tech peoples But is largely understood sometimes blown out or under proportion right.

175
00:12:15,695 --> 00:12:18,683
Speaker 0: so there is a tool called YouTube DL.

176
00:12:18,944 --> 00:12:26,050
Speaker 0: it lets you download videos from YouTube a feature that is not an official YouTube feature, but According to whispers.

177
00:12:26,171 --> 00:12:29,439
Speaker 0: I have heard from unnamed Unverified sources.

178
00:12:30,767 --> 00:12:38,460
Speaker 0: It is I think Likely in my opinion that there are developers working at YouTube who intentionally?

179
00:12:39,080 --> 00:12:48,593
Speaker 0: Design YouTube in such a way that it is not obvious That you can download things from YouTube because obviously they would never allow that.

180
00:12:48,853 --> 00:12:58,790
Speaker 0: right, but it is always Possible if you are technologically sophisticated enough, and that is why a tool like YouTube DL Yeah, it is possible to create.

181
00:12:58,831 --> 00:13:00,440
Speaker 0: it'll always be possible to create.

182
00:13:00,540 --> 00:13:04,296
Speaker 1: I love always if I can see it I can records it right.

183
00:13:04,316 --> 00:13:05,160
Speaker 0: There's always a hole.

184
00:13:05,320 --> 00:13:07,671
Speaker 1: I'm gonna use OBS right now to record my screen.

185
00:13:07,711 --> 00:13:09,560
Speaker 1: I could just record a YouTube video with OBS.

186
00:13:09,601 --> 00:13:10,219
Speaker 0: I mean there's even.

187
00:13:10,882 --> 00:13:19,720
Speaker 0: There's even just a hole just in the fact that you know while the video has to come to your computer and get decoded into Frames you could always grab it there because you're the administrator of your own computer.

188
00:13:19,800 --> 00:13:26,017
Speaker 1: Yeah, also because it can't even rely on like private encryption keys and browsers because YouTube works on multiple browsers.

189
00:13:26,098 --> 00:13:32,415
Speaker 0: right so it always be Possible, but it's slightly easier than it could be because of those developers that I have heard.

190
00:13:32,475 --> 00:13:33,939
Speaker 0: rumors of that are unverified.

191
00:13:33,979 --> 00:13:36,651
Speaker 0: so This tool YouTube DL exists.

192
00:13:36,852 --> 00:13:43,595
Speaker 0: it is used by many people for Various reasons some legitimate some not legitimate some piracy.

193
00:13:43,655 --> 00:13:47,466
Speaker 0: Yeah all of which I'm okay with personally, but the law.

194
00:13:47,686 --> 00:13:49,511
Speaker 0: the law is not okay with some of them.

195
00:13:49,631 --> 00:13:50,994
Speaker 1: Well the law is complicated.

196
00:13:51,035 --> 00:13:54,877
Speaker 1: the RIAA is Against this.

197
00:13:56,281 --> 00:13:59,589
Speaker 0: Understand this issue sufficiently right, but I'm saying it's some of the.

198
00:13:59,730 --> 00:14:03,178
Speaker 0: some of these uses are illegal under some laws and some are not.

199
00:14:03,660 --> 00:14:11,079
Speaker 0: so Yeah What happened is the source code of this tool YouTube DL is available on github because this is an open source project.

200
00:14:11,763 --> 00:14:20,539
Speaker 0: github received a DMCA takedown request From the RAS you know and they were saying hey take down this YouTube DL.

201
00:14:21,062 --> 00:14:24,680
Speaker 1: There are even was quote that the code is inherently illegal.

202
00:14:25,563 --> 00:14:25,763
Speaker 0: Right.

203
00:14:25,804 --> 00:14:28,773
Speaker 0: so this is interesting because a right.

204
00:14:28,933 --> 00:14:32,043
Speaker 0: it would not be under the normal DMCA.

205
00:14:32,083 --> 00:14:32,926
Speaker 0: takedown notice.

206
00:14:33,006 --> 00:14:37,620
Speaker 0: because YouTube DL the source code itself is not a copyright infringement.

207
00:14:37,761 --> 00:14:38,162
Speaker 0: It's not like.

208
00:14:38,182 --> 00:14:42,438
Speaker 0: there's some other tool that YouTube DL is a pirated copy of.

209
00:14:42,518 --> 00:14:43,422
Speaker 0: yeah, right It's not.

210
00:14:43,582 --> 00:14:43,964
Speaker 0: it's not like.

211
00:14:44,004 --> 00:14:46,053
Speaker 0: it's a pirated Photoshop or something.

212
00:14:46,415 --> 00:14:51,947
Speaker 0: It's a. it's a. it's a bread Unique you know on its own Freely licensed piece of software.

213
00:14:51,967 --> 00:14:54,416
Speaker 0: so the code itself is not illegal.

214
00:14:55,078 --> 00:15:00,071
Speaker 0: the argument The RA was making was that this is a tool for circumvention.

215
00:15:00,512 --> 00:15:08,866
Speaker 0: right a copyright circumvention tool, but The DMCA is like hey you can't have DRM Circumventing tools.

216
00:15:08,906 --> 00:15:12,139
Speaker 0: well YouTube DL isn't really circumventing a DRM.

217
00:15:12,480 --> 00:15:15,913
Speaker 0: There isn't like a DRM on YouTube the way that like CSS is.

218
00:15:16,013 --> 00:15:17,879
Speaker 1: I was about to bring up CSS DCSS.

219
00:15:18,181 --> 00:15:20,614
Speaker 1: They had a better case on but that wasn't the RIAA.

220
00:15:20,634 --> 00:15:21,399
Speaker 1: that was the MPAA.

221
00:15:22,194 --> 00:15:26,243
Speaker 0: right You know YouTube DL is Sort of.

222
00:15:26,263 --> 00:15:31,260
Speaker 0: you know YouTube isn't implementing a DRM to keep you from downloading videos.

223
00:15:31,381 --> 00:15:33,090
Speaker 0: It's just not providing that as a feature.

224
00:15:33,111 --> 00:15:41,619
Speaker 0: Yeah, so you're sort of you're saying I believe the lack of a download button a You know and you sort of adding one.

225
00:15:41,981 --> 00:15:47,627
Speaker 1: I do believe that YouTube has language to the effect of we do not allow Downloads.

226
00:15:47,950 --> 00:15:48,940
Speaker 0: that is something like that.

227
00:15:49,000 --> 00:15:54,838
Speaker 0: But that would be a fight that would be a violation of between you and Google as a breach of contract.

228
00:15:54,898 --> 00:15:59,530
Speaker 1: another lawyer That's between you and your god on your YouTube.

229
00:15:59,750 --> 00:16:03,038
Speaker 0: You know user agreement right out, whatever.

230
00:16:03,179 --> 00:16:08,676
Speaker 0: so So this takedown notice itself is questionable right on its on its.

231
00:16:08,796 --> 00:16:14,870
Speaker 1: oh, I'm pretty confident not being a lawyer That however this would not hold up.

232
00:16:14,931 --> 00:16:17,158
Speaker 1: a real lawyer actually wanted to wow this.

233
00:16:17,178 --> 00:16:20,049
Speaker 0: well So here's here's some problems right.

234
00:16:20,149 --> 00:16:22,800
Speaker 0: so so it's a positive and a negative problem.

235
00:16:23,081 --> 00:16:30,526
Speaker 0: So the negative problem YouTube DL made a big boo-boo and that in their documentation They specifically cited examples of like.

236
00:16:30,546 --> 00:16:31,529
Speaker 0: hey like.

237
00:16:31,589 --> 00:16:37,184
Speaker 0: literally the examples in the documentation were like if you wanted to download some Taylor Swift Or some you know whatever.

238
00:16:37,204 --> 00:16:38,969
Speaker 0: yeah, right like they.

239
00:16:39,390 --> 00:16:49,860
Speaker 0: they used copyright infringement as an example in their Documentation of how to use YouTube DL basically admitting that they were you know a tool for illegal uses.

240
00:16:49,980 --> 00:16:50,563
Speaker 0: They weren't like.

241
00:16:50,603 --> 00:17:04,859
Speaker 0: hey if you want to download some Creative Commons licensed videos from YouTube But you just can't download them because you know YouTube doesn't have a download button like say some geek nights videos Yeah, which are all Creative Commons attribution only licensed.

242
00:17:06,342 --> 00:17:07,547
Speaker 0: You know that wasn't their example.

243
00:17:07,567 --> 00:17:09,876
Speaker 0: their example was use this for piracy right?

244
00:17:09,915 --> 00:17:10,859
Speaker 0: Yeah, so that's a negative.

245
00:17:11,021 --> 00:17:16,679
Speaker 0: So I think a non technologically sophisticated judge would see that as very bad.

246
00:17:16,940 --> 00:17:21,839
Speaker 1: Yeah, even though they sell this is yeah, this feels more akin to lock picking tools.

247
00:17:22,162 --> 00:17:25,020
Speaker 1: It is perfectly legal for me to own and carry lock picking tools.

248
00:17:25,320 --> 00:17:28,590
Speaker 1: What is legal is to use them on someone else's property.

249
00:17:29,172 --> 00:17:32,020
Speaker 0: right as long as you only pick locks you own you're all good.

250
00:17:32,080 --> 00:17:33,915
Speaker 0: Yeah, anyway, thank you.

251
00:17:33,936 --> 00:17:38,065
Speaker 0: lock picking law Yes So then the.

252
00:17:38,206 --> 00:17:44,151
Speaker 0: the good part though is apparently this is really interesting There's apparently a bug in github right.

253
00:17:44,171 --> 00:17:44,814
Speaker 0: so in github.

254
00:17:45,255 --> 00:17:46,420
Speaker 0: You know you see a repository.

255
00:17:46,621 --> 00:17:47,464
Speaker 0: What do you do you make?

256
00:17:47,485 --> 00:17:48,549
Speaker 0: you want to contribute to it.

257
00:17:48,569 --> 00:17:49,674
Speaker 0: you make a fork right.

258
00:17:49,714 --> 00:17:52,625
Speaker 0: so it starts out as like Scott slash front row crew.

259
00:17:52,866 --> 00:17:55,094
Speaker 0: rim makes a fork rim slash front row crew.

260
00:17:55,154 --> 00:17:56,980
Speaker 0: he makes his own version of the source code.

261
00:17:57,503 --> 00:18:01,000
Speaker 0: Then he submits a pull request to me and says hey, I made some changes.

262
00:18:01,140 --> 00:18:06,154
Speaker 0: Here's the changes, please merge rim front row crew slash changes into Scott front row crew.

263
00:18:06,495 --> 00:18:08,180
Speaker 0: you know main branch slash changes.

264
00:18:08,240 --> 00:18:11,993
Speaker 0: Yeah, whatever slash main branch, and I can say yes or no.

265
00:18:12,515 --> 00:18:32,894
Speaker 0: apparently it is a bug in github Where if you it's arguable whether it's a bug or who knows, but if you make a pull request It'll actually sort of it'll actually make a copy of that code onto the Sort of the the this the receiving repos You know area right?

266
00:18:34,420 --> 00:18:40,871
Speaker 0: So like for example you might make a pull request to me and then delete your repo I'll still have the pull request you sent to me right?

267
00:18:40,891 --> 00:18:43,620
Speaker 0: so it's sort of like a necessary thing in case you delete yourself.

268
00:18:43,786 --> 00:18:47,089
Speaker 0: Yeah In case I wanted to keep that or something.

269
00:18:47,612 --> 00:18:56,600
Speaker 0: so somebody made a pull request To the github because github has a repo that consists of every DMC a takedown notice that has been sent to github.

270
00:18:57,263 --> 00:19:05,211
Speaker 0: So they sent a pull request to that DMC a github that wasn't merged But it contained the source code of youtube DL.

271
00:19:05,251 --> 00:19:13,700
Speaker 0: Yes, so you can see you can go to github.com Slash DMC a or slash github slash DMC a you know to find this repo.

272
00:19:14,382 --> 00:19:17,540
Speaker 0: And then if you you it won't be linked easily anywhere.

273
00:19:17,640 --> 00:19:20,012
Speaker 0: It'll be hard to find but people are sharing the link.

274
00:19:20,555 --> 00:19:21,480
Speaker 0: so you can go.

275
00:19:21,640 --> 00:19:32,010
Speaker 0: It's like you go to github.com such github such DMC a to the repository of DMC a takedown notices the github has received and Poof there's the youtube DL source code.

276
00:19:32,070 --> 00:19:40,420
Speaker 1: now even if it wasn't the like DCSS despite the fact that that one long integer was technically illegal.

277
00:19:41,301 --> 00:19:43,972
Speaker 1: Somehow it was very easy to find that integer online.

278
00:19:44,334 --> 00:19:46,139
Speaker 1: this code is going to be like that All right.

279
00:19:46,541 --> 00:19:49,329
Speaker 0: Also the fact that the entire world has switched to.

280
00:19:49,509 --> 00:19:56,693
Speaker 0: effectively the entire world has switched from SVN to get I don't know if people Realize that would distributed source code control software as opposed.

281
00:19:56,713 --> 00:19:58,600
Speaker 0: We've talked about this in the past actually on geek nights.

282
00:19:58,702 --> 00:19:59,740
Speaker 0: Go find those episodes.

283
00:20:00,362 --> 00:20:04,835
Speaker 0: Every single clone of that repo has the entire history of the repo.

284
00:20:04,875 --> 00:20:11,600
Speaker 0: if you get clone the Linux kernel You will have every version of the Linux kernel going back to the first version like on your computer.

285
00:20:11,600 --> 00:20:13,080
Speaker 0: It takes forever to download the friggin thing.

286
00:20:13,080 --> 00:20:21,106
Speaker 0: Yeah, so if you if you have ever cloned YouTube DL on to your local computer You have the code.

287
00:20:21,126 --> 00:20:22,050
Speaker 0: no right.

288
00:20:22,110 --> 00:20:25,543
Speaker 0: and so all these YouTube DL developers have all the code You can't really?

289
00:20:25,945 --> 00:20:28,740
Speaker 0: you know taking it down off github.com doesn't really stop anything.

290
00:20:29,384 --> 00:20:31,738
Speaker 0: And they can also file a counterclaim to get it put back up.

291
00:20:31,819 --> 00:20:44,124
Speaker 0: so Nothing has changed on the YouTube side, so there's really no danger of us losing the YouTube DL functionality Yeah, which I rely on heavily All that you know.

292
00:20:44,185 --> 00:20:55,765
Speaker 0: and even so there are other Git posting places including host it yourself, so it's like as long as the YouTube DL developers carry on their work Hosting their repo somewhere else

293
00:20:56,026 --> 00:21:02,999
Speaker 1: as long as there's a discord somewhere server somewhere where people talk about and use a private repo This code will exist.

294
00:21:03,742 --> 00:21:10,560
Speaker 0: So there's really nothing to get worried about here unless the YouTube GL developers decide to you know throw in the towel or something.

295
00:21:10,640 --> 00:21:11,866
Speaker 1: Yeah, but even then what else?

296
00:21:11,886 --> 00:21:14,520
Speaker 0: just someone else will just carry that or continue to work.

297
00:21:15,404 --> 00:21:23,700
Speaker 0: Yes, as long as those rumored YouTube developers Don't make it much much more difficult to download things from YouTube, then we'll have some trouble.

298
00:21:23,820 --> 00:21:31,047
Speaker 1: But on the other hand I can't see them being able to make it impossible because the frame It won't be impossible, but they could.

299
00:21:31,187 --> 00:21:34,820
Speaker 0: they could make it extremely much more difficult than it is they could.

300
00:21:35,001 --> 00:21:39,660
Speaker 1: It's not very and the current way things they could make it to where it's easier to use the analog hole.

301
00:21:40,215 --> 00:21:42,790
Speaker 0: Yes It's not nearly as difficult as it could be.

302
00:21:42,850 --> 00:22:05,040
Speaker 1: I guess what I would say is with this kind of copyright takedown The best case scenario for the people who are going after this kind of tool the best they could possibly achieve is to make the Thing they're trying to protect more of a pain in the ass than using the analog hole Because the analog hole will always exist no matter what and you can always Copy any media at a real-time playback rate.

303
00:22:06,325 --> 00:22:06,912
Speaker 0: Yep, pretty much.

304
00:22:07,801 --> 00:22:11,511
Speaker 1: But uh the EFF did respond to this and they said the thing they always say.

305
00:22:11,872 --> 00:22:20,940
Speaker 1: they called the RA's notice disappointing and counterproductive Pretty much a sentiment echoed by activists who work in the fields of civil rights human rights and hate speech.

306
00:22:21,663 --> 00:22:24,940
Speaker 0: Yeah, like I said, you know, there are many legitimate uses for YouTube DL.

307
00:22:25,040 --> 00:22:29,215
Speaker 0: They're not copyright infringement and that should have been what the examples also notice.

308
00:22:29,335 --> 00:22:32,000
Speaker 1: It's not the movie industry It's not videos.

309
00:22:32,080 --> 00:22:32,200
Speaker 1: The.

310
00:22:32,441 --> 00:22:38,056
Speaker 1: the people who went after this code are the music industry because they are arguing.

311
00:22:38,076 --> 00:22:39,641
Speaker 1: we talked about this on our last gig nights Too.

312
00:22:40,023 --> 00:22:43,158
Speaker 1: the RIA and the music industry is really thrashing right now.

313
00:22:43,982 --> 00:22:44,886
Speaker 0: Generally, yeah, they're.

314
00:22:45,328 --> 00:22:47,799
Speaker 0: they're really going after twitch as well, right?

315
00:22:48,200 --> 00:22:50,677
Speaker 0: That's why twitch is all is bringing the hammer down a little bit.

316
00:22:51,461 --> 00:22:57,482
Speaker 1: There's a reason why we've always hated the RIAA in particular because even though we do have to figure out a way to pay Musicians.

317
00:22:57,803 --> 00:23:04,124
Speaker 1: the RIAA is basically never acted in good faith when it comes to copyright at any point in its existence Right.

318
00:23:04,245 --> 00:23:09,360
Speaker 0: Well and at least in the twitch like YouTube does pay people In fact, it's sort of you know goes too far.

319
00:23:09,500 --> 00:23:21,826
Speaker 0: Right, if I make a video and include a copyrighted song in my in my video then the YouTube policy is basically Okay, I use Rolling Stones right now and they come the Rolling Stones come and say hey used our song We're gonna put a claim on your video.

320
00:23:21,866 --> 00:23:25,660
Speaker 0: your videos now showing ads and the Rolling Stones take all the money.

321
00:23:26,162 --> 00:23:28,010
Speaker 0: It's like excuse me, right?

322
00:23:28,090 --> 00:23:30,400
Speaker 0: You should get maybe a portion of the money from this video.

323
00:23:30,520 --> 00:23:32,774
Speaker 0: But there's also the part that I made right.

324
00:23:32,794 --> 00:23:33,640
Speaker 0: you're taking my money.

325
00:23:33,760 --> 00:23:36,330
Speaker 1: My 20-minute video has one of minutes of your audio.

326
00:23:36,371 --> 00:23:38,720
Speaker 1: you get two out of 20 of my revenue.

327
00:23:39,720 --> 00:23:41,045
Speaker 0: Exactly, right some sort of.

328
00:23:41,306 --> 00:23:43,293
Speaker 0: but no they just take everything which is bullshit right.

329
00:23:43,333 --> 00:23:44,678
Speaker 0: and then on twitch.

330
00:23:44,979 --> 00:23:45,926
Speaker 0: twitch It's you know.

331
00:23:45,986 --> 00:23:50,769
Speaker 0: twitch is obviously, you know Don't think that because twitch is not completely capitulating.

332
00:23:50,789 --> 00:23:52,414
Speaker 0: It's sort of covering for its streamers.

333
00:23:52,454 --> 00:23:54,520
Speaker 1: now twitch is completely capitulating.

334
00:23:55,641 --> 00:23:56,163
Speaker 0: No, it's not.

335
00:23:56,284 --> 00:23:59,557
Speaker 0: It's it's sort of like it's sort of being evil both ways right.

336
00:23:59,577 --> 00:24:04,820
Speaker 0: on the one hand They're like taking down people's videos just without even saying, you know, just like I'll take all this crap down.

337
00:24:04,960 --> 00:24:28,849
Speaker 0: so, you know to get these people off our back, but on the other hand, it's like Twitch is has purchased music licenses, but it's only purchased Performance licenses which are the kind for playing music and like a restaurant, you know that kind of thing It's technically supposed to be part because you're synchronizing video and the music You're legally supposed to be purchasing sync licenses.

338
00:24:28,869 --> 00:24:32,899
Speaker 0: I mean and then if it's a VOD mechanical licenses, right.

339
00:24:33,300 --> 00:24:37,483
Speaker 0: so Twitch is not paying the Musicians, right?

340
00:24:37,503 --> 00:24:42,620
Speaker 0: So YouTube steals all your money on the video creator and gives it to the musicians, right?

341
00:24:42,660 --> 00:24:43,987
Speaker 0: It's almost too music friendly.

342
00:24:44,007 --> 00:24:44,670
Speaker 0: Right?

343
00:24:45,012 --> 00:24:48,571
Speaker 0: But meanwhile twitch is The opposite.

344
00:24:48,632 --> 00:24:49,964
Speaker 0: it's like, okay They're they're.

345
00:24:49,984 --> 00:24:55,540
Speaker 0: they're taking down streamers videos to make them upset Right and to sort of get the RA to go away a little bit.

346
00:24:55,640 --> 00:24:59,173
Speaker 0: but they're also ripping off the music people not paying them.

347
00:24:59,273 --> 00:25:00,999
Speaker 1: and this is at the intersection of.

348
00:25:01,880 --> 00:25:06,520
Speaker 1: Someone buys a game but the game has music that is licensed within it.

349
00:25:06,882 --> 00:25:09,200
Speaker 1: But do they have a right to stream that game or not?

350
00:25:09,601 --> 00:25:11,168
Speaker 1: Like this is this whole area.

351
00:25:11,188 --> 00:25:14,040
Speaker 0: the person who makes the game the person who makes the game doesn't care.

352
00:25:14,464 --> 00:25:21,384
Speaker 0: Yeah, they're like, please stream Please stream because that is better marketing and gets me more sales than anything else I could spend marketing money on.

353
00:25:21,445 --> 00:25:24,215
Speaker 0: in fact, I will give the streamer money to stream the game.

354
00:25:24,275 --> 00:25:38,980
Speaker 1: I'm paying you to do this Yep, and you do see some more and more games are having completely original soundtracks to bypass this entire process Where the game itself like music came with the game and it's just part of that game.

355
00:25:39,101 --> 00:25:40,660
Speaker 1: There's no separate music license.

356
00:25:41,781 --> 00:25:53,499
Speaker 0: Anyway, the final story is if you didn't know that it's possible to download YouTube videos permanently I'll come to the party and you've got and you've got videos that might be taken down at some point or you're worried about not Being able to see them easily again.

357
00:25:53,640 --> 00:25:57,317
Speaker 0: You should get the YouTube DL tool or a similar tool and download all your.

358
00:25:57,337 --> 00:26:01,954
Speaker 1: be wary though If you get a similar tool from an untrusted source, and it's not source code.

359
00:26:02,516 --> 00:26:03,500
Speaker 1: It's probably malware.

360
00:26:04,382 --> 00:26:05,909
Speaker 0: Make sure you're getting some sort of legitimate.

361
00:26:06,150 --> 00:26:09,399
Speaker 1: if you google for YouTube downloader You're gonna find a lot of malware.

362
00:26:10,845 --> 00:26:11,957
Speaker 0: You are there's some out there.

363
00:26:11,978 --> 00:26:18,239
Speaker 0: yet There's also some older un-maintained tools that don't work because YouTube changed yep since and they didn't maintain them.

364
00:26:18,279 --> 00:26:29,001
Speaker 1: YouTube DL is actively main thing, but anyway things Of the day, so check this out.

365
00:26:29,042 --> 00:26:32,835
Speaker 1: This is a two second video Looping in a tweet.

366
00:26:33,116 --> 00:26:46,130
Speaker 1: That is my thing of the day, but what this video is is the surface of a comet Seen through a blizzard of dust ice and cosmic rays and a backdrop of stars.

367
00:26:46,391 --> 00:26:49,845
Speaker 1: This is a fucking video of the surface of a comet.

368
00:26:50,126 --> 00:26:52,423
Speaker 0: Holy shit Is it a comet?

369
00:26:52,443 --> 00:26:53,109
Speaker 0: just a ball of ice?

370
00:26:53,590 --> 00:26:55,862
Speaker 1: Yeah, and rocks it looks like wouldn't it wouldn't?

371
00:26:55,882 --> 00:26:57,330
Speaker 0: it just look like a ice cube with rocks in it.

372
00:26:57,370 --> 00:27:01,850
Speaker 1: It looks like a cliff face with rocks and a blizzard happening and snow.

373
00:27:03,332 --> 00:27:04,438
Speaker 1: Okay, it's really cool.

374
00:27:04,458 --> 00:27:06,268
Speaker 1: It's a goddamn comet.

375
00:27:07,491 --> 00:27:09,116
Speaker 1: All right, I mean we've landed things on.

376
00:27:09,157 --> 00:27:10,782
Speaker 1: comets like comets are neat.

377
00:27:11,183 --> 00:27:17,259
Speaker 1: comets are probably where a lot of that water on the moon comes from - Yeah, anyway, that's all I got.

378
00:27:18,534 --> 00:27:24,626
Speaker 0: Yeah, so I've been doing the bike to nowhere situation because it there's nowhere to go and the weather sucks now, right?

379
00:27:25,790 --> 00:27:27,054
Speaker 0: So beat when you're doing the bike to nowhere.

380
00:27:27,094 --> 00:27:27,776
Speaker 0: the bike to nowhere.

381
00:27:27,816 --> 00:27:28,879
Speaker 0: software, right?

382
00:27:28,899 --> 00:27:32,850
Speaker 0: You sort of can't avoid learning these new things like FTP.

383
00:27:32,871 --> 00:27:33,495
Speaker 0: ever heard of FTP?

384
00:27:33,535 --> 00:27:35,548
Speaker 0: not the file transfer protocol, but the other FTP.

385
00:27:35,588 --> 00:27:38,577
Speaker 0: now It's functional threshold power.

386
00:27:38,657 --> 00:27:40,423
Speaker 0: It's like the average wattage.

387
00:27:40,463 --> 00:27:42,590
Speaker 0: You're capable of putting out in one hour.

388
00:27:42,690 --> 00:27:46,863
Speaker 1: I've seen people talking about wattage hours stuff on blogs, but I don't care.

389
00:27:46,943 --> 00:27:49,310
Speaker 0: So you bike as hard as you can for an hour.

390
00:27:49,491 --> 00:27:52,586
Speaker 0: What's the average wattage you would put out over the hour?

391
00:27:52,647 --> 00:27:53,209
Speaker 0: That's your FTP.

392
00:27:54,111 --> 00:27:55,375
Speaker 0: Yeah, TSS.

393
00:27:55,495 --> 00:27:57,361
Speaker 0: It's like this scoring system.

394
00:27:57,401 --> 00:27:58,986
Speaker 0: They use it's not very complicated.

395
00:27:59,066 --> 00:27:59,287
Speaker 0: Actually.

396
00:27:59,307 --> 00:28:02,517
Speaker 0: It's a simple formula Which has to do with this has to.

397
00:28:02,739 --> 00:28:09,763
Speaker 0: it's mostly based on wattage and length and time But it's basically a scoring system for how difficult a particular bike ride is.

398
00:28:09,944 --> 00:28:13,057
Speaker 0: So it's like if it's a 200 TSS That's a really hard bike ride.

399
00:28:13,117 --> 00:28:15,229
Speaker 0: But like a 50 is like alright you biked, you know.

400
00:28:15,992 --> 00:28:17,476
Speaker 0: Anyway, so all this stuff.

401
00:28:17,496 --> 00:28:32,617
Speaker 0: so I started going on YouTube to better understand what the hell this stuff meant and I found this channel this guy Dylan Johnson, so Dylan Johnson is a. I don't know if he's professional or not, but he is definitely a competitive Endurance mountain biker, right?

402
00:28:32,857 --> 00:28:35,765
Speaker 0: So and he places highly so he's really good at it.

403
00:28:36,206 --> 00:28:39,670
Speaker 1: endurance mountain biking is extremely difficult and painful.

404
00:28:40,951 --> 00:28:44,000
Speaker 0: It actually looks to me like I would do that.

405
00:28:44,542 --> 00:28:48,338
Speaker 0: then regular mountain biking cuz like regular mountain biking like, you know Downhill is like no.

406
00:28:48,418 --> 00:28:49,810
Speaker 0: No, you're you mistake.

407
00:28:50,470 --> 00:28:53,460
Speaker 1: Downhill is a separate subcategory of mountain biking.

408
00:28:53,501 --> 00:28:59,170
Speaker 1: most people who say they're mountain bikers do not do downhill mountain biking Right, but it's like this looks way less scary.

409
00:28:59,271 --> 00:29:03,154
Speaker 0: It just looks like hey you're biking a long way Which I do anyway.

410
00:29:03,455 --> 00:29:07,771
Speaker 0: only it happens to be in the dirt and there might be some rocks and shit and you have a mountain bike Instead.

411
00:29:07,792 --> 00:29:08,916
Speaker 1: that's most mountain biking.

412
00:29:08,936 --> 00:29:10,463
Speaker 1: That's when I say I'm doing mountain biking.

413
00:29:10,503 --> 00:29:11,527
Speaker 1: That's what I'm doing 90% of the time.

414
00:29:12,893 --> 00:29:13,945
Speaker 0: That actually seems okay.

415
00:29:13,986 --> 00:29:19,822
Speaker 0: But anyway So this guy does that very well and he is also I guess his profession is.

416
00:29:19,863 --> 00:29:25,460
Speaker 0: he is a coach right athletic coach trainer person And he's got a YouTube channel where he talks about stuff.

417
00:29:25,600 --> 00:29:32,784
Speaker 0: and normally when I search and try to find any Exercise health fitness wellness, you know, I even like the term wellness.

418
00:29:32,824 --> 00:29:35,396
Speaker 0: if they say you they use the word wellness That's usually up flags.

419
00:29:35,537 --> 00:29:36,201
Speaker 1: Nope right out of there.

420
00:29:36,221 --> 00:29:37,830
Speaker 1: That's one step before homeopathy.

421
00:29:38,652 --> 00:29:39,167
Speaker 0: That's right.

422
00:29:40,511 --> 00:29:44,210
Speaker 0: Whenever you see that kind of YouTube channel, you're like this is gonna be a bunch of woo bullshit, right?

423
00:29:44,370 --> 00:29:44,673
Speaker 0: I can't.

424
00:29:44,834 --> 00:29:45,298
Speaker 0: how do I know?

425
00:29:45,318 --> 00:29:46,689
Speaker 0: I can trust what this guy's gonna say?

426
00:29:47,510 --> 00:29:49,890
Speaker 0: Dylan Johnson has done the best job of convincing me.

427
00:29:49,910 --> 00:30:03,750
Speaker 0: He knows what he's talking about because all his videos right are all about him reading every single study about the topic right and Saying well this one says this this one says that this one says this this one says that and this one says this.

428
00:30:04,111 --> 00:30:09,550
Speaker 0: So in the end, what can we say is true to the best of human knowledge as of today?

429
00:30:10,270 --> 00:30:26,753
Speaker 0: We don't know or this or the opposite of you know What you thought and he answers a lot of questions that are very relevant to any cyclist Who is even slightly more than non serious, even if you're not considering racing Like.

430
00:30:26,774 --> 00:30:27,750
Speaker 0: I have no interest in racing.

431
00:30:28,231 --> 00:30:32,926
Speaker 0: But he answers a lot of questions like are the clipless pedals more efficient than just?

432
00:30:33,066 --> 00:30:39,589
Speaker 1: that's the one I'm looking at And the one I'm gonna watch after the show are is is what's the right seat height?

433
00:30:40,152 --> 00:30:43,307
Speaker 0: What you know is it is it better be low high in the middle, right?

434
00:30:43,347 --> 00:30:43,870
Speaker 0: How do you figure?

435
00:30:45,230 --> 00:31:00,049
Speaker 1: Biking specifically because the thing I found about biking that I've not found with all a lot of the other sports I do is that biking people tend to always say there is a one true way to do a specific thing and There's never seems to be evidence to back that up.

436
00:31:00,431 --> 00:31:07,730
Speaker 1: But it's like this very specific dogma about there like here's the rule on how to size your seat exactly to you for your bike.

437
00:31:08,453 --> 00:31:09,660
Speaker 1: Do not do a lot of kids.

438
00:31:09,962 --> 00:31:15,802
Speaker 0: Yes, there is a lot of Conventional wisdom in the cycling community and also a lot of just common sense.

439
00:31:15,963 --> 00:31:18,570
Speaker 0: Yeah, that's sort of two things that just make sense, right?

440
00:31:20,402 --> 00:31:27,090
Speaker 0: and You know things you see you look at pro cyclists and you like Tour de France You see them doing something and you say well that must be the right way.

441
00:31:27,110 --> 00:31:32,370
Speaker 0: Yeah, but now maybe not not always but maybe but maybe not right, you know, I saw video.

442
00:31:32,694 --> 00:31:33,969
Speaker 0: Did I do that as a thing of the day?

443
00:31:34,190 --> 00:31:36,610
Speaker 0: Like what's the best position to go to be in going down?

444
00:31:37,550 --> 00:31:44,590
Speaker 1: Because we talked a lot when we did that show about Superman's the best position but don't do Superman right exactly so.

445
00:31:46,053 --> 00:31:49,453
Speaker 0: but yeah, Dylan Johnson's channel is Incredible.

446
00:31:49,533 --> 00:31:53,170
Speaker 0: I've learned a lot so far and I've only watched like a handful of the videos.

447
00:31:53,271 --> 00:31:57,030
Speaker 0: I'm probably gonna watch all the ones relevant to myself and adjust accordingly.

448
00:31:58,371 --> 00:32:04,107
Speaker 0: So yeah, if you if you bike at all and care about getting better at biking is a must watch channel.

449
00:32:04,648 --> 00:32:05,330
Speaker 0: Good job, Dylan.

450
00:32:05,370 --> 00:32:12,955
Speaker 1: I guess it's interesting that when I think about this like I run into that conventional wisdom and doctrine way more recycling than other sports I do specifically like.

451
00:32:13,015 --> 00:32:14,863
Speaker 1: even skiing does not have this.

452
00:32:14,943 --> 00:32:21,426
Speaker 1: like we ask a question of a ski instructor The answer is full of a lot of caveats and it depends and a lot of sort of like.

453
00:32:21,848 --> 00:32:23,273
Speaker 1: this might work But it might not.

454
00:32:23,333 --> 00:32:24,156
Speaker 1: let's try.

455
00:32:24,377 --> 00:32:27,810
Speaker 1: but with cycling it's just always like put your seat at this height based on this formula.

456
00:32:29,854 --> 00:32:30,337
Speaker 0: Well, there is.

457
00:32:30,377 --> 00:32:32,670
Speaker 0: there is a formula which one's the right exactly.

458
00:32:33,531 --> 00:32:34,072
Speaker 0: Dylan knows.

459
00:32:34,132 --> 00:32:36,701
Speaker 1: he'll tell you so in the moment stuck at home.

460
00:32:37,082 --> 00:32:38,346
Speaker 1: You can watch our panels on YouTube.

461
00:32:38,908 --> 00:32:40,297
Speaker 1: join our discord I don't know.

462
00:32:40,317 --> 00:32:43,235
Speaker 1: I Thought about fucking like vote.

463
00:32:43,275 --> 00:32:43,636
Speaker 1: That's it.

464
00:32:43,676 --> 00:32:47,588
Speaker 1: Just go fucking vote for Joe Biden or things are gonna get real bad in a hurry.

465
00:32:47,769 --> 00:32:55,112
Speaker 0: or If you if you are eligible to vote in the United States of America and you do not vote for Joe Biden get the hell Out.

466
00:32:55,152 --> 00:33:02,010
Speaker 0: Yep, and if you live in New York State You should vote for Joe Biden on the working families party line should to make to make the governor mad.

467
00:33:02,875 --> 00:33:03,479
Speaker 0: It will can't?

468
00:33:03,519 --> 00:33:04,425
Speaker 0: it will count the same.

469
00:33:04,506 --> 00:33:04,969
Speaker 0: I promise.

470
00:33:04,989 --> 00:33:08,785
Speaker 1: Yep It still counts for Joe Biden, but it pisses off Cuomo and all.

471
00:33:08,805 --> 00:33:09,668
Speaker 1: good people hate him.

472
00:33:10,695 --> 00:33:12,510
Speaker 1: Bad people hate him too, but so do good people.

473
00:33:13,912 --> 00:33:14,678
Speaker 0: Same thing for the mayor.

474
00:33:14,758 --> 00:33:14,940
Speaker 0: Yeah.

475
00:33:14,960 --> 00:33:19,543
Speaker 0: Anyway, I Thought about reading a chapter of our book club.

476
00:33:19,664 --> 00:33:24,362
Speaker 0: Oh so fancy of Genji, but instead I read a comic book.

477
00:33:24,563 --> 00:33:25,627
Speaker 1: We're in a comic book.

478
00:33:25,667 --> 00:33:26,973
Speaker 1: You know what I did I can't?

479
00:33:27,113 --> 00:33:30,930
Speaker 1: I played a bunch of Star Wars and like made some food and went hiking.

480
00:33:31,991 --> 00:33:32,754
Speaker 0: I made some food.

481
00:33:32,774 --> 00:33:34,759
Speaker 0: I played some links of the past.

482
00:33:34,779 --> 00:33:36,344
Speaker 1: problem is I like normally this.

483
00:33:36,384 --> 00:33:42,810
Speaker 1: like this season when I'd go hiking I'd have a long train ride to and from the hiking that I would read books on.

484
00:33:43,191 --> 00:33:45,799
Speaker 1: But now the only way to hike safely is to drive places.

485
00:33:46,160 --> 00:33:47,865
Speaker 1: So I have to drive the whole time.

486
00:33:47,885 --> 00:33:49,149
Speaker 0: I just hike to the hiking.

487
00:33:50,210 --> 00:33:55,405
Speaker 1: That'd be like yeah, let me bike to Poughkeepsie to then bike to Albany.

488
00:33:55,685 --> 00:33:58,572
Speaker 1: I Mean.

489
00:33:58,592 --> 00:33:59,315
Speaker 1: I guess I could get.

490
00:33:59,395 --> 00:34:00,560
Speaker 1: I could get the.

491
00:34:00,580 --> 00:34:07,848
Speaker 1: probably I'd have to camp because to get all those mountains I would have to hike about 70 miles from Queens to get to those mountain trailheads.

492
00:34:11,570 --> 00:34:15,222
Speaker 1: Three days two days if I'd like death marched it but really that's a three-day.

493
00:34:15,242 --> 00:34:17,449
Speaker 0: I could bike 70 miles in like seven hours.

494
00:34:17,570 --> 00:34:18,213
Speaker 1: Yeah, bike it.

495
00:34:18,253 --> 00:34:19,819
Speaker 1: Let's see how far you can walk.

496
00:34:20,141 --> 00:34:21,446
Speaker 1: Okay, how long they'll take you to walk.

497
00:34:21,446 --> 00:34:23,833
Speaker 1: 70 miles Days.

498
00:34:24,137 --> 00:34:27,893
Speaker 1: Yeah take about three days Three Days.

499
00:34:27,954 --> 00:34:31,155
Speaker 1: if you're really fit and good at walking I'm good at walking.

500
00:34:31,496 --> 00:34:32,040
Speaker 1: Are you really?

501
00:34:32,081 --> 00:34:32,724
Speaker 1: we should test this?

502
00:34:32,745 --> 00:34:33,449
Speaker 1: you want to go for a walk?

503
00:34:34,971 --> 00:34:38,040
Speaker 0: My legs are long Therefore I'm good at walking.

504
00:34:38,322 --> 00:34:39,110
Speaker 1: But yeah vote.

505
00:34:39,770 --> 00:34:42,603
Speaker 1: Otherwise, like the next week is gonna be rough for a lot of people.

506
00:34:42,623 --> 00:34:49,487
Speaker 1: So just be ready for that and We'll try to just keep doing geek nights until then and maybe at some point We'll have some downtime and actually read books again.

507
00:34:50,771 --> 00:35:02,421
Speaker 0: my major concern these days is that Hockey will resume and for some reason they will allow people to go to hockey Before it is because then we have to sell all those tickets.

508
00:35:02,441 --> 00:35:04,068
Speaker 1: because I'm not going to have a to sell right.

509
00:35:04,490 --> 00:35:12,441
Speaker 0: they will have to sell all these tickets because Even though I said I would go as soon as I could I'm not gonna go if it's not that was assuming they wouldn't let Me go until it was safe.

510
00:35:12,482 --> 00:35:12,886
Speaker 0: Yeah, right.

511
00:35:12,967 --> 00:35:15,252
Speaker 0: So That's my.

512
00:35:15,272 --> 00:35:18,524
Speaker 0: that's you know and for that to be my concern I'm doing pretty well.

513
00:35:18,564 --> 00:35:24,002
Speaker 1: honestly though the way the news is trending I'm pretty sure COVID is gonna be like a wave worse than anything.

514
00:35:24,042 --> 00:35:25,145
Speaker 1: We've seen up to this point.

515
00:35:25,807 --> 00:35:27,395
Speaker 1: over the next few weeks That's gonna happen.

516
00:35:27,415 --> 00:35:42,470
Speaker 1: Like things are looking pretty bad in most of the country I'll say the only saving grace for us personally Luckily is if we live in New York City Which except for one neighborhood is actually trending very well like one specific neighbor a couple couple couple neighborhoods.

517
00:35:42,691 --> 00:35:44,140
Speaker 1: Only one neighborhood is the red zone.

518
00:35:44,341 --> 00:35:45,629
Speaker 1: There are three yellow zones.

519
00:35:46,577 --> 00:35:48,050
Speaker 0: Yeah, then one of the other ones is pretty big.

520
00:35:48,150 --> 00:35:51,569
Speaker 1: Yeah, but that one big yellow zone is around the red zone.

521
00:35:52,751 --> 00:35:54,777
Speaker 0: No, no, no, there's another one out by Q Gardens.

522
00:35:55,580 --> 00:35:56,242
Speaker 1: That one's pretty big.

523
00:35:56,302 --> 00:35:57,566
Speaker 0: It's like a second one.

524
00:35:57,667 --> 00:36:00,750
Speaker 1: Anyway, anyway So, uh, let's get right into it.

525
00:36:00,810 --> 00:36:04,063
Speaker 1: We talked about facial recognition off and out over the years, but we never really did a whole.

526
00:36:04,083 --> 00:36:05,690
Speaker 0: she had some AI episodes.

527
00:36:05,891 --> 00:36:07,500
Speaker 0: You've had some privacy episodes.

528
00:36:07,540 --> 00:36:09,229
Speaker 0: Yeah, you know things like that.

529
00:36:09,832 --> 00:36:12,990
Speaker 0: But it's in the news so much and we don't have a lot of Monday ideas.

530
00:36:13,050 --> 00:36:14,755
Speaker 0: So send your Monday ideas to us.

531
00:36:15,036 --> 00:36:15,237
Speaker 0: cuz.

532
00:36:15,437 --> 00:36:21,778
Speaker 0: Mondays for some reason are like slightly more popular than the other days even though They're the hardest to come up with ideas.

533
00:36:21,959 --> 00:36:23,669
Speaker 1: Wednesdays are the least popular days.

534
00:36:23,990 --> 00:36:26,790
Speaker 1: I think it's because we're the old men of anime at this point.

535
00:36:27,395 --> 00:36:33,977
Speaker 0: Maybe it's because we just know more about Technology and gaming than we know about anime and that's why well We don't watch as much anime as we used to like.

536
00:36:34,017 --> 00:36:35,543
Speaker 1: I basically only watch great shows.

537
00:36:35,603 --> 00:36:41,810
Speaker 1: now though total aside Remember way back we're talking about how we should watch decadence because there's people keep talking about it.

538
00:36:42,494 --> 00:36:48,657
Speaker 1: we watch the first two episodes, uh Yeah You're the whole first episode.

539
00:36:48,678 --> 00:36:49,804
Speaker 1: You're like, alright, it's a show.

540
00:36:49,824 --> 00:36:50,890
Speaker 1: Alright, I like it.

541
00:36:50,930 --> 00:36:53,177
Speaker 1: Okay at the very end of the episode you like wait what.

542
00:36:53,899 --> 00:36:54,802
Speaker 1: and then episode two.

543
00:36:55,424 --> 00:36:58,461
Speaker 1: the show is not what it looks like Holy shit.

544
00:36:58,501 --> 00:36:59,490
Speaker 1: It is not what it looks like.

545
00:37:00,971 --> 00:37:04,705
Speaker 1: Okay, but I think it's specifically up your alley directly.

546
00:37:04,725 --> 00:37:07,598
Speaker 0: you person Is there?

547
00:37:07,962 --> 00:37:08,870
Speaker 0: is there a terrible secret?

548
00:37:09,512 --> 00:37:20,058
Speaker 0: Oh is there and when you're there a train that is there a train that quietly goes out into the night and has a moral What a certain character looks like.

549
00:37:20,760 --> 00:37:22,405
Speaker 1: you'll see what I mean, that's all I can say.

550
00:37:22,425 --> 00:37:24,010
Speaker 0: a robot a cute robot.

551
00:37:24,875 --> 00:37:25,570
Speaker 1: Okay, I can't.

552
00:37:26,531 --> 00:37:28,723
Speaker 1: Everyone who's tried to get me to watch the show has said the same thing.

553
00:37:28,884 --> 00:37:29,850
Speaker 1: I won't tell you anything about it.

554
00:37:29,890 --> 00:37:30,352
Speaker 1: Just watch it.

555
00:37:31,115 --> 00:37:31,918
Speaker 1: Do not read about it.

556
00:37:31,999 --> 00:37:32,360
Speaker 0: Maybe I'll.

557
00:37:32,581 --> 00:37:33,725
Speaker 0: where is it available?

558
00:37:33,786 --> 00:37:34,830
Speaker 1: funimation and Hulu?

559
00:37:36,572 --> 00:37:37,416
Speaker 0: I don't have either one of them.

560
00:37:37,436 --> 00:37:39,226
Speaker 1: Yeah, we happen to have Hulu by accident.

561
00:37:39,246 --> 00:37:40,010
Speaker 1: So we watch it on Hulu.

562
00:37:41,152 --> 00:37:42,259
Speaker 1: No, I didn't know we had Hulu.

563
00:37:42,279 --> 00:37:43,064
Speaker 1: Apparently we had Hulu.

564
00:37:43,990 --> 00:37:46,038
Speaker 0: I Previously had Hulu before I had Netflix.

565
00:37:46,058 --> 00:37:47,163
Speaker 0: I'm currently in Netflix mode.

566
00:37:47,243 --> 00:37:48,347
Speaker 0: Once I get out of Netflix mode.

567
00:37:48,387 --> 00:37:49,231
Speaker 0: I might go back to Hulu.

568
00:37:49,291 --> 00:37:52,710
Speaker 1: yeah, it's hard though cuz Netflix has just got so much constant good stuff lately.

569
00:37:53,551 --> 00:37:54,013
Speaker 0: Yeah, I did.

570
00:37:54,033 --> 00:37:55,622
Speaker 0: I watched like that Michael Jordan documentary.

571
00:37:55,642 --> 00:37:56,004
Speaker 0: That was good.

572
00:37:56,144 --> 00:37:57,089
Speaker 1: Yeah, you can watch a keep.

573
00:37:57,149 --> 00:38:01,270
Speaker 1: Oh Season 3 the third season is the last season anyway.

574
00:38:01,870 --> 00:38:09,190
Speaker 1: facial recognition technology is a good topic because It is so illustrative of the tech transfer ethics concerns of technology.

575
00:38:10,171 --> 00:38:13,580
Speaker 1: But unlike nuclear weapons, it's not like just killing people.

576
00:38:13,961 --> 00:38:15,144
Speaker 1: There aren't that many.

577
00:38:15,284 --> 00:38:17,510
Speaker 1: like peaceful uses for nuclear weapons.

578
00:38:18,393 --> 00:38:18,935
Speaker 1: There were a few.

579
00:38:18,995 --> 00:38:21,686
Speaker 1: there were terrible ideas, but there were a few.

580
00:38:22,107 --> 00:38:23,312
Speaker 1: you can read about it You want.

581
00:38:23,633 --> 00:38:25,339
Speaker 1: nuclear mining was a thing for a while.

582
00:38:25,399 --> 00:38:26,423
Speaker 1: It was a terrible idea.

583
00:38:26,443 --> 00:38:27,808
Speaker 0: That's a bad idea.

584
00:38:28,310 --> 00:38:29,855
Speaker 1: Oh It was tested.

585
00:38:29,915 --> 00:38:35,994
Speaker 1: It's a bad idea, but you don't say facial recognition It's one of those technologies.

586
00:38:36,275 --> 00:38:37,860
Speaker 1: that one is unstoppable.

587
00:38:38,060 --> 00:38:38,421
Speaker 1: We did.

588
00:38:38,642 --> 00:38:43,630
Speaker 1: there is no way to get rid of this technology So you need cannot go back in the bottle.

589
00:38:43,670 --> 00:38:47,670
Speaker 0: We know how to do it therefore and all you need to do it is a camera and a computer.

590
00:38:48,013 --> 00:38:49,506
Speaker 0: You can buy those things off the shelf.

591
00:38:49,688 --> 00:38:51,689
Speaker 0: Yep There is no putting the genie back in the bottle.

592
00:38:51,910 --> 00:38:53,515
Speaker 0: It can be done and it will be done.

593
00:38:53,555 --> 00:39:01,627
Speaker 1: if the world banned facial recognition technology the source code exists and just like YouTube DL people would use it on their own hardware and it would still happen.

594
00:39:02,069 --> 00:39:05,901
Speaker 0: and Unlike, you know, if the world banned nuclear weapons, well, you could.

595
00:39:06,102 --> 00:39:10,630
Speaker 0: there's only so much plutonium, you know, it can be regulated It can be detected with your counters.

596
00:39:10,771 --> 00:39:12,598
Speaker 0: It could be well also do something about.

597
00:39:12,618 --> 00:39:17,341
Speaker 1: almost no one on earth knows how to make a thermonuclear device Like that is there's.

598
00:39:17,461 --> 00:39:18,629
Speaker 0: there are very easy.

599
00:39:19,091 --> 00:39:26,230
Speaker 0: It is relatively easy ways to enforce that law and detect violators to stop people from using facial recognition.

600
00:39:26,330 --> 00:39:41,047
Speaker 0: It's almost impossible because it's already in so many things And you have to ban like digital cameras which are being like there's mostly factories in the on earth Various places just like spewing out thousands of digital cameras a second.

601
00:39:41,087 --> 00:39:44,075
Speaker 0: Like it's unstoppable You can't do anything about it.

602
00:39:44,657 --> 00:39:49,070
Speaker 1: I just think of that old Bloom County strip randomly where they're making that illegal drug.

603
00:39:49,210 --> 00:40:01,029
Speaker 1: It's a legal drug like opus and the crew are making it and the government just declares it illegal and they see the news and they Just yell like stop pouring the stuff Oliver like stop cut it off.

604
00:40:01,934 --> 00:40:09,871
Speaker 0: But like they're literally there must be like a factory somewhere where they're just like, you know Like little tiny cell phone cameras like just coming out and like a flood like there's tons of them anyway,

605
00:40:10,072 --> 00:40:16,556
Speaker 1: but also facial recognition has a huge number of Extremely beneficial uses

606
00:40:17,218 --> 00:40:17,940
Speaker 0: and a huge

607
00:40:18,000 --> 00:40:20,467
Speaker 1: number of extremely dangerous uses,

608
00:40:21,169 --> 00:40:21,330
Speaker 0: right?

609
00:40:22,411 --> 00:40:26,845
Speaker 0: Yeah, it bears mentioning that facial recognition is really just is not actually the technology?

610
00:40:26,905 --> 00:40:29,492
Speaker 0: the technology is simply image Recognition.

611
00:40:29,553 --> 00:40:33,905
Speaker 0: right is where you use machine learning to train a computer.

612
00:40:34,246 --> 00:40:39,104
Speaker 0: you give it lots of data So images and you tell it hey, this is a cat.

613
00:40:39,264 --> 00:40:39,967
Speaker 0: This is a cat.

614
00:40:40,027 --> 00:40:40,710
Speaker 0: This is a cat.

615
00:40:41,131 --> 00:40:41,893
Speaker 0: Here's a new image.

616
00:40:41,913 --> 00:40:42,916
Speaker 0: You've never seen before.

617
00:40:43,157 --> 00:40:43,919
Speaker 0: Is there a cat?

618
00:40:43,999 --> 00:40:45,965
Speaker 0: and if so, where is the cat?

619
00:40:46,326 --> 00:40:47,750
Speaker 0: and this is a software.

620
00:40:47,911 --> 00:40:48,634
Speaker 0: You can this.

621
00:40:48,734 --> 00:40:52,690
Speaker 0: instructions online like tutorials how to write that exact software yourself.

622
00:40:53,293 --> 00:40:56,850
Speaker 0: You can write it if you want to make one to the text something right?

623
00:40:57,310 --> 00:40:59,984
Speaker 0: All you need is the database of whatever it is.

624
00:41:00,024 --> 00:41:02,275
Speaker 0: You want to detect so Human face.

625
00:41:02,396 --> 00:41:05,189
Speaker 0: is simply that you're like detect human face, right?

626
00:41:06,110 --> 00:41:11,307
Speaker 0: And then you go to the next step and you say okay you found human face right in this picture.

627
00:41:11,368 --> 00:41:12,110
Speaker 0: You've counted them.

628
00:41:13,511 --> 00:41:18,264
Speaker 0: Now here's a database of human faces which have names and phone numbers attached.

629
00:41:18,645 --> 00:41:21,607
Speaker 0: find the phone numbers of all these faces Right, that's the step two.

630
00:41:21,898 --> 00:41:26,726
Speaker 1: Yep Now even worse think about I could find the phone numbers of all the cats, but they don't have phones.

631
00:41:26,826 --> 00:41:37,387
Speaker 1: humans are uniquely suited like we can identify other people very quickly and Unconsciously just by seeing their face like some people can.

632
00:41:37,568 --> 00:41:44,970
Speaker 0: well, yeah Have a Disability like face blindness, which is a great reason for a positive use of facial recognition.

633
00:41:45,111 --> 00:41:52,269
Speaker 1: Imagine Penn Jillette could wear a Google Glass and whenever someone walked up to him It showed them if he knows this person and what their name was.

634
00:41:52,952 --> 00:41:59,190
Speaker 0: Right a face blind person with ARG that identified faces of that the person already knew and told them their names.

635
00:41:59,591 --> 00:42:03,624
Speaker 0: Like that would even be useful for people who weren't face blind if used appropriately.

636
00:42:03,644 --> 00:42:05,430
Speaker 0: Yeah, I don't remember any remember?

637
00:42:06,272 --> 00:42:08,550
Speaker 1: Great that would be if people's names just appeared over there.

638
00:42:08,992 --> 00:42:11,244
Speaker 1: I was in a panel at a PAX taking questions, right?

639
00:42:11,545 --> 00:42:21,184
Speaker 1: or someone comes up to me And I they act like they know me and I have no idea who they are right, it's like if people who already did know me and had seen my face and knew my name.

640
00:42:21,245 --> 00:42:26,400
Speaker 0: if only they could see my Face and instantly recall my name with a memory aid.

641
00:42:26,700 --> 00:42:29,769
Speaker 0: Maybe they're maybe they aren't face blind, but their memory is just normal.

642
00:42:29,869 --> 00:42:37,843
Speaker 0: like You didn't say bad you just said normal Yeah, or you know not not perfect as no one's is right or hardly anyone's is.

643
00:42:38,244 --> 00:42:41,475
Speaker 0: that would be a great and good use of Technology, right?

644
00:42:41,495 --> 00:42:42,239
Speaker 0: Yeah, people would know.

645
00:42:42,682 --> 00:42:45,495
Speaker 0: it would help people connect with each other What Facebook says they want to do.

646
00:42:45,595 --> 00:43:06,170
Speaker 1: but it also makes this technology uniquely suited to identifying individual humans because the human brain like we are so we sort of evolved to where Specific cues in our faces can be rapidly parsed by our brains to remember who someone is meaning It doesn't take that much facial data for a computer to do the same thing.

647
00:43:06,210 --> 00:43:10,114
Speaker 1: You only need a little bit of data from someone's face Right.

648
00:43:10,174 --> 00:43:12,002
Speaker 0: a face is easy to find.

649
00:43:12,082 --> 00:43:17,543
Speaker 1: It's got you know, mostly usually Humans are good at finding faces that don't even exist, right?

650
00:43:17,924 --> 00:43:20,876
Speaker 0: It's a roughly circular Oval type shape.

651
00:43:21,097 --> 00:43:28,522
Speaker 0: Yeah, there's a limited number of shapes and it has distinct features Right that are easy for this kind of computer software to find.

652
00:43:29,184 --> 00:43:34,296
Speaker 0: and then it's also so Even though it's able to say it's easy to say that's a face.

653
00:43:34,797 --> 00:43:45,504
Speaker 0: But then once you have the face cut out Faces are so unique and varied and detailed especially with high-resolution cameras It's easy to tell faces apart and say well, which face is it?

654
00:43:45,945 --> 00:43:47,150
Speaker 0: It's like two different questions.

655
00:43:47,271 --> 00:43:49,306
Speaker 0: It's actually two technologies at work, right one.

656
00:43:49,931 --> 00:43:51,129
Speaker 0: So here's another one for step two.

657
00:43:51,290 --> 00:43:57,931
Speaker 1: Here's a an example of how the exact same app even that use facial recognition could be used for great harm or great good Cuz.

658
00:43:57,951 --> 00:43:58,514
Speaker 1: this is an app.

659
00:43:58,534 --> 00:43:59,177
Speaker 1: I've always wanted.

660
00:43:59,217 --> 00:44:04,217
Speaker 1: I even wrote like a document and RIT Describing this app is one of my classes.

661
00:44:04,257 --> 00:44:16,149
Speaker 1: So I remember like I did a lot of research on facial recognition back in 2003 for my tech transfer class and The app I put together was basically I called it that guy.

662
00:44:17,072 --> 00:44:17,995
Speaker 1: But what the app was?

663
00:44:18,296 --> 00:44:28,930
Speaker 1: so you and your group of friends like our group of friends or like the geek nice listeners as a whole We all like friend each other in this app and say one of us of our group of friends

664
00:44:29,331 --> 00:44:29,752
Speaker 0: encounter

665
00:44:29,792 --> 00:44:51,643
Speaker 1: someone who is an abuser or a piece of shit like someone who is an annoying that guy at conventions or someone who is like just being misogynist and racist and terrible at conventions if I flag that person to say That person like like that person was racist to me at this con and that at another con Scott Meet someone and his Google s just pops up and says hey Scott.

666
00:44:51,664 --> 00:44:53,550
Speaker 1: So, you know rim said this guy was a racist.

667
00:44:54,353 --> 00:44:54,514
Speaker 0: Right.

668
00:44:54,554 --> 00:44:58,270
Speaker 0: Now the only reason this is working is because me and rim already agreed.

669
00:44:58,532 --> 00:45:04,902
Speaker 0: You know the fact that I friended rim on this service Yeah already means I trust his opinion and I'm only seeing the opinions of people.

670
00:45:04,962 --> 00:45:07,610
Speaker 0: I trust I'm not just seeing data from randos exactly.

671
00:45:08,592 --> 00:45:12,068
Speaker 1: So even in that context evil use exact same app.

672
00:45:12,089 --> 00:45:32,722
Speaker 1: an Evil person who would say a stalker or a sexual abuser flags the faces of people who are easy marks or who have been abused successfully so that Friends in their shitty pickup artist Network could get a notification of hey this person you're talking to in a bar This is a person that this other pickup artist already fucked with.

673
00:45:33,023 --> 00:45:34,909
Speaker 1: and here's the cheat sheet on how to pick them up.

674
00:45:35,631 --> 00:45:36,735
Speaker 1: That's far right.

675
00:45:37,296 --> 00:45:45,730
Speaker 0: same or exact same at or a way to detect like, you know People who are like, you know good at detecting them, right?

676
00:45:45,890 --> 00:45:50,230
Speaker 0: So it's like, you know, maybe you're a criminal and you got all the cops faces, right?

677
00:45:50,550 --> 00:45:58,996
Speaker 0: Maybe you're a protester and you got all the cops or maybe you're a cop and you got all the protesters faces or The criminals faces.

678
00:45:59,137 --> 00:46:00,846
Speaker 0: Yeah, and this is be good.

679
00:46:00,906 --> 00:46:01,590
Speaker 0: It could be bad.

680
00:46:02,112 --> 00:46:05,890
Speaker 1: This is why there's so many serial killer on the loose.

681
00:46:05,950 --> 00:46:09,310
Speaker 0: We have a video of the serial killer and now we're gonna use facial.

682
00:46:09,411 --> 00:46:12,630
Speaker 0: It's an actual serial killer actual crime not bullshit, right?

683
00:46:12,970 --> 00:46:25,810
Speaker 0: and now we're gonna use the facial recognition technology on all the security cameras and an alarm will go off if this if the serial killer is spotted anywhere in range of a camera and then we can go get Them to stop them from serial killing again, right?

684
00:46:25,990 --> 00:46:37,790
Speaker 1: Oh, so I was actually looking something up because I remember seeing something about how the BCEC we're packs East is every year Had that system Remember when they put in that system that could figure out the number of unique people in the building based on like low-key facial recognition.

685
00:46:38,532 --> 00:46:47,288
Speaker 1: the city of Boston has actually recently banned the use of facial recognition technology and I wonder what that will mean for the fire code counts at the BCEC.

686
00:46:48,332 --> 00:46:50,650
Speaker 0: Yeah, I wonder I don't know about that system specifically.

687
00:46:50,710 --> 00:47:01,020
Speaker 0: I know I heard rumors that it could count people but I don't know if it could like look up people but in China They absolutely have ones that can identify Individuals in a crowd basically cheat sheet.

688
00:47:01,060 --> 00:47:01,723
Speaker 0: Where's Waldo?

689
00:47:01,783 --> 00:47:03,250
Speaker 0: Yeah, that's literally what it is.

690
00:47:03,370 --> 00:47:04,293
Speaker 0: No, they can take.

691
00:47:04,494 --> 00:47:05,878
Speaker 1: how good could that technology be?

692
00:47:05,939 --> 00:47:07,985
Speaker 1: a kid get ridiculously good.

693
00:47:08,006 --> 00:47:09,330
Speaker 1: Yeah, I could get lost.

694
00:47:09,430 --> 00:47:12,359
Speaker 1: I know I'm saying how good in terms of boons to society.

695
00:47:12,660 --> 00:47:26,456
Speaker 1: a kid gets lost in a city or a giant convention center and Cameras are media like the kid is here highlight him give him give coordinates and follow him until someone can go find the kid and save them or The bank is robbed.

696
00:47:26,657 --> 00:47:27,881
Speaker 0: we arrested this person.

697
00:47:27,901 --> 00:47:30,730
Speaker 0: Oh, but then we checked the camera footage and we exonerated them.

698
00:47:30,810 --> 00:47:33,588
Speaker 0: They were actually at this other location at the time of the bank robbery.

699
00:47:33,608 --> 00:47:49,350
Speaker 1: Yep The problem the main problem is that generally when a technology is powerful like this The people who have more real-world power can leverage this technology more effectively Against people who wield less real-world power.

700
00:47:49,872 --> 00:47:53,690
Speaker 1: What that means is that governments can use it more effectively than private citizens.

701
00:47:53,972 --> 00:47:57,030
Speaker 1: Police can use it more effectively than private citizens.

702
00:47:58,516 --> 00:48:04,610
Speaker 1: Generally Disadvantaged communities can use it less effectively than more privileged communities, right?

703
00:48:04,931 --> 00:48:07,310
Speaker 1: So we'll exacerbate existing divides in society.

704
00:48:08,292 --> 00:48:10,159
Speaker 0: We do have access to this technology.

705
00:48:10,219 --> 00:48:12,990
Speaker 0: You can get it and just use it yourself for your own purposes.

706
00:48:13,130 --> 00:48:15,179
Speaker 0: the problem is your data.

707
00:48:15,420 --> 00:48:20,760
Speaker 0: will that your database of faces and Information about those faces will be severely limited?

708
00:48:21,101 --> 00:48:23,570
Speaker 0: and in order to build up that database, right?

709
00:48:23,916 --> 00:48:29,470
Speaker 0: It will require You know, you're not going to be able to build as good of one as the government who can put cameras all over the city.

710
00:48:29,531 --> 00:48:32,458
Speaker 0: Yeah right and use use tax money to buy giant comp.

711
00:48:32,538 --> 00:48:38,618
Speaker 1: or you could even see hackers like a lot of facial recognition software that is Made by like smaller companies or private individuals.

712
00:48:38,940 --> 00:48:42,352
Speaker 1: They all train it using the same like generic data sets You can buy.

713
00:48:42,753 --> 00:48:51,910
Speaker 1: meaning if you understand the biases in that data set you could possibly exploit Someone's implementation of facial recognition to paint a crime on someone else.

714
00:48:52,531 --> 00:48:54,035
Speaker 0: That's actually a huge problem right?

715
00:48:54,096 --> 00:49:04,980
Speaker 0: is that even people who do have you know, lots of data right advanced systems Those systems are often racist like you have a huge database But the database is all white people.

716
00:49:05,583 --> 00:49:06,990
Speaker 0: or you know, let's say China.

717
00:49:07,131 --> 00:49:10,030
Speaker 0: The Chinese database is almost entirely Chinese people, right?

718
00:49:10,270 --> 00:49:13,710
Speaker 0: You know, so if I go over to China that system might you know?

719
00:49:13,750 --> 00:49:17,110
Speaker 0: I imagine it would just be like unknown not Chinese person, right?

720
00:49:17,671 --> 00:49:22,729
Speaker 0: or maybe they would take my photos when I Come to China and then they would put me in the database and that way they could find me.

721
00:49:24,071 --> 00:49:28,286
Speaker 0: But it might have a hard time because it's heavily trained on Chinese people, right?

722
00:49:28,426 --> 00:49:29,510
Speaker 0: So different facial structure.

723
00:49:29,671 --> 00:49:36,172
Speaker 1: So some of the things that second do can day are things like from a data set find one person or find like.

724
00:49:36,193 --> 00:49:47,864
Speaker 1: the people are most likely to be one particular person or like they're especially powerful when you focus them on a very Specific community like that's right now where the tech is the most powerful.

725
00:49:48,145 --> 00:49:51,500
Speaker 1: the committee could be just you like Your face unlocks your phone.

726
00:49:51,561 --> 00:49:53,189
Speaker 1: That's a community of one person.

727
00:49:55,171 --> 00:49:58,527
Speaker 1: Or you and your like small group of friends like you use it as a like.

728
00:49:58,547 --> 00:49:58,808
Speaker 1: I've already.

729
00:49:58,828 --> 00:50:06,090
Speaker 1: you can buy Door locks that are electronic and some of them have the ability to like unlock your house based on just looking at your face.

730
00:50:06,539 --> 00:50:06,969
Speaker 1: That's cool.

731
00:50:08,072 --> 00:50:10,623
Speaker 0: One really another if you're talking about small uses, right?

732
00:50:10,684 --> 00:50:15,484
Speaker 0: So your camera it Using the first part of the technology just find human faces.

733
00:50:15,504 --> 00:50:16,890
Speaker 0: I don't care who they are, right?

734
00:50:16,971 --> 00:50:26,270
Speaker 0: So you take a camera it finds faces and therefore Autofocus is really well because most of the time when you take a photo and the face in it You want to focus on the face because who wants photo of the tree, right?

735
00:50:26,390 --> 00:50:30,378
Speaker 0: No, you take if you're taking a photo of the tree It wouldn't be a person in the photo right looking at the camera.

736
00:50:30,398 --> 00:50:32,925
Speaker 0: Yeah, and then step two, right?

737
00:50:33,006 --> 00:50:39,644
Speaker 0: This is also a really, you know completely Mundane but useful use of the technology.

738
00:50:40,045 --> 00:50:40,909
Speaker 0: whose face is it?

739
00:50:41,710 --> 00:50:48,199
Speaker 0: Automatically organize my photo collection based on who's in the photo now Forever my fucking old gx-1.

740
00:50:48,279 --> 00:50:52,879
Speaker 1: I programmed in the names of a bunch of our friends and It'll tag the photos.

741
00:50:52,919 --> 00:50:54,309
Speaker 1: I take with that thing with their names.

742
00:50:54,470 --> 00:50:57,881
Speaker 0: Yeah, Lightroom Lightroom does it pretty much every photo app these days.

743
00:50:57,922 --> 00:50:58,503
Speaker 0: does that right?

744
00:50:58,523 --> 00:50:59,206
Speaker 0: It's like basic.

745
00:50:59,286 --> 00:51:00,530
Speaker 0: It's like a standard feature.

746
00:51:00,611 --> 00:51:02,406
Speaker 0: It's not even an advanced new feature.

747
00:51:02,467 --> 00:51:07,065
Speaker 0: It's like Expected that you have this where it's like, okay find me photos.

748
00:51:07,125 --> 00:51:08,190
Speaker 0: I've taken of Scott.

749
00:51:08,411 --> 00:51:10,908
Speaker 0: Oh, here's all the photos in my library with my face in them.

750
00:51:11,069 --> 00:51:12,013
Speaker 0: Yep I didn't have to.

751
00:51:12,234 --> 00:51:17,537
Speaker 0: I didn't have to do any, you know, like tagging by hand, you know Like I used to have to do in the old days.

752
00:51:17,597 --> 00:51:20,410
Speaker 1: Yeah, I don't even like I. increasingly I take a lot of photos.

753
00:51:20,430 --> 00:51:21,308
Speaker 1: They generate a lot of video.

754
00:51:21,795 --> 00:51:29,790
Speaker 1: I've increasingly stopped Bothering to fully and like add the metadata necessarily to find specific things in the future because it's not worth the effort.

755
00:51:30,131 --> 00:51:34,750
Speaker 1: I'm sort of waiting and relying on technology like this to do that for me in the future.

756
00:51:35,651 --> 00:51:39,530
Speaker 0: Yeah, I really wish people would apply because there's no reason technologically speaking.

757
00:51:39,590 --> 00:51:46,550
Speaker 0: This couldn't be applied to audio, you know, you should be able to say that wave that mp3 is a geek nights episode, right?

758
00:51:46,891 --> 00:51:50,073
Speaker 0: Yep, without having to put id3 tags on it It should just know it.

759
00:51:50,977 --> 00:51:52,724
Speaker 0: might I know which episode or the topic?

760
00:51:53,266 --> 00:52:00,069
Speaker 1: but it should be able to Commute and then Okay, so that doesn't narrow it down at all.

761
00:52:00,971 --> 00:52:11,790
Speaker 0: No, it doesn't right, but you should be able to have one that's like oh, here's a song It's a just automatically tag the genre and possibly even the artist right if it doesn't and not base.

762
00:52:11,850 --> 00:52:13,662
Speaker 0: You know the way most music things work is.

763
00:52:13,682 --> 00:52:14,870
Speaker 0: they're looking at like, you know.

764
00:52:14,890 --> 00:52:17,397
Speaker 0: They just have a database of the exact songs right like.

765
00:52:17,758 --> 00:52:22,030
Speaker 0: oh, here's the M is exactly, you know, the song, you know a Beatles song.

766
00:52:22,372 --> 00:52:23,841
Speaker 0: Oh, this is that exact song.

767
00:52:23,982 --> 00:52:29,810
Speaker 0: It's it's my guitar gently weeps I know this song right, but what if it just didn't have a database of songs?

768
00:52:29,931 --> 00:52:37,481
Speaker 0: it just had a database of you know singers and Musical styles and was saying oh that's a rock song and who do we hear?

769
00:52:37,541 --> 00:52:41,654
Speaker 0: singing sounds like the Beatles and automatically tagged the music Library that way.

770
00:52:41,754 --> 00:52:42,437
Speaker 0: Yep.

771
00:52:42,477 --> 00:52:45,290
Speaker 0: There's no reason that couldn't exist right now if the database is large enough.

772
00:52:45,310 --> 00:52:51,242
Speaker 1: Yep No actually getting to the database because one of the listeners Sam is actually posting a lot of good info in the stream right now Because he works in this space.

773
00:52:52,051 --> 00:52:55,549
Speaker 0: But the data says if you're doing evil shit quit the job.

774
00:52:56,554 --> 00:53:00,170
Speaker 1: And he seems like he's not doing evil shit because he's talking a lot about ethical concerns.

775
00:53:00,938 --> 00:53:01,545
Speaker 0: Good and leak.

776
00:53:01,585 --> 00:53:04,758
Speaker 1: that shit You know But evil think it's of what you just said.

777
00:53:04,879 --> 00:53:11,169
Speaker 1: a lot of the more advanced uses of facial recognition Require machine learning of some kind meaning training data sets.

778
00:53:12,432 --> 00:53:19,948
Speaker 1: The data sets encode the bias of society at the time The data set was collected and the biases of the people doing the collecting.

779
00:53:21,711 --> 00:53:33,810
Speaker 1: We've told we've talked about that a lot on geek nights and it doubly applies here like The facial recognition software built into like my smartphone just fucking doesn't work as well on non-white faces.

780
00:53:35,351 --> 00:53:37,519
Speaker 1: No, of course not like straight up because that's what.

781
00:53:37,579 --> 00:53:38,382
Speaker 1: what do they train it on?

782
00:53:38,623 --> 00:53:45,650
Speaker 0: they trained it on data sets that are mostly white Yeah, or they you know the people working in the office where they made the software, right?

783
00:53:45,750 --> 00:53:46,132
Speaker 0: I think that's.

784
00:53:46,212 --> 00:53:47,617
Speaker 0: that's an old story that they liked.

785
00:53:47,778 --> 00:53:50,810
Speaker 0: it's true story, but they tell it a lot where it's not even facial recognition.

786
00:53:50,914 --> 00:53:57,429
Speaker 0: it was like It was like a paper towel dispenser or soap dispenser where you put your hand in front of it And then the soap just comes out without you having to touch it.

787
00:53:57,571 --> 00:54:08,770
Speaker 0: Yeah And all the people who worked in the in the engineering team that designed the hand Detector were white and then black people put their hand under it and the soap didn't come out and the paper towel didn't move.

788
00:54:09,011 --> 00:54:11,021
Speaker 0: Yeah, right and it's like oops.

789
00:54:11,142 --> 00:54:23,389
Speaker 1: Maybe you should have had some for you know employees but the racist the weird danger of facial recognition is that a lot of the most powerful things we could do with it for the benefit of society are also the most dangerous.

790
00:54:23,409 --> 00:54:36,284
Speaker 1: and If we make the data sets more equitable and we make the core technology better, which is going to happen We actually enable those larger broader uses more readily.

791
00:54:36,304 --> 00:54:39,698
Speaker 0: It's like yeah, let's put you know We wanted this to be less racist.

792
00:54:39,838 --> 00:54:42,651
Speaker 0: Okay, add more black faces to the database All right now.

793
00:54:42,772 --> 00:54:46,138
Speaker 0: equally good on the races and all the black people.

794
00:54:46,238 --> 00:54:47,872
Speaker 0: Yeah, it's like Aha.

795
00:54:48,973 --> 00:54:49,536
Speaker 0: So great.

796
00:54:49,596 --> 00:54:52,088
Speaker 0: now the cops have an even better idea of who to go after.

797
00:54:52,148 --> 00:55:09,150
Speaker 1: Thanks I don't know how like society is going to deal with the like the next generation of facial tech like 10 15 20 years from now when the some of the Examples we've talked about become like not only widespread but easy to implement.

798
00:55:10,051 --> 00:55:16,972
Speaker 0: Yeah, I think this technology is massively disruptive at least in countries where it's well and heavily Regulated.

799
00:55:17,313 --> 00:55:23,130
Speaker 0: right the wrong large the corporations and government won't be using it for nefarious purposes.

800
00:55:23,293 --> 00:55:29,110
Speaker 1: We live in America But I'm not even worried about the government using it so much as I'm worried about private companies using right reg.

801
00:55:29,271 --> 00:55:31,590
Speaker 0: I don't regulate a well-regulated country, right?

802
00:55:31,790 --> 00:55:33,234
Speaker 0: It'll be those Pete.

803
00:55:33,274 --> 00:55:44,808
Speaker 0: the corporations and government won't be using it as much or in in ways that aren't you know Well controlled but there's nothing stopping the general public from using it and who knows what way.

804
00:55:44,828 --> 00:55:52,682
Speaker 0: yeah As long as they you know, if they're technologically sophisticated So there will be you know, some sort of YouTube DL.

805
00:55:52,863 --> 00:56:00,480
Speaker 0: ask right if you're in the know And you can set up your own fancy tech and you get this database from some Pirate Bay.

806
00:56:01,411 --> 00:56:07,437
Speaker 0: then you're gonna have evil facial recognition technology and use it for evil and But also maybe use it for good.

807
00:56:07,899 --> 00:56:22,338
Speaker 1: Yep, but I don't know like there is Really kick-ass to constrain the power of this technology except through government regulation and transparency Or Adversarial use of it.

808
00:56:22,379 --> 00:56:34,230
Speaker 1: there have been a number of articles recently talking about cyberpunk and like you want to know what cyberpunk really is because basically The police and law enforcement America are already heavily using and abusing facial recognition.

809
00:56:34,350 --> 00:56:36,599
Speaker 1: There's a whole like not to get into specifics.

810
00:56:36,920 --> 00:56:38,065
Speaker 1: They're definitely using it.

811
00:56:38,105 --> 00:56:39,330
Speaker 1: They're definitely abusing it.

812
00:56:39,591 --> 00:56:42,179
Speaker 1: They're definitely targeting protesters with it.

813
00:56:42,841 --> 00:56:43,985
Speaker 0: Yeah, you were at a protest.

814
00:56:44,025 --> 00:56:45,710
Speaker 0: All right, we took photos of everyone at the protest.

815
00:56:45,791 --> 00:56:46,639
Speaker 0: We didn't do anything.

816
00:56:46,760 --> 00:56:47,830
Speaker 0: and then we walk around town.

817
00:56:47,930 --> 00:56:59,469
Speaker 0: People are just going to get groceries and then with facial recognition technology says oh that person in the grocery checkout aisle They they were at the protest last night and they were anti cop and then they go out to put the groceries in the car And then we get them and they're alone.

818
00:56:59,932 --> 00:57:03,487
Speaker 0: All right, then we know it's them because we photo their face at the.

819
00:57:03,728 --> 00:57:07,630
Speaker 0: you know Yeah, we captured everyone's face at the protest with our facial recognition technology.

820
00:57:07,971 --> 00:57:12,130
Speaker 1: Yeah, but then that opened up the door to let's say we have good regulations and controls.

821
00:57:12,191 --> 00:57:22,030
Speaker 1: So, all right the Baltimore Convention Center has their facial recognition thing that that keeps track of The fire code count and they use it if a kid gets lost to reunite them with their parents.

822
00:57:22,112 --> 00:57:22,688
Speaker 1: That's all good.

823
00:57:23,072 --> 00:57:30,109
Speaker 1: And let's say they even they even like don't use it on ethically Directly like the company just uses it for its purpose and that's all fine.

824
00:57:31,330 --> 00:57:35,423
Speaker 1: One low paid employee who has access to that data could abuse it.

825
00:57:35,443 --> 00:57:35,564
Speaker 1: Mm-hmm.

826
00:57:36,326 --> 00:57:37,229
Speaker 0: That's right or leak it.

827
00:57:37,550 --> 00:57:39,069
Speaker 1: Yep, leak it or abuse it.

828
00:57:39,391 --> 00:57:45,590
Speaker 1: How many times does some cop get caught looking up an ex-girlfriend in a database to then go commit some crime?

829
00:57:46,750 --> 00:57:47,814
Speaker 0: Yeah, something like that.

830
00:57:48,015 --> 00:57:59,962
Speaker 0: Yep, but or you know, maybe there's somebody who's like you know like Trying to hide and that person Who controls the thing is like the evil person is trying to find them.

831
00:58:00,122 --> 00:58:02,610
Speaker 0: or maybe you know, someone just breaks in right?

832
00:58:02,730 --> 00:58:12,370
Speaker 0: It's like, you know, you know that the person you're you're a stalker and the person you're stalking You know is that is that a convention in the building and you break into the office and you use it and you find where?

833
00:58:12,410 --> 00:58:13,588
Speaker 0: They are in the building, right?

834
00:58:14,110 --> 00:58:26,849
Speaker 1: Otherwise you your chance of finding them are so small or you just call someone who works there and socially engineer them similar to how people's Sims get stolen all the time with cell phones Mm-hmm because a lot of people in the chain are not paid enough to care.

835
00:58:27,611 --> 00:58:43,407
Speaker 0: Alright, so the last thing we should talk about which is pretty obvious and well-spoken on the internet But is that yeah, this technology is at least for the foreseeable future Somewhat actually easy to foil because it's not so advanced to the point where you can see through things.

836
00:58:43,427 --> 00:58:43,728
Speaker 0: Yep.

837
00:58:43,788 --> 00:58:44,370
Speaker 1: Wear a mask.

838
00:58:45,253 --> 00:58:46,296
Speaker 0: You can use masks.

839
00:58:46,557 --> 00:58:48,102
Speaker 0: You can just shield your face.

840
00:58:48,303 --> 00:58:50,410
Speaker 0: You can use various kinds of makeup.

841
00:58:50,530 --> 00:58:54,730
Speaker 1: No, so I have read numerous studies showing that that dazzling makeup has zero effects.

842
00:58:54,850 --> 00:59:00,548
Speaker 0: That is not that maybe that specific but there are other kinds you can use though that makeup basically doesn't do anything.

843
00:59:00,869 --> 00:59:02,889
Speaker 1: only cover and concealment does.

844
00:59:03,652 --> 00:59:07,063
Speaker 0: Obviously that helps the most right but there there you can do.

845
00:59:07,083 --> 00:59:10,870
Speaker 0: I'm not like not talking about like makeup like You know somewhat tame makeup.

846
00:59:10,975 --> 00:59:18,275
Speaker 1: No No the extreme makeup seemed to have no effect either because some of the more on your face like some extra eyeballs that are like Realistic and shit.

847
00:59:18,376 --> 00:59:21,830
Speaker 1: I saw a lot of papers talking about how that stuff's all trivially foiled.

848
00:59:22,771 --> 00:59:26,269
Speaker 0: Maybe we'll say but I'm pretty sure a mask is a mask.

849
00:59:27,106 --> 00:59:29,170
Speaker 0: Yeah Cameras can't see through things.

850
00:59:29,230 --> 00:59:35,450
Speaker 0: I guess you can infrared cameras, but that's still not gonna give you the resolution that you need to really identify a face.

851
00:59:35,555 --> 00:59:50,398
Speaker 1: Yeah I was reading right with earlier today because they get written ready for the show an article about using basically LIDAR and other technology from cameras to generate pseudo 3d models of faces as well as 2d and how that Drastically improves the quality.

852
00:59:51,080 --> 00:59:52,948
Speaker 0: the new Pro iPhones have that at least.

853
00:59:53,008 --> 01:00:02,668
Speaker 0: yeah I'm very surprised the new Pro iPhones are adding that and using it for autofocus When like a Canon DSLR doesn't have it and maybe it should.

854
01:00:02,889 --> 01:00:03,712
Speaker 1: no Maybe it should.

855
01:00:04,153 --> 01:00:20,729
Speaker 0: I have seen people selling systems third-party systems that you can basically attach to any manual lens Gears and it will use LIDAR to slowly and shittily Focus a manual focus lens automatically not too bad.

856
01:00:21,312 --> 01:00:25,949
Speaker 0: There might be someone who needs that like you really need autofocus no matter how shitty it is.

857
01:00:26,330 --> 01:00:30,950
Speaker 1: Yeah, you as and you care about like some ancient specialized manual only lens.

858
01:00:31,434 --> 01:00:39,344
Speaker 0: Yeah, you care about it focusing accurately But you don't care and you care about focusing automatically and you don't care about money And you don't care about how much equipment you carry.

859
01:00:39,364 --> 01:00:44,341
Speaker 0: Yeah But you don't care how slowly it autofocus is either.

860
01:00:44,382 --> 01:00:45,124
Speaker 0: you're okay with that.

861
01:00:45,565 --> 01:00:50,984
Speaker 0: then this is a good choice for you But that could develop into something much better as time goes on.

862
01:00:51,104 --> 01:00:55,228
Speaker 1: now one direction this could go is You know that whole cyberpunk thing I was talking about.

863
01:00:56,253 --> 01:01:05,801
Speaker 1: One way to control this kind of technology if it gets out of control is for organized Activists to aggressively use it against the entities that are oppressing them.

864
01:01:06,282 --> 01:01:16,990
Speaker 1: case in point the recent groups of protesters Who have been cataloging police violence from videos and then using facial recognition to identify and document the specific?

865
01:01:17,232 --> 01:01:18,729
Speaker 1: officers involved in the abuse.

866
01:01:19,491 --> 01:01:19,612
Speaker 0: Right.

867
01:01:19,632 --> 01:01:23,110
Speaker 0: So you're hanging out and there's a bunch of cops coming and you're like, alright, those are all good cops.

868
01:01:23,190 --> 01:01:23,854
Speaker 0: They have like this.

869
01:01:23,874 --> 01:01:27,050
Speaker 0: many of us looks up their faces and looks up in the database and says here's how many?

870
01:01:27,310 --> 01:01:28,334
Speaker 0: Violations of those cops have?

871
01:01:28,395 --> 01:01:30,484
Speaker 0: how many times they've gotten in trouble for beating someone up?

872
01:01:30,885 --> 01:01:32,010
Speaker 0: Oh zero zero zero zero.

873
01:01:32,091 --> 01:01:33,369
Speaker 0: These are the four good cops in town.

874
01:01:33,452 --> 01:01:35,095
Speaker 0: Okay You know the next one right.

875
01:01:35,356 --> 01:01:39,333
Speaker 0: and then you come across another cop and it's like 20 and it's like whoop taking another street getting Away from that.

876
01:01:39,373 --> 01:01:41,689
Speaker 1: Yeah, or even just as simple as all right.

877
01:01:42,330 --> 01:01:43,798
Speaker 1: Some violence in the street.

878
01:01:44,119 --> 01:01:46,410
Speaker 1: you hit the you have some video of it and you approve.

879
01:01:46,811 --> 01:01:49,439
Speaker 1: Oh, that's the same cop who attacked this other person two days ago.

880
01:01:49,459 --> 01:01:51,305
Speaker 1: attack this other person three days ago.

881
01:01:51,325 --> 01:01:54,516
Speaker 0: or a bunch of undercover Cops ain't so undercover anymore.

882
01:01:54,677 --> 01:01:55,560
Speaker 0: Yeah their face.

883
01:01:55,821 --> 01:01:58,230
Speaker 0: It's like oh, there's a bunch of people coming to join our protest.

884
01:01:58,330 --> 01:01:58,732
Speaker 0: Wait a minute.

885
01:01:59,213 --> 01:02:01,180
Speaker 0: Those are cops and get them out of here.

886
01:02:01,341 --> 01:02:02,324
Speaker 1: getting back to YouTube.

887
01:02:02,365 --> 01:02:03,830
Speaker 1: GL kind of bringing this all together.

888
01:02:05,130 --> 01:02:14,570
Speaker 1: that is the kind of tech activism that will be necessary if Governments don't step in and properly regulate these things in an equitable manner.

889
01:02:14,992 --> 01:02:16,018
Speaker 1: That is the only way to.

890
01:02:16,440 --> 01:02:23,369
Speaker 1: the only way to protect yourself from technology That's out of the genie bottle is to use the technology against the people who are using it against you.

891
01:02:24,333 --> 01:02:30,683
Speaker 0: The last the last thing I think is one possible route that I hasn't I haven't seen explored much.

892
01:02:31,125 --> 01:02:33,795
Speaker 0: is a Database poisoning right?

893
01:02:33,875 --> 01:02:39,150
Speaker 0: It's like if they're collecting faces and failing and that's learning more faces, right?

894
01:02:40,631 --> 01:02:50,075
Speaker 0: You know just somehow feed into the cameras lots of wrong or messed up or otherwise, you know faces Theoretically possible.

895
01:02:50,155 --> 01:02:59,850
Speaker 1: I just don't know if it's practical but it is a thing I've thought about so I Have been doing a database poisoning experiment for most of my life since late middle school.

896
01:03:00,736 --> 01:03:07,104
Speaker 1: uh-huh, I Put slightly wrong birthdays into everything I ever put my birthday into.

897
01:03:07,385 --> 01:03:27,467
Speaker 1: that is not an official government record And what I found as time goes on Websites that I do not have accounts with or that I make a new account with like things that I interact with will sometimes Have my birthday somewhere or will send me a birthday thing Even though I didn't tell them my birthday and based on what birthday they

898
01:03:27,668 --> 01:03:28,070
Speaker 0: say

899
01:03:28,613 --> 01:03:32,190
Speaker 1: I can trace back to what database they got that data from

900
01:03:33,832 --> 01:03:34,034
Speaker 0: Mm-hmm.

901
01:03:35,002 --> 01:03:35,970
Speaker 1: I haven't done much with it.

902
01:03:35,990 --> 01:03:53,143
Speaker 1: It's just something I noticed over the years, but I was thinking of it because a couple weeks ago I remembered that my birthday and steam is from the year 1978 because that's the year I picked for that particular platform and Sither to the 1978 birth year has never shown up anywhere else.

903
01:03:53,203 --> 01:03:54,046
Speaker 1: It's only in Steve.

904
01:03:55,330 --> 01:03:58,529
Speaker 1: I Was just thinking about that the other day, but it's relevant to the whole database poisoning thing.

905
01:03:59,152 --> 01:03:59,835
Speaker 0: Yeah, all right.

906
01:03:59,915 --> 01:04:10,596
Speaker 0: So facial recognition is gonna be with us for the rest of our lives Cannot make it go as long as you have a face and a computer and a camera and the Cameras are connected to the computers.

907
01:04:11,158 --> 01:04:14,950
Speaker 0: You're gonna have to learn to live with it somehow, right?

908
01:04:15,290 --> 01:04:16,274
Speaker 0: Yeah, I recommend it.

909
01:04:16,394 --> 01:04:20,390
Speaker 0: a good start extreme government regulation, which is happening already.

910
01:04:20,450 --> 01:04:21,053
Speaker 0: So that's a good sign.

911
01:04:21,254 --> 01:04:25,270
Speaker 1: Yep, extreme government regulation coupled with cyberpunk activism.

912
01:04:31,265 --> 01:04:33,800
Speaker 1: This has been geek nights with rim and Scott special.

913
01:04:33,860 --> 01:04:38,599
Speaker 1: Thanks to DJ pretzel for the opening music cat leave for web design and Brando K for the logos.

914
01:04:38,921 --> 01:04:43,940
Speaker 0: Be sure to visit our website at front row crew comm for show notes discussion news and more.

915
01:04:44,220 --> 01:04:51,240
Speaker 1: Remember geek nights is not one but four different shows sci-tech Mondays gaming Tuesdays anime comic Wednesdays and indiscriminate Thursdays.

916
01:04:51,980 --> 01:04:55,156
Speaker 0: Geek nights is distributed under a Creative Commons attribution 3.0 license.

917
01:04:56,402 --> 01:04:59,480
Speaker 0: Geek nights is recorded live with no studio and no audience.

918
01:04:59,682 --> 01:05:09,690
Speaker 1: But unlike those other late shows It's actually recorded at night and the patreon patrons for this episode of geek nights are Alan Joyce had McNichol Mardi Gras Nia Graham Finch.

919
01:05:09,751 --> 01:05:11,199
Speaker 1: you hold the key to my heart.

920
01:05:11,722 --> 01:05:14,536
Speaker 1: The license plate said fresh and it had dice in the mirror.

921
01:05:14,636 --> 01:05:18,449
Speaker 1: Clinton Walton J Bats friend who's been from New Zealand Ryan parent Chris mint.

922
01:05:18,469 --> 01:05:20,857
Speaker 1: give dread lily tenebrae just like a dude guy.

923
01:05:21,198 --> 01:05:27,019
Speaker 1: Chris Reimer Finn chirping von Horrell and a bunch of people who do not want me to say their names.

924
01:05:28,141 --> 01:05:33,180
Speaker 1: We'll see if Scott actually watches decadence yet, but uh, yeah you saw that bit in the stream.

925
01:05:33,662 --> 01:05:35,933
Speaker 1: You should go check out that show and for right now.

926
01:05:35,953 --> 01:05:37,019
Speaker 1: I leave you with.

927
01:05:43,263 --> 01:05:49,319
Speaker 0: I'm such a loser My guts my guts.

