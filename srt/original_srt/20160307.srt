1
00:00:08,940 --> 00:00:10,444
Speaker 0: It's Monday, March 7th, 2016.

2
00:00:10,444 --> 00:00:10,926
Speaker 0: I'm Rem.

3
00:00:14,402 --> 00:00:14,900
Speaker 1: I'm Scott.

4
00:00:15,100 --> 00:00:16,685
Speaker 0: And this is Geek Nights tonight.

5
00:00:16,946 --> 00:00:19,373
Speaker 0: That whole iPhone encryption FBI thing.

6
00:00:19,393 --> 00:00:21,399
Speaker 0: because most of you don't seem to understand what's going on.

7
00:00:24,460 --> 00:00:25,458
Speaker 0: Let's do this.

8
00:00:28,370 --> 00:00:32,063
Speaker 0: So real soon, I'll have the video of one of our panels from PAX South up.

9
00:00:32,323 --> 00:00:33,950
Speaker 0: It took me a while because I was traveling and everything.

10
00:00:35,471 --> 00:00:37,799
Speaker 0: But I ran into a little bit of trouble because I edited it together.

11
00:00:38,361 --> 00:00:39,324
Speaker 0: and remember that knob?

12
00:00:39,445 --> 00:00:40,890
Speaker 0: The knob that I bought with the Patreon money?

13
00:00:42,830 --> 00:00:54,587
Speaker 0: It reduced the time it takes me from I have all the files on my computer to I'm rendering the video of the panel from a couple days work, like three or four days of work, like off and on, to.

14
00:00:54,607 --> 00:00:56,970
Speaker 0: it took me three hours to get the whole thing edited and done.

15
00:00:57,150 --> 00:01:01,990
Speaker 1: It tells me you're just bad at clicking and good at knobbing, which I understand because you play Kaboom a lot.

16
00:01:02,573 --> 00:01:04,470
Speaker 0: Yeah, Kaboom trained me to be a video editor.

17
00:01:04,870 --> 00:01:06,362
Speaker 1: Right, but I mean I'm pretty good at keyboard.

18
00:01:06,402 --> 00:01:07,270
Speaker 1: I think I could race you.

19
00:01:07,612 --> 00:01:09,060
Speaker 0: You want to have an editor race?

20
00:01:09,100 --> 00:01:10,750
Speaker 0: We should get someone to send us both the same.

21
00:01:11,255 --> 00:01:12,710
Speaker 1: I just got to learn some keyboard shortcuts.

22
00:01:13,630 --> 00:01:16,689
Speaker 0: Yeah, you could have a knob that has all the keyboard shortcuts and the macros built in.

23
00:01:16,890 --> 00:01:22,190
Speaker 1: This is the situation where somebody is buying something or doing work to avoid learning something.

24
00:01:22,490 --> 00:01:24,516
Speaker 0: No, because a lot of these are macros.

25
00:01:24,576 --> 00:01:26,140
Speaker 0: And two, the buttons are not the important thing.

26
00:01:26,160 --> 00:01:29,810
Speaker 0: The important thing is the shuttle and the jog for moving around within.

27
00:01:30,111 --> 00:01:31,190
Speaker 0: I just used my mouse wheel.

28
00:01:32,652 --> 00:01:35,219
Speaker 0: Do you use the mouse wheel in the latest updated version of Premiere?

29
00:01:35,239 --> 00:01:38,690
Speaker 0: Because they reversed what the direction is and they added momentum and some stuff.

30
00:01:39,530 --> 00:01:41,263
Speaker 1: They fucking Apple-fied it?

31
00:01:41,343 --> 00:01:42,250
Speaker 1: Can you change it in the settings?

32
00:01:42,690 --> 00:01:43,676
Speaker 0: I couldn't find a way to.

33
00:01:44,079 --> 00:01:45,829
Speaker 0: And basically I went to all those...

34
00:01:46,010 --> 00:01:47,517
Speaker 1: Because that's basically what a knob does.

35
00:01:47,537 --> 00:01:50,510
Speaker 1: It momentumifies and makes it go in the right direction.

36
00:01:50,870 --> 00:01:56,106
Speaker 0: The main reason I got this was, one, I missed having the jog and the shuttle, but that wasn't the most important thing.

37
00:01:56,146 --> 00:01:57,350
Speaker 0: I could have lived without having both.

38
00:01:57,711 --> 00:02:03,427
Speaker 0: But it was because the mouse was actually not working that well, because if you started scrolling too fast, it'd be like, "He must want to go faster.

39
00:02:03,447 --> 00:02:04,210
Speaker 0: I'm going to start skipping a longer

40
00:02:04,230 --> 00:02:04,330
Speaker 0: frame.".

41
00:02:04,350 --> 00:02:04,873
Speaker 1: Oh, but that's right.

42
00:02:04,933 --> 00:02:05,717
Speaker 1: We already discussed this.

43
00:02:05,757 --> 00:02:08,289
Speaker 1: You don't have the mouse wheel that unlocks and spins freely.

44
00:02:08,750 --> 00:02:12,984
Speaker 0: Yeah, but I tried one of those, because I had one at work, so I just brought it home and used it to edit video.

45
00:02:13,505 --> 00:02:14,950
Speaker 0: Doesn't work great with Premiere.

46
00:02:15,090 --> 00:02:16,349
Speaker 1: Was it a high-quality mouse?

47
00:02:17,554 --> 00:02:19,622
Speaker 0: Yeah, I have the company card.

48
00:02:19,743 --> 00:02:21,530
Speaker 0: I have the best computer stuff.

49
00:02:21,750 --> 00:02:24,249
Speaker 1: What was that Logitech mouse you tried at PAX once?

50
00:02:24,712 --> 00:02:26,610
Speaker 0: Oh yeah, it had the little shelf for your thumb.

51
00:02:26,710 --> 00:02:29,843
Speaker 1: The thing is, all those mice look basically like Digimon.

52
00:02:29,924 --> 00:02:31,310
Speaker 1: It's like, "I don't want to put my hand on

53
00:02:31,330 --> 00:02:31,410
Speaker 1: this.".

54
00:02:31,410 --> 00:02:34,110
Speaker 0: Well, I had that SteelSeries that I bought in 2009.

55
00:02:34,110 --> 00:02:35,289
Speaker 1: But that looked like a normal mouse.

56
00:02:35,650 --> 00:02:41,030
Speaker 0: I fucking love that SteelSeries, but it eventually died, and I actually a couple years ago replaced it with a Razer DeathAdder.

57
00:02:42,291 --> 00:02:44,330
Speaker 0: Spend the money on a good mouse, but I digress.

58
00:02:44,450 --> 00:02:46,358
Speaker 1: My Logitech mice, I've never had one die.

59
00:02:46,479 --> 00:02:48,568
Speaker 1: They just get dirty, and I'm like, "Eh, I don't feel like cleaning this

60
00:02:48,608 --> 00:02:48,809
Speaker 1: again.".

61
00:02:49,211 --> 00:02:52,308
Speaker 0: Well, that's what I get for getting the SteelSeries, because one of the clickers died.

62
00:02:52,771 --> 00:02:56,790
Speaker 1: I always buy the one that only costs tens of dollars, not the one that costs hundreds of dollars.

63
00:02:56,850 --> 00:02:57,868
Speaker 1: So I don't care about, you know.

64
00:02:58,291 --> 00:03:02,550
Speaker 0: I'll spend the $60 or $70, but I would not spend more than that on a mouse.

65
00:03:02,770 --> 00:03:03,319
Speaker 1: No, that's my max.

66
00:03:04,550 --> 00:03:06,009
Speaker 0: If I was a professional gamer, maybe.

67
00:03:07,150 --> 00:03:08,570
Speaker 1: If I'm a professional gamer, I hope to have it sponsored.

68
00:03:09,170 --> 00:03:13,090
Speaker 0: Yeah, but it would be a multi-hundred dollar mouse, probably custom-tuned to me.

69
00:03:13,390 --> 00:03:15,950
Speaker 1: I mean, your sponsor's gonna give you the top tier, right?

70
00:03:17,891 --> 00:03:19,097
Speaker 1: Here, I'm sponsoring you.

71
00:03:19,238 --> 00:03:21,610
Speaker 1: Please use my bicycle in the Tour de France.

72
00:03:21,830 --> 00:03:24,769
Speaker 1: It is Huffy from Walmart that is built in Craghood.

73
00:03:25,870 --> 00:03:26,594
Speaker 0: You know, that would be fun.

74
00:03:27,077 --> 00:03:29,350
Speaker 0: The sabotage sponsor, the saboteur.

75
00:03:29,510 --> 00:03:33,310
Speaker 0: Your company sponsors people and gives them garbage equipment just because it's really funny.

76
00:03:34,574 --> 00:03:35,890
Speaker 0: You might get more publicity for that.

77
00:03:36,210 --> 00:03:38,310
Speaker 1: Or, you know, Huffy, I mean, it is a bicycle factory.

78
00:03:38,410 --> 00:03:40,850
Speaker 1: They probably could make a good bike if they wanted to.

79
00:03:41,532 --> 00:03:46,550
Speaker 0: No, I imagine you sponsor Tiger Woods and you give him these garbage, broken golf clubs.

80
00:03:46,750 --> 00:03:50,670
Speaker 1: He has Tiger Woods golf clubs, so I think they're Nike clubs or something, whatever.

81
00:03:51,030 --> 00:03:57,050
Speaker 0: So anyway, I hit render when I was done, and it's 4K video because that's easy and everything was fine.

82
00:03:57,170 --> 00:04:06,670
Speaker 0: I had my same render template that I always have, and it was gonna downscale it from 4K to 2K because you're not getting a lot out of the 4K of me and Scott on a dark stage at a convention.

83
00:04:07,932 --> 00:04:09,749
Speaker 0: And then I walked away from the computer like I always do.

84
00:04:10,591 --> 00:04:13,648
Speaker 0: And later in the day, I came back to the room like, "Oh, that render's probably

85
00:04:13,688 --> 00:04:13,869
Speaker 0: done."

86
00:04:14,351 --> 00:04:25,670
Speaker 0: And I saw that it was not done, and I saw that it projected 115 more hours of rendering, and I saw that the time it was gonna take to finish was going up at about two minutes a second.

87
00:04:28,370 --> 00:04:32,647
Speaker 0: So I fussed around a little bit, and the first thing I thought is, "My computer does feel pretty

88
00:04:32,687 --> 00:04:33,009
Speaker 0: warm.".

89
00:04:33,711 --> 00:04:38,567
Speaker 0: And then I fussed around a little bit and realized that my processor was actually running at about 1.1 GHz.

90
00:04:41,291 --> 00:04:47,650
Speaker 0: And it was running very hot, so I pulled the computer apart and dusted it for one, because it's been a while.

91
00:04:48,110 --> 00:04:55,910
Speaker 0: And two, I discovered that basically all my case fans, except one on the front and the top one, had died at some point since 2009.

92
00:04:55,910 --> 00:04:57,049
Speaker 1: Why is your computer so bad?

93
00:04:57,652 --> 00:04:59,290
Speaker 0: Because the computer's from 2009.

94
00:04:59,290 --> 00:05:04,910
Speaker 1: But I mean, my computer is also very old, and I've never had a computer where the case fans have stopped working, ever.

95
00:05:05,270 --> 00:05:11,210
Speaker 0: So the case fans broke except for the front one and the top one, so I didn't notice that all these extraneous case fans died.

96
00:05:11,670 --> 00:05:15,710
Speaker 0: That wasn't a big problem, but it was also dusty in there, because it was winter, and I usually dust my computer in the spring.

97
00:05:16,191 --> 00:05:24,008
Speaker 0: So I dust it all out, and my processor's running back at its 2.6 GHz, and I hit render, and it's still gonna take, like, 90 hours.

98
00:05:26,111 --> 00:05:37,369
Speaker 0: So I did a little differential diagnosis beyond that point, and I realized that I had actually done more color correction than, like, those sort of, like, mastering effects than I usually do.

99
00:05:38,130 --> 00:05:42,710
Speaker 0: Because I finished the editing so early, I did all the polish that I usually don't bother with in these videos.

100
00:05:42,710 --> 00:05:43,970
Speaker 1: You're supposed to do that stuff at the beginning.

101
00:05:45,531 --> 00:05:46,450
Speaker 1: Yeah, usually.

102
00:05:46,570 --> 00:05:54,330
Speaker 0: No, because then you gotta turn off those lights, because usually if you apply a lot of those effects, they're actually intensive, and it'll slow down your jogging and seeking and scanning while you're editing.

103
00:05:54,652 --> 00:05:58,430
Speaker 0: You apply them to the master clip at the end, and it cascades through everywhere you use those clips.

104
00:05:58,610 --> 00:06:07,090
Speaker 1: Or you color correct and render everything, the raw clips at the start, and then you can edit them together without having to do it again later.

105
00:06:07,550 --> 00:06:08,712
Speaker 0: Nah, cool.

106
00:06:09,173 --> 00:06:21,638
Speaker 0: Not to get into my process, but basically, the combination of color correction effects I'd used appears to be just very adverse on consumer-level PCs, because my video card's an old 770.

107
00:06:21,638 --> 00:06:23,870
Speaker 0: It's not the fastest thing in the world anymore.

108
00:06:24,830 --> 00:06:33,990
Speaker 0: And I got the render time down to 30 hours, which is reasonable, by undoing a lot of the fancy stuff I was doing to clean up some of the fact that it was a really dark video.

109
00:06:34,490 --> 00:06:34,973
Speaker 0: And you know what?

110
00:06:35,717 --> 00:06:37,769
Speaker 0: It's a video of me and Scott in a dark room.

111
00:06:38,711 --> 00:06:44,230
Speaker 0: There is a noticeable difference if you really care, but not enough for me to care that much.

112
00:06:45,032 --> 00:06:52,208
Speaker 0: But this is the first time in my life that I had a computer that outlived its case, because when I took the computer part, I started poking around.

113
00:06:52,248 --> 00:06:53,170
Speaker 0: I noticed a few other things.

114
00:06:53,610 --> 00:07:01,010
Speaker 0: That front panel on the case that has USB headers and firewire headers and whatever, literally every part of that no longer works.

115
00:07:01,490 --> 00:07:02,108
Speaker 1: Oh, mine still works.

116
00:07:02,770 --> 00:07:04,170
Speaker 0: Yeah, mine does no longer work.

117
00:07:04,450 --> 00:07:05,550
Speaker 1: Why does your stuff suck so much?

118
00:07:05,650 --> 00:07:07,060
Speaker 0: It's an Antec 900, pretty standard case.

119
00:07:09,193 --> 00:07:10,650
Speaker 1: It's got a big window on the side.

120
00:07:11,090 --> 00:07:12,549
Speaker 1: Most cases have a big window on the side.

121
00:07:12,590 --> 00:07:13,657
Speaker 1: It's the standard Antec that I got.

122
00:07:13,678 --> 00:07:15,329
Speaker 1: that's just solid all sides.

123
00:07:16,210 --> 00:07:17,338
Speaker 1: You can't trust anything.

124
00:07:17,379 --> 00:07:18,850
Speaker 1: that's all gamer-y, you can't trust.

125
00:07:19,270 --> 00:07:22,410
Speaker 0: This isn't really all that gamer-y though, it just has a glass panel on one side.

126
00:07:22,591 --> 00:07:24,309
Speaker 1: I see some blue LEDs shining in my face right now.

127
00:07:24,490 --> 00:07:27,930
Speaker 0: That blue LED is actually an aftermarket fan that I put in when I built the thing.

128
00:07:28,732 --> 00:07:30,909
Speaker 1: Uh, maybe the aftermarket fan is what did you in?

129
00:07:31,130 --> 00:07:33,909
Speaker 0: No, actually that fan is the one fucking fan that's still working.

130
00:07:34,351 --> 00:07:35,270
Speaker 1: It doesn't look like it's spinning.

131
00:07:35,850 --> 00:07:35,954
Speaker 0: What?

132
00:07:35,975 --> 00:07:36,328
Speaker 0: Which one?

133
00:07:36,531 --> 00:07:37,068
Speaker 1: The bottom one.

134
00:07:37,491 --> 00:07:37,958
Speaker 0: Oh, the bottom one.

135
00:07:37,979 --> 00:07:38,710
Speaker 0: No, that stopped a long time ago.

136
00:07:38,710 --> 00:07:39,767
Speaker 1: That's the one with the LEDs in it.

137
00:07:40,553 --> 00:07:43,146
Speaker 0: Oh yeah, the top one's supposed to have an LED.

138
00:07:43,206 --> 00:07:44,050
Speaker 0: I guess the LEDs stopped.

139
00:07:45,771 --> 00:07:49,710
Speaker 1: Anyway, so in better opening bits, first of all, warm today!

140
00:07:50,750 --> 00:07:51,570
Speaker 0: Ah, it was nicely warm.

141
00:07:51,730 --> 00:07:52,517
Speaker 0: Good times.

142
00:07:52,759 --> 00:07:53,890
Speaker 1: It was nice and warm today.

143
00:07:54,591 --> 00:07:56,349
Speaker 1: And it was a warm day, I moved to a new office.

144
00:07:56,751 --> 00:07:58,029
Speaker 0: Oh yeah, you were talking about that.

145
00:07:58,150 --> 00:07:59,869
Speaker 1: It's across the street from the old office.

146
00:08:00,915 --> 00:08:01,530
Speaker 1: It's an office.

147
00:08:02,931 --> 00:08:06,998
Speaker 1: Instead of being in a little room with like six people, I'm at like a desk.

148
00:08:07,079 --> 00:08:11,590
Speaker 1: that's not really a cubicle, but sort of is, like the walls aren't really that tall.

149
00:08:12,091 --> 00:08:15,970
Speaker 1: It's like if you're sitting, you can't really see the person in front of you, but you can see the person to your right.

150
00:08:16,390 --> 00:08:17,309
Speaker 0: I've had desks like that before.

151
00:08:17,590 --> 00:08:18,550
Speaker 1: But there's like a little wall.

152
00:08:19,312 --> 00:08:25,570
Speaker 1: And we went to the Muji store and we all together, we loaded up on awesome stuff to see who would make their desk the fanciest.

153
00:08:25,750 --> 00:08:30,890
Speaker 0: And when my company moved into their current office, there was a startup that I guess had gone out of business like on another floor.

154
00:08:31,253 --> 00:08:34,347
Speaker 0: And they just went up there and they were like, "Here's a thousand bucks, just give us all your fucking

155
00:08:34,386 --> 00:08:34,690
Speaker 0: furniture.".

156
00:08:36,240 --> 00:08:42,769
Speaker 1: Yeah, we went and we bought like plants, and we bought little plastic drawer thingies, and everyone was getting good stuff.

157
00:08:44,018 --> 00:08:44,210
Speaker 1: All right.

158
00:08:44,490 --> 00:08:45,649
Speaker 1: So that was a good time.

159
00:08:46,652 --> 00:08:51,910
Speaker 1: Also, I'm near the window, so I have less people in the office walking past me and shit.

160
00:08:52,170 --> 00:08:57,650
Speaker 0: Well, it's an open floor plan, so I don't have like an office really, not like I did when I worked at IBM or the hospital or whatever.

161
00:08:58,211 --> 00:09:06,248
Speaker 0: But I'm in the corner of a big room, so I actually do have two corner windows right behind me, which is very nice because we opened all the windows today.

162
00:09:07,271 --> 00:09:11,289
Speaker 0: Of course, there was a jackhammer going off somewhere, but I get to know that because I had headphones on.

163
00:09:12,250 --> 00:09:15,788
Speaker 1: A lot of people who did not have the headphones previously are now trying to buy them.

164
00:09:17,031 --> 00:09:18,870
Speaker 0: So I actually had the weirdest experience.

165
00:09:19,251 --> 00:09:20,542
Speaker 0: I visited a company.

166
00:09:22,530 --> 00:09:30,348
Speaker 0: I have a meeting and I went to this company, and I walked through their space and they had one room that was like a big cube farm like you see, kind of like what you were talking about.

167
00:09:31,033 --> 00:09:34,030
Speaker 0: But it was real big, like real, real, real big.

168
00:09:34,751 --> 00:09:40,329
Speaker 0: And usually when they're real big like that, they have a white noise generator of some kind, so people don't go crazy.

169
00:09:40,654 --> 00:09:41,086
Speaker 1: You'd hope so.

170
00:09:41,510 --> 00:09:42,757
Speaker 0: This one was 100% silence.

171
00:09:46,213 --> 00:09:48,330
Speaker 0: There were hundreds of people in there.

172
00:09:48,690 --> 00:09:53,290
Speaker 0: There was one person on the phone, like way far away, and I heard every word he was saying.

173
00:09:53,812 --> 00:09:57,830
Speaker 0: I could feel the rage and anger of all those people working in that office.

174
00:09:58,375 --> 00:09:59,069
Speaker 0: It was palpable.

175
00:10:00,439 --> 00:10:02,130
Speaker 1: At least we have little walls.

176
00:10:02,630 --> 00:10:06,809
Speaker 1: Most people these days, I go to startup offices and shit, and they just have a long table.

177
00:10:07,431 --> 00:10:08,009
Speaker 1: One long table.

178
00:10:09,670 --> 00:10:11,509
Speaker 1: Computer, computer, computer, chair, chair, chair, chair.

179
00:10:11,630 --> 00:10:13,229
Speaker 0: Yeah, a bunch of MacBooks, a bunch of Aerons.

180
00:10:14,351 --> 00:10:15,470
Speaker 1: Even monitors and stuff.

181
00:10:15,873 --> 00:10:17,030
Speaker 1: It's just a long table.

182
00:10:17,210 --> 00:10:19,230
Speaker 1: And then facing it, another long table.

183
00:10:19,370 --> 00:10:20,503
Speaker 1: And I'm like, "Wow, how do you work like

184
00:10:20,543 --> 00:10:20,726
Speaker 1: that?".

185
00:10:21,251 --> 00:10:22,862
Speaker 0: If I ever make my own office that's mine...

186
00:10:24,050 --> 00:10:25,369
Speaker 1: Even for way more hours than I work.

187
00:10:25,871 --> 00:10:30,610
Speaker 0: I would probably get a sit-up stand-up desk and a super lounging hammock nonsense.

188
00:10:30,950 --> 00:10:34,550
Speaker 1: I would have... Everyone has an actual office office.

189
00:10:34,871 --> 00:10:35,970
Speaker 1: It doesn't have to be big ones.

190
00:10:37,253 --> 00:10:38,909
Speaker 1: Everyone would have a door and walls.

191
00:10:40,310 --> 00:10:42,730
Speaker 1: You should see their designs that exist these days.

192
00:10:43,110 --> 00:10:46,799
Speaker 1: You can have a lot of those kind of spaces without having actual square rooms with doors.

193
00:10:49,530 --> 00:10:52,330
Speaker 1: There are other ways to have doors with less space.

194
00:10:52,790 --> 00:10:54,089
Speaker 0: I'd be happy with the George Costanza.

195
00:10:54,492 --> 00:10:55,809
Speaker 0: The nap room in my desk.

196
00:10:56,733 --> 00:10:57,325
Speaker 0: That'd be good enough.

197
00:10:58,191 --> 00:11:00,907
Speaker 0: Honestly, I just need a place I can go take a nap without people bothering me.

198
00:11:01,710 --> 00:11:05,250
Speaker 0: I can take a nap in the middle of my office without just falling asleep in my chair.

199
00:11:05,510 --> 00:11:09,069
Speaker 1: What I really want to do is just be able to play music without having to wear headphones.

200
00:11:09,750 --> 00:11:11,390
Speaker 0: I have really good headphones, though.

201
00:11:12,070 --> 00:11:13,409
Speaker 0: Like, really good headphones.

202
00:11:13,770 --> 00:11:16,010
Speaker 1: Sure, but not wearing headphones.

203
00:11:16,130 --> 00:11:19,890
Speaker 1: Just sitting there comfortably with nothing on my head, no wires and just music playing.

204
00:11:20,010 --> 00:11:20,530
Speaker 1: That's way better.

205
00:11:21,431 --> 00:11:23,490
Speaker 0: We've got to found a company and we'll have two offices.

206
00:11:24,151 --> 00:11:25,409
Speaker 1: Well, it's called Beach.

207
00:11:25,870 --> 00:11:26,400
Speaker 1: Just lay on a beach.

208
00:11:26,462 --> 00:11:26,808
Speaker 1: Don't work.

209
00:11:27,511 --> 00:11:28,804
Speaker 0: I visited a company that did that.

210
00:11:28,845 --> 00:11:29,330
Speaker 0: Big company.

211
00:11:29,690 --> 00:11:32,029
Speaker 0: They put their main offices in SoCal by the beach.

212
00:11:32,971 --> 00:11:33,939
Speaker 1: No, no, no.

213
00:11:34,161 --> 00:11:35,270
Speaker 1: See, that's not what I'm talking about.

214
00:11:35,290 --> 00:11:36,909
Speaker 1: That's an office building next to a beach.

215
00:11:37,150 --> 00:11:39,166
Speaker 0: So you want to be sitting on the beach with Wi-Fi.

216
00:11:39,910 --> 00:11:48,390
Speaker 1: There should be a box that contains a bunch of Wi-Fi equipment and a cable that's protected from elements and then a pool and then sand and then ocean.

217
00:11:49,150 --> 00:11:50,290
Speaker 1: You know, because you need the pool sometimes.

218
00:11:50,290 --> 00:11:53,650
Speaker 0: Wait, Scott, I've got a high priority Jira ticket for you.

219
00:11:53,890 --> 00:11:55,990
Speaker 0: We need another sandcastle.

220
00:11:56,450 --> 00:12:04,930
Speaker 1: Everyone lays down in some beach chair action with some umbrellas and some, you know, you've got an infinite supply of lotions, bathing suits, towels.

221
00:12:05,470 --> 00:12:08,390
Speaker 0: You've got a little... Putting on lotion, sitting by the ocean.

222
00:12:08,410 --> 00:12:08,580
Speaker 0: Exactly.

223
00:12:09,332 --> 00:12:10,230
Speaker 0: Rubbing it on my body.

224
00:12:10,470 --> 00:12:11,348
Speaker 1: A little tiki bar action.

225
00:12:12,754 --> 00:12:14,510
Speaker 1: You know, just a little cabana or whatever.

226
00:12:14,890 --> 00:12:15,729
Speaker 1: Everyone just hanging out.

227
00:12:16,131 --> 00:12:17,409
Speaker 1: And you've got a laptop and you can still work.

228
00:12:17,883 --> 00:12:18,070
Speaker 0: Yeah.

229
00:12:18,530 --> 00:12:20,930
Speaker 0: Or why don't we just be rich and just do that without the working?

230
00:12:21,090 --> 00:12:21,763
Speaker 1: That's what I want to do.

231
00:12:23,375 --> 00:12:28,710
Speaker 0: The reason I brought up the whole thing is that I have in front of me, you know, my computer, the invoice from when I bought this computer.

232
00:12:29,433 --> 00:12:34,910
Speaker 0: I think this computer is going to last at least another ten years because nothing on it has ever failed except the case.

233
00:12:36,132 --> 00:12:39,130
Speaker 0: And the case is real old and cases just kind of suck.

234
00:12:39,430 --> 00:12:42,449
Speaker 1: Every computer I've ever gotten has lasted five to six years.

235
00:12:42,991 --> 00:12:44,650
Speaker 1: Except for those Athlons that exploded.

236
00:12:45,170 --> 00:12:48,950
Speaker 0: Yeah, but you can't blame the kids who had Athlons because every Athlon exploded.

237
00:12:49,290 --> 00:12:51,250
Speaker 1: We couldn't afford Pentium 4s or whatever it was in those days.

238
00:12:51,410 --> 00:12:52,830
Speaker 0: Yeah, also the Athlons were way fast.

239
00:12:53,312 --> 00:12:54,470
Speaker 1: And we knew better than to get RD RAM.

240
00:12:56,755 --> 00:12:59,290
Speaker 0: But yeah, I guess it proves that way back.

241
00:12:59,431 --> 00:13:02,544
Speaker 0: I spent thousands of dollars in 2009 on a top of the line computer.

242
00:13:04,190 --> 00:13:05,485
Speaker 0: Like the best Core i7.

243
00:13:06,050 --> 00:13:08,610
Speaker 0: Like I bought everything top of the line.

244
00:13:09,131 --> 00:13:12,070
Speaker 0: And it's still a super fast computer.

245
00:13:12,351 --> 00:13:17,730
Speaker 0: I don't think I need to buy a new computer this year unless I jinxed myself and something blows up on the motherboard soon.

246
00:13:17,890 --> 00:13:19,320
Speaker 1: I mean I have 32 gigs of RAM and a 680 and an i7.

247
00:13:21,414 --> 00:13:22,408
Speaker 1: What is there to upgrade to?

248
00:13:22,852 --> 00:13:23,946
Speaker 0: So Scott, my computer is so old.

249
00:13:24,710 --> 00:13:27,253
Speaker 0: As I've said, I've got USB 2.

250
00:13:27,253 --> 00:13:28,689
Speaker 0: I've got SATA 3 gigabit.

251
00:13:29,830 --> 00:13:31,090
Speaker 0: That's already too serious problems.

252
00:13:31,610 --> 00:13:36,190
Speaker 1: Not having USB 3 is kind of eh, but other than that it's like you know, SATA 3 is fine.

253
00:13:36,490 --> 00:13:37,150
Speaker 1: Not really.

254
00:13:37,290 --> 00:13:38,850
Speaker 1: How often are you filling your SATA pipe?

255
00:13:39,430 --> 00:13:44,909
Speaker 0: So when I'm editing 4K video and I'm streaming it live off the disc I bump into that problem sometimes.

256
00:13:45,390 --> 00:13:46,089
Speaker 1: I do not have that problem.

257
00:13:46,630 --> 00:13:48,469
Speaker 0: How often do you edit hour long 4K videos?

258
00:13:48,632 --> 00:13:49,109
Speaker 1: I'm just saying.

259
00:13:49,190 --> 00:13:49,548
Speaker 1: In Premiere.

260
00:13:50,131 --> 00:13:52,310
Speaker 1: I don't think the disc is actually the bottleneck there.

261
00:13:53,773 --> 00:13:55,210
Speaker 0: Well the video card is not the bottleneck.

262
00:13:55,450 --> 00:13:56,470
Speaker 1: And actually I can see the disc.

263
00:13:56,610 --> 00:14:00,890
Speaker 1: And even if the disc is the bottleneck, I think it's the disc itself and not the pipe between the disc and the motherboard.

264
00:14:01,010 --> 00:14:05,292
Speaker 0: Well I can see the disc getting pegged and it appears that this would not be the case with 6.

265
00:14:05,292 --> 00:14:06,450
Speaker 0: But anyway, I digress.

266
00:14:07,751 --> 00:14:16,830
Speaker 0: Now, like the next four or five months I think is a similar performance price plateau to when I built this computer back in 2009.

267
00:14:16,830 --> 00:14:22,050
Speaker 0: So if you need a computer, buy one this year and you can probably keep it for a decade.

268
00:14:22,410 --> 00:14:23,930
Speaker 1: Moore's Law is sort of, right?

269
00:14:24,090 --> 00:14:25,430
Speaker 1: So you can just buy a computer whenever.

270
00:14:25,890 --> 00:14:29,570
Speaker 0: But a bunch of new standards just got, you know, sort of accepted.

271
00:14:29,933 --> 00:14:35,630
Speaker 0: So if you buy a computer in the next few months it'll have all the stuff to where by the time the new stuff comes out.

272
00:14:35,851 --> 00:14:40,850
Speaker 0: you'll be like I am now with an ancient computer that's still fast enough but it doesn't have USB 8 or whatever.

273
00:14:41,090 --> 00:14:43,330
Speaker 1: Things aren't advancing enough to where it's pretty much.

274
00:14:43,471 --> 00:14:45,910
Speaker 1: it's always a safe time to buy a computer basically these days.

275
00:14:46,230 --> 00:14:49,390
Speaker 1: Because it's, you know, thus you're buying like a Mac, right?

276
00:14:49,410 --> 00:14:53,030
Speaker 1: In which case like you buy one right before the new Mac come out and it's just like ahhh.

277
00:14:53,690 --> 00:14:56,710
Speaker 1: But other than that, it's for PCs, buy one whenever.

278
00:14:56,911 --> 00:14:57,549
Speaker 1: No, it doesn't matter.

279
00:14:57,930 --> 00:14:58,051
Speaker 0: Yep.

280
00:14:58,413 --> 00:15:03,289
Speaker 0: And you know the idea that you can get an upgradeable PC really panned out for me.

281
00:15:03,370 --> 00:15:05,050
Speaker 0: Because I've upgraded this thing a whole bunch of times.

282
00:15:05,190 --> 00:15:07,049
Speaker 0: I doubled the RAM in it for like 20 bucks.

283
00:15:07,574 --> 00:15:09,209
Speaker 0: Do you got any news?

284
00:15:10,670 --> 00:15:11,610
Speaker 1: Yeah, I do have a news.

285
00:15:11,930 --> 00:15:16,546
Speaker 1: So we are never going to stop talking about Google's or Europe's right to be forgotten.

286
00:15:16,586 --> 00:15:19,310
Speaker 1: nonsense and Google valiantly fighting against it, right?

287
00:15:19,350 --> 00:15:21,089
Speaker 0: It's one of the worst things Europe's ever done.

288
00:15:21,550 --> 00:15:21,691
Speaker 1: Right.

289
00:15:21,711 --> 00:15:22,576
Speaker 1: But anyway, so...

290
00:15:25,610 --> 00:15:26,028
Speaker 0: Anyway.

291
00:15:26,331 --> 00:15:27,029
Speaker 0: I'm glad you caught that.

292
00:15:27,370 --> 00:15:27,970
Speaker 1: Yes, anyway.

293
00:15:29,831 --> 00:15:31,590
Speaker 1: Basically the new development is this, right?

294
00:15:31,750 --> 00:15:36,450
Speaker 1: Europe was upset that if you were in Europe let's say you're in Germany, right?

295
00:15:36,510 --> 00:15:42,750
Speaker 1: And you go to google.de, the German Google, and you search for someone who was forgotten, you won't find the results that were forgotten.

296
00:15:43,292 --> 00:15:49,910
Speaker 1: But you could just go to your browser and type google.com or google.non-european-country, you know, say Brazil.

297
00:15:50,713 --> 00:15:57,448
Speaker 1: And if you could speak Portuguese or English, then you could search and the results that were supposed to be forgotten showed up.

298
00:15:58,193 --> 00:16:04,070
Speaker 1: And it's like, well, they were sort of like, hey, people just in Europe shouldn't be able to find these results at all.

299
00:16:04,110 --> 00:16:08,110
Speaker 1: And they were like, you gotta delete them from all the Googles, all the different countries, right?

300
00:16:08,832 --> 00:16:13,770
Speaker 1: If German dude wants to be forgotten, he needs to be forgotten everywhere, not just in Europe.

301
00:16:14,110 --> 00:16:18,909
Speaker 0: This is not to forget the actual information, like so-and-so was acquitted of this thing, or so-and-so is a jerk.

302
00:16:19,130 --> 00:16:22,150
Speaker 1: It's just remove the search results from Google so it's hard to find.

303
00:16:22,290 --> 00:16:24,870
Speaker 0: Yeah, they're not even going after smaller search providers.

304
00:16:25,090 --> 00:16:26,489
Speaker 0: They're pretty much just going after Google.

305
00:16:26,790 --> 00:16:31,310
Speaker 1: Right, and they're not going after the person who's hosting the website with the forgotten information, right?

306
00:16:31,390 --> 00:16:36,710
Speaker 1: So it's like, I make a blog post with photos of RIM's early days, and RIM says I need that to be forgotten.

307
00:16:36,890 --> 00:16:40,969
Speaker 1: We can't be having Dragon High School RIM on the Internet anymore, right?

308
00:16:41,610 --> 00:16:48,189
Speaker 1: So it doesn't show up in the Google search results, but if you can find the URL, you can still find -- they don't come after me.

309
00:16:48,430 --> 00:16:49,621
Speaker 0: Yeah, there's still links to it.

310
00:16:49,682 --> 00:16:50,490
Speaker 0: It still exists.

311
00:16:50,770 --> 00:16:54,290
Speaker 0: They're literally trying to -- it's not like move the headstones, leave the bodies.

312
00:16:54,610 --> 00:16:57,788
Speaker 0: It's leave the bodies and the headstones, but take the street sign down.

313
00:16:57,990 --> 00:17:00,190
Speaker 1: It's don't burn the books, just rip the index off the back.

314
00:17:00,952 --> 00:17:09,369
Speaker 1: Anyway, so basically what Google has agreed to do is this, which is kind of fine by me, actually.

315
00:17:10,031 --> 00:17:29,335
Speaker 1: It shows that Google is actually doing the right thing, or at least knows the right thing to do, is that now, after they put the change out, which they may have done already, if not, they're going to do it soon, is if you're in Germany, say, or France, or whatever Europe place, Belgium, and you do the search, it doesn't

316
00:17:29,375 --> 00:17:30,039
Speaker 0: matter which

317
00:17:30,803 --> 00:17:32,090
Speaker 1: country Google you use.

318
00:17:32,230 --> 00:17:38,944
Speaker 1: It doesn't matter if you go to com, or fee, or FIFO, or FUM, or D, or UK, or CX.

319
00:17:41,732 --> 00:17:48,046
Speaker 1: You're not going to see forgotten search results if you're in Europe, but if you were to, say, get a little plane, come over to the

320
00:17:48,167 --> 00:17:48,470
Speaker 1: U.S.,

321
00:17:49,534 --> 00:17:54,917
Speaker 1: where the good people are, where the people who know how to Internet are, anyway, it doesn't matter which Google -- if you're in the U.S.

322
00:17:54,937 --> 00:17:59,206
Speaker 1: and you go to Google.France, you're going to see the forgotten shit, because you're in the U.S.

323
00:17:59,247 --> 00:17:59,430
Speaker 1: Yeah.

324
00:17:59,650 --> 00:18:01,330
Speaker 0: -So, what argument could Europe make now?

325
00:18:01,510 --> 00:18:02,909
Speaker 0: Google is doing everything they wanted.

326
00:18:03,310 --> 00:18:03,657
Speaker 0: -Yeah.

327
00:18:03,800 --> 00:18:04,209
Speaker 1: Well, no.

328
00:18:04,410 --> 00:18:06,467
Speaker 1: They wanted the person to be forgotten in the U.S.

329
00:18:06,608 --> 00:18:06,810
Speaker 1: also.

330
00:18:06,890 --> 00:18:09,557
Speaker 1: So, now, someone in Europe who does a little VPN over to the U.S.

331
00:18:09,597 --> 00:18:10,769
Speaker 1: can get real search results.

332
00:18:11,010 --> 00:18:11,414
Speaker 0: -Oh, no.

333
00:18:11,474 --> 00:18:13,170
Speaker 0: Someone in Europe can actually access free speech.

334
00:18:13,370 --> 00:18:13,695
Speaker 0: -Yeah.

335
00:18:13,715 --> 00:18:14,630
Speaker 1: Well, that's a shame.

336
00:18:16,253 --> 00:18:22,309
Speaker 1: So, yeah, if I was Google, though, they still didn't do what I really wanted them to do, which is, fine, no Google for you.

337
00:18:22,690 --> 00:18:22,944
Speaker 0: -Yep.

338
00:18:23,390 --> 00:18:27,030
Speaker 1: -Google should just tell that to everyone, which is, like, we do things the way we want.

339
00:18:27,131 --> 00:18:28,290
Speaker 1: Otherwise, you don't have any Google.

340
00:18:28,511 --> 00:18:29,189
Speaker 1: Gmail just -- Oh!

341
00:18:29,350 --> 00:18:30,907
Speaker 1: You can't access Gmail from your country.

342
00:18:30,927 --> 00:18:31,190
Speaker 1: Sorry.

343
00:18:31,690 --> 00:18:35,235
Speaker 1: And then when the millions of Gmail users in their country go, "Holy fuck!

344
00:18:35,417 --> 00:18:36,710
Speaker 1: I can't access my e-mail.

345
00:18:36,810 --> 00:18:37,660
Speaker 1: I can't do anything.

346
00:18:37,680 --> 00:18:38,570
Speaker 1: My life is ruined.

347
00:18:39,071 --> 00:18:40,184
Speaker 1: Let Google have their way.

348
00:18:40,265 --> 00:18:40,569
Speaker 1: Holy

349
00:18:40,589 --> 00:18:40,690
Speaker 1: fuck!".

350
00:18:41,750 --> 00:18:42,490
Speaker 0: You're going to go to Bing?

351
00:18:42,810 --> 00:18:44,309
Speaker 0: You're going to type bing.com in your browser?

352
00:18:44,370 --> 00:18:45,748
Speaker 0: -They just turn off people's e-mail.

353
00:18:46,030 --> 00:18:47,710
Speaker 0: What are you going to do about it, right?

354
00:18:47,850 --> 00:18:49,710
Speaker 0: -Well, to be fair, only old people use e-mail.

355
00:18:50,190 --> 00:18:50,432
Speaker 1: -Yeah.

356
00:18:50,513 --> 00:18:52,470
Speaker 1: It's like, you know, that's the problem.

357
00:18:52,570 --> 00:18:55,704
Speaker 1: If Google didn't become a public company, if it was private, they could do stuff like that.

358
00:18:55,725 --> 00:18:56,110
Speaker 1: -Yeah.

359
00:18:56,110 --> 00:19:01,086
Speaker 1: -But because they're public, the shareholders would be like, "Uh, you just lost, like, the jillions of revenues from

360
00:19:01,167 --> 00:19:01,389
Speaker 1: Europe.".

361
00:19:01,811 --> 00:19:16,947
Speaker 0: -I mean, the reason -- It seems like Europe is sort of, like, a very easy example for us to talk about, but the reason I'm so opposed to this isn't necessarily because I'm opposed to them trying to sort of quiet down the fact that, say, someone was a user of pedophilia and then found it.

362
00:19:16,967 --> 00:19:17,150
Speaker 0: -Yeah, no.

363
00:19:17,150 --> 00:19:18,990
Speaker 1: They have good intent, right?

364
00:19:19,150 --> 00:19:21,450
Speaker 1: They're just -- Their method is wrong.

365
00:19:21,530 --> 00:19:32,570
Speaker 0: -But letting anyone stifle that sort of speech or that sort of access to true information on the Internet is a short step away from -- -Or even false information, right?

366
00:19:32,610 --> 00:19:32,713
Speaker 1: -Yeah.

367
00:19:32,734 --> 00:19:35,929
Speaker 1: -Someone may -- If it's false information -- -Well, libel and slander should handle that.

368
00:19:36,050 --> 00:19:38,630
Speaker 1: -Sure, and it could be considered an artwork, right?

369
00:19:38,670 --> 00:19:42,649
Speaker 1: And you're gonna -- You know, just because someone else doesn't like it, you're going to, you know --.

370
00:19:42,690 --> 00:19:43,880
Speaker 0: -Well, that's another thing.

371
00:19:43,981 --> 00:19:44,506
Speaker 0: where, in the

372
00:19:44,607 --> 00:19:44,909
Speaker 0: U.S.,

373
00:19:45,091 --> 00:19:51,604
Speaker 0: if you slander someone and you're not lying versus if you slander someone and you're making it up, we handle that pretty well.

374
00:19:51,624 --> 00:19:52,030
Speaker 0: -Yeah.

375
00:19:52,030 --> 00:19:56,670
Speaker 0: -As opposed to Britain, at least, where, even if you're right, you might still get in trouble.

376
00:19:57,350 --> 00:20:10,326
Speaker 1: -Dude, from watching so many Korean TVs and such and, you know, going to places where people who also watch them on the Internet discuss things, the libel and slander laws there are like, "You just don't say anything about anyone, and you'll be

377
00:20:10,367 --> 00:20:10,628
Speaker 1: safe.".

378
00:20:10,870 --> 00:20:11,414
Speaker 0: -But that's it.

379
00:20:11,515 --> 00:20:12,664
Speaker 0: Never forget, the U.S.

380
00:20:12,684 --> 00:20:17,689
Speaker 1: -- -They legally go after people who post comments online that are mean.

381
00:20:18,090 --> 00:20:25,730
Speaker 1: Like, if you troll -- If you go and, like, post comments on a news article in Korea, they could come after you if you say stuff like, "Uh, you know, try to harass someone,".

382
00:20:25,730 --> 00:20:28,316
Speaker 1: or it's like, "That's sort of a nice thing when you think about

383
00:20:28,357 --> 00:20:28,417
Speaker 1: it.".

384
00:20:28,800 --> 00:20:33,645
Speaker 1: But it actually goes -- It's like, "Yeah, we wish we could have the police go after trolls, right, or sue

385
00:20:33,685 --> 00:20:34,069
Speaker 1: trolls.".

386
00:20:34,291 --> 00:20:35,390
Speaker 1: -So, see, we can't let Google --.

387
00:20:35,390 --> 00:20:37,170
Speaker 1: -But they take it way too far.

388
00:20:37,370 --> 00:20:46,410
Speaker 0: -We can't let Google be beholden to European rules because Europe is pretty bad compared to the U.S The whole rest of the world is way worse than Europe.

389
00:20:46,650 --> 00:20:47,013
Speaker 1: -Mm-hmm.

390
00:20:47,034 --> 00:20:50,869
Speaker 0: -I mean, imagine what Saudi Arabia would do if they could tell Google to take search results down.

391
00:20:50,950 --> 00:20:54,106
Speaker 1: -I think they just, you know -- They censor their whole Internet, right?

392
00:20:54,126 --> 00:20:54,410
Speaker 1: Don't they?

393
00:20:54,410 --> 00:20:58,370
Speaker 0: -Yeah, but imagine if they could censor other people's Internet outside of Saudi Arabia.

394
00:20:59,391 --> 00:21:00,170
Speaker 0: That's the problem.

395
00:21:00,590 --> 00:21:05,827
Speaker 1: -Yeah, I think that's Google's excuse for limiting search results in countries like Saudi Arabia and China.

396
00:21:06,150 --> 00:21:07,788
Speaker 0: -Some access is better than no access.

397
00:21:08,050 --> 00:21:16,304
Speaker 1: -No, it's because, basically, they just -- The only search results they cut out are search results to things that are blocked, so it's like, "Why should they provide a search result to something you can't visit

398
00:21:16,365 --> 00:21:16,649
Speaker 1: anyway?".

399
00:21:16,690 --> 00:21:21,486
Speaker 0: -Well, a better way is to provide those results anyway and say, "Yeah, your fucking government won't let you see

400
00:21:21,526 --> 00:21:21,708
Speaker 0: this.".

401
00:21:22,010 --> 00:21:22,862
Speaker 1: -They could.

402
00:21:22,882 --> 00:21:23,410
Speaker 1: -Yeah.

403
00:21:23,852 --> 00:21:25,630
Speaker 0: They kind of did for a while in a lot of places.

404
00:21:25,650 --> 00:21:29,169
Speaker 1: -They could just, like, have search results, then put, like, big red X's on the ones you can't go to.

405
00:21:29,330 --> 00:21:30,144
Speaker 0: -Well, it's very analogous.

406
00:21:30,490 --> 00:21:33,149
Speaker 1: -But their names themselves might get blocked, you know?

407
00:21:33,450 --> 00:21:40,290
Speaker 0: -Well, like, Starbucks in Saudi Arabia plays up to the severe gender discrimination, and there's separate entrances for men and women.

408
00:21:40,430 --> 00:21:40,755
Speaker 0: -Sure.

409
00:21:40,775 --> 00:21:41,870
Speaker 0: -Yeah, is that okay?

410
00:21:41,970 --> 00:21:46,050
Speaker 0: Should Starbucks be allowing that to be in that market, or should they take a moral high ground?

411
00:21:46,150 --> 00:21:47,099
Speaker 1: -It's a public company.

412
00:21:47,160 --> 00:21:48,029
Speaker 1: You can invest in it, right?

413
00:21:48,190 --> 00:21:54,770
Speaker 1: So, because the shareholders only care about money, they have to do that, regardless of -- even if all the employees think otherwise.

414
00:21:54,970 --> 00:21:56,289
Speaker 0: -Never go public unless you're selling out.

415
00:21:56,770 --> 00:22:01,690
Speaker 1: -Right, 'cause then you basically have to do what makes you the most money, regardless of whether you agree with it or not.

416
00:22:01,810 --> 00:22:06,302
Speaker 1: Even if the CEO disagreed with it, the shareholders would just be like, "Did that just make us lose

417
00:22:06,343 --> 00:22:06,525
Speaker 1: money?".

418
00:22:07,031 --> 00:22:07,779
Speaker 1: Well, you're out.

419
00:22:07,799 --> 00:22:08,810
Speaker 1: -Caduciary duty.

420
00:22:09,150 --> 00:22:09,633
Speaker 1: -That's right.

421
00:22:09,654 --> 00:22:15,110
Speaker 0: -So, food poisoning is something I've experienced three times in my life severely.

422
00:22:16,032 --> 00:22:16,778
Speaker 0: Not a fan.

423
00:22:17,282 --> 00:22:18,069
Speaker 0: Your day will come.

424
00:22:18,712 --> 00:22:20,510
Speaker 1: -I just know, 'cause I eat food safely.

425
00:22:21,230 --> 00:22:22,850
Speaker 0: -Yeah, but, statistically, your day will come.

426
00:22:22,950 --> 00:22:25,580
Speaker 0: Though, we have some of the best food safety in the world in the

427
00:22:25,681 --> 00:22:25,984
Speaker 0: U.S.,

428
00:22:26,024 --> 00:22:27,630
Speaker 0: which is shocking when you think about how bad it is.

429
00:22:27,650 --> 00:22:29,463
Speaker 1: -This is the thing that -- It's like, everything in the U.S.

430
00:22:29,524 --> 00:22:30,781
Speaker 1: is so bad, and people in the U.S.

431
00:22:30,842 --> 00:22:32,185
Speaker 1: always complain about things in the U.S.

432
00:22:32,246 --> 00:22:32,998
Speaker 1: are bad, and you know what?

433
00:22:33,180 --> 00:22:33,769
Speaker 1: They are bad.

434
00:22:34,491 --> 00:22:36,550
Speaker 1: Almost every other place on Earth is worse.

435
00:22:36,650 --> 00:22:39,397
Speaker 0: -Yeah, look at food poisoning statistics in South Korea or Japan or Europe.

436
00:22:41,451 --> 00:22:42,143
Speaker 0: Good God.

437
00:22:42,163 --> 00:22:42,509
Speaker 0: -Yeah.

438
00:22:43,492 --> 00:22:45,565
Speaker 0: -So, we inspect restaurants.

439
00:22:45,586 --> 00:22:46,250
Speaker 0: We have all this stuff.

440
00:22:46,430 --> 00:22:48,050
Speaker 0: So, remember, way back -- I've talked about this a lot.

441
00:22:48,210 --> 00:23:05,990
Speaker 0: There is a -- Like, machine learning hit this sort of critical mass where, suddenly, it's causing a lot of really important changes to happen, and not many people are sort of realizing that all these advances that happened in the last few months are basically all underpinned by a small number of machine-learning algorithms that are being used more widely.

442
00:23:06,410 --> 00:23:08,790
Speaker 0: -Right, I mean, you just get enough data together, right?

443
00:23:08,870 --> 00:23:14,864
Speaker 1: And instead of coming up with a way to program a computer to work like a human brain, it's just like, "Well, we'll just get so much data that it doesn't

444
00:23:14,904 --> 00:23:15,147
Speaker 1: matter.".

445
00:23:15,470 --> 00:23:20,809
Speaker 0: -So, food poisoning is a problem, and a machine-learning algorithm has been set.

446
00:23:20,829 --> 00:23:22,209
Speaker 0: -- Now, they call it n-emesis.

447
00:23:23,330 --> 00:23:24,730
Speaker 0: Do you know what the word "emesis" means?

448
00:23:25,274 --> 00:23:25,355
Speaker 1: -No.

449
00:23:25,376 --> 00:23:26,430
Speaker 0: -The process or act of...

450
00:23:26,430 --> 00:23:27,523
Speaker 1: -It sounds like osmosis.

451
00:23:27,543 --> 00:23:28,109
Speaker 1: -...vomiting.

452
00:23:28,970 --> 00:23:29,192
Speaker 0: Emesis.

453
00:23:29,212 --> 00:23:31,290
Speaker 0: That's why enemetic is a drug that makes you vomit.

454
00:23:31,910 --> 00:23:32,837
Speaker 0: -Mm-hmm.

455
00:23:32,857 --> 00:23:36,465
Speaker 0: -So, they call it n-emesis, like enemy or enemy of emesis.

456
00:23:36,566 --> 00:23:36,970
Speaker 0: It's clever.

457
00:23:37,813 --> 00:23:44,370
Speaker 0: And they wanted to use it to analyze social media to identify the sources of food poisoning.

458
00:23:45,050 --> 00:23:45,709
Speaker 0: It was an experiment.

459
00:23:46,490 --> 00:23:51,930
Speaker 0: And rather than just being kind of cool or, like, interesting, it ended up being way better than inspections.

460
00:23:53,151 --> 00:24:03,005
Speaker 0: So, analyzing social media with heuristic algorithms to basically looking for, like, "All right, a bunch of people use these kinds of words in this area, which correlates with food

461
00:24:03,046 --> 00:24:03,370
Speaker 0: poisoning.".

462
00:24:03,794 --> 00:24:14,010
Speaker 0: And then the algorithm would go one step further and eventually figure out, by linking social profiles together, which restaurants caused the food poisoning or which restaurants likely had violations.

463
00:24:14,771 --> 00:24:25,190
Speaker 0: And it is 63% more effective than any actual, like, modern system of inspections or data analysis that have ever been used to detect food poisoning.

464
00:24:25,530 --> 00:24:28,510
Speaker 1: There's a related but similarly interesting story, right?

465
00:24:28,771 --> 00:24:40,710
Speaker 1: There's this thing that police use, which is, like, some sort of method where you put the, you know, the pins of locations of, like, crimes on a map, and then you can sort of usually figure out where the person lives, right, or where their base was or something.

466
00:24:40,710 --> 00:24:43,749
Speaker 0: Well, yeah, like, the serial killer is, like, within a radius of their house.

467
00:24:44,110 --> 00:24:50,430
Speaker 1: Right, no, but there's actually, like, a method where, like, you can figure out based on, like, the transportation network and other things, like, where the person's likely from.

468
00:24:50,631 --> 00:24:51,490
Speaker 0: I played Scotland Yard.

469
00:24:51,991 --> 00:24:53,650
Speaker 1: Right, but it's even better than that.

470
00:24:53,991 --> 00:24:54,861
Speaker 1: There's a name for it.

471
00:24:54,901 --> 00:24:56,910
Speaker 1: If I had the name for it, you could read the Wikipedia about it.

472
00:24:56,930 --> 00:24:59,130
Speaker 1: It's much more interesting than just, like, radius, right?

473
00:24:59,211 --> 00:25:04,790
Speaker 0: I actually read about it recently because I was reading about the Zodiac Killer because I was going to make a really nuanced Ted Cruz joke.

474
00:25:04,990 --> 00:25:11,949
Speaker 1: So somebody used this on all the locations of Banksy's artworks in, you know, the UK or whatever.

475
00:25:13,531 --> 00:25:28,670
Speaker 1: And all the different locations that it pointed to as being bases or homes were connected to this guy who people were pretty sure was Banksy, like, years ago, but it was, like, independently pointed at the same guy without, you know, it wasn't like they had this guy in mind.

476
00:25:28,850 --> 00:25:32,398
Speaker 1: It just, it pointed to all these locations, and then they said, "Oh, what are these locations?

477
00:25:32,418 --> 00:25:38,828
Speaker 1: I want an apartment rented by this guy and a house where the guy lived once, and it was a guy that was previously suspected to be

478
00:25:38,888 --> 00:25:39,009
Speaker 1: him.".

479
00:25:39,150 --> 00:25:41,089
Speaker 1: So it's like, you probably know who it is now.

480
00:25:41,290 --> 00:25:42,578
Speaker 1: So I was going

481
00:25:42,598 --> 00:25:55,869
Speaker 0: to talk about this on the next Tuesday's show, so I won't get into it, but I was going to talk on Tuesday about machine learning algorithms that, you know, Pete, there was kind of an open secret that a lot of crossword puzzle artists just straight up plagiarized from each other.

482
00:25:56,230 --> 00:25:59,550
Speaker 0: But no one could really prove, like, who was doing it or what was plagiarized.

483
00:25:59,631 --> 00:26:01,810
Speaker 0: It was just sort of, like, everyone kind of suspected.

484
00:26:02,410 --> 00:26:18,579
Speaker 0: Machine learning algorithms recently figured out exactly who was plagiarizing whom and how widespread it was, very specifically, and that all ties into, remember I talked about a while ago how there's that algorithm, the Google AI, that is beating humans at Go, 19 by 19 Go.

485
00:26:19,991 --> 00:26:23,610
Speaker 0: Well, it was doing so well that it's finally happening.

486
00:26:24,092 --> 00:26:33,469
Speaker 0: It beat the best Go player in Europe, and now South Korean's Lee Se-do, who is apparently the Go champion of the world, it's going to play against him.

487
00:26:33,670 --> 00:26:34,990
Speaker 0: Like, this was announced today.

488
00:26:35,131 --> 00:26:36,349
Speaker 1: How many times is he going to play it?

489
00:26:36,550 --> 00:26:37,830
Speaker 1: If he plays once, he might beat it.

490
00:26:37,830 --> 00:26:42,429
Speaker 1: If he plays it, like, ten times and has got nine games of data, it might, like, you know, it might win.

491
00:26:42,710 --> 00:26:44,159
Speaker 0: This is, like, this is it.

492
00:26:44,240 --> 00:26:45,810
Speaker 0: This is going to be the moment, very likely.

493
00:26:45,930 --> 00:26:54,270
Speaker 0: It's either the moment where the AI proves that it's better than all humans that go forever, or it proves that it's about to be better than all humans that go forever.

494
00:26:54,777 --> 00:26:55,366
Speaker 1: Or we got to wait.

495
00:26:56,052 --> 00:26:58,490
Speaker 1: It's not a matter of if it's going to beat him.

496
00:26:58,710 --> 00:26:59,490
Speaker 1: It's when, right?

497
00:26:59,690 --> 00:27:01,888
Speaker 1: Will it be now, or will it be the rematch later?

498
00:27:02,350 --> 00:27:12,230
Speaker 0: I was talking to some machine learning and Go experts, actually, about this at MagFest, and here's my thought on all this machine learning, because I keep talking about this.

499
00:27:12,290 --> 00:27:14,050
Speaker 0: There's something going on, like machine learning.

500
00:27:14,371 --> 00:27:16,290
Speaker 1: We finally have access to enough data, right?

501
00:27:16,390 --> 00:27:23,070
Speaker 1: Someone can just get enough data just from social media alone, publicly available data from the internet APIs.

502
00:27:23,130 --> 00:27:25,430
Speaker 0: A specific restaurant and a specific food poisoning event.

503
00:27:25,650 --> 00:27:35,310
Speaker 1: Right, so if you're, say, a government who has a government's amount of data, or a research institute with a studies worth of data, or many studies worth of data, you're, you know, you're looking good.

504
00:27:35,730 --> 00:27:52,467
Speaker 0: Yeah, so the two tests will be one, if the AI beats this guy in the first try, if the AI starts beating him within a few weeks of playing against him, but the third one, the real interesting one, is if this AI starts beating any humans at 20 by 20 Go.

505
00:27:54,251 --> 00:27:55,810
Speaker 1: What size, I was going to ask, what size is playing?

506
00:27:55,810 --> 00:27:57,849
Speaker 0: 19 by 19, full on fucking Go.

507
00:27:58,630 --> 00:28:04,070
Speaker 0: Which, there's comments in our YouTube video about bad games, where we talk about how Go will likely never be solved, and we talk about complexity.

508
00:28:04,470 --> 00:28:05,450
Speaker 1: Someone said just make it bigger?

509
00:28:06,050 --> 00:28:08,710
Speaker 0: Yeah, no, basically what they were saying is that you guys are morons.

510
00:28:08,850 --> 00:28:09,949
Speaker 0: Go will not be solved.

511
00:28:11,450 --> 00:28:14,390
Speaker 0: AIs cannot beat humans in the time frames you're saying.

512
00:28:14,510 --> 00:28:14,972
Speaker 0: Guess what?

513
00:28:15,555 --> 00:28:19,129
Speaker 0: I think we said 2040 is when I thought AIs would be able to beat humans.

514
00:28:19,350 --> 00:28:24,230
Speaker 1: Well, because we thought that they were going to use the old, you know, the method of, you know, the deep blue, right?

515
00:28:24,851 --> 00:28:28,029
Speaker 1: We didn't think they were going to use some fancy way of doing it with machine learning.

516
00:28:28,530 --> 00:28:34,009
Speaker 0: So, suck it haters, and watch out for the machine learning revolution, because this shit is really taking off.

517
00:28:35,094 --> 00:28:37,270
Speaker 0: But, for whatever reason, the heuristics that work...

518
00:28:37,270 --> 00:28:38,742
Speaker 1: Let's find who we can invest in.

519
00:28:38,802 --> 00:28:39,730
Speaker 1: that's doing machine learning.

520
00:28:39,870 --> 00:28:47,847
Speaker 0: The heuristics that work in 19 by 19 Go seem to have thus far been unintelligible to computers when they try to apply it to 20 by 20 Go.

521
00:28:47,847 --> 00:29:03,650
Speaker 0: 19 by 19 Go and 20 by 20 Go are shockingly different, but human Go players have had just, you know, their sort of soft, fuzzy intuition play and their experience with Go to reliably beat AIs all the time in 20 by 20.

522
00:29:03,650 --> 00:29:08,370
Speaker 1: Right, I mean, you could basically play infinite board Go to like, you know, whatever, right?

523
00:29:08,671 --> 00:29:10,808
Speaker 0: And eventually, like right now, at least human intuition.

524
00:29:11,371 --> 00:29:16,270
Speaker 1: In any individual subset of the board, right, a human can comprehend, right?

525
00:29:16,630 --> 00:29:28,110
Speaker 0: If there is a leap from 19 by 19 being unbeatable by humans against the AI to 20 by 20 within the next year or so, that means the pace of this shit is way out of control.

526
00:29:35,140 --> 00:29:37,320
Speaker 0: So anyway, things of the day!

527
00:29:38,161 --> 00:29:46,340
Speaker 0: In case you guys are not aware of this, there's a really, really, really famous book about UX or HCI, The Design of Everyday Things by Don Norman.

528
00:29:46,820 --> 00:29:51,520
Speaker 0: If you do UIs or UXs, that's something you, like, that's like a basic textbook for design.

529
00:29:52,601 --> 00:29:57,020
Speaker 0: And if you're not familiar with that, Vox, they make YouTube videos and such.

530
00:29:57,381 --> 00:29:58,420
Speaker 0: They're an entity on the internet.

531
00:29:59,224 --> 00:30:00,799
Speaker 1: They own The Verge and Polygon and such.

532
00:30:01,301 --> 00:30:01,542
Speaker 0: Yeah.

533
00:30:01,883 --> 00:30:07,940
Speaker 0: There is a door in one of the floors of the Vox office that commits the cardinal sin of doors.

534
00:30:08,641 --> 00:30:13,720
Speaker 0: It looks like it's a push door or a pull door, but it's actually a push door or vice versa.

535
00:30:14,323 --> 00:30:17,259
Speaker 0: You know, the situation where it has a pull handle, but you got to push it?

536
00:30:18,220 --> 00:30:19,605
Speaker 0: So that's the example.

537
00:30:19,645 --> 00:30:22,938
Speaker 0: I mean, this book was only written in the late 80s, I think, mid to late 80s.

538
00:30:23,861 --> 00:30:29,020
Speaker 0: And it might seem obvious, like, oh, you design things so that people will intuitively know how to use them.

539
00:30:29,481 --> 00:30:33,339
Speaker 0: But that idea has not been widely executed anywhere, because if it was...

540
00:30:33,520 --> 00:30:36,160
Speaker 1: Well, the person who installs the doors isn't thinking too hard about that.

541
00:30:36,480 --> 00:30:36,963
Speaker 0: Exactly.

542
00:30:37,043 --> 00:30:40,080
Speaker 0: So there's a lot of solid problems like this that are not well solved.

543
00:30:40,400 --> 00:30:42,130
Speaker 0: So if you are an aspiring UX...

544
00:30:45,723 --> 00:30:52,979
Speaker 1: Unless you have the interior designer or the architect install every door, the person who installs the door is just a laborer who installs doors.

545
00:30:53,220 --> 00:30:54,259
Speaker 0: Well, I guess I'll put it another way.

546
00:30:55,041 --> 00:31:02,099
Speaker 0: A lot of people design things, and it seems like there are obvious common sense design principles that would make things easier to use.

547
00:31:02,641 --> 00:31:09,475
Speaker 0: But in practice, a lot of the people who designed the things that we use on a daily basis just didn't put that much thought into it or suck at it.

548
00:31:10,460 --> 00:31:15,260
Speaker 1: This is why you can make a living being like Apple or Muji that I went to today, right?

549
00:31:15,360 --> 00:31:17,460
Speaker 1: Where it's like you just design things fancily.

550
00:31:18,041 --> 00:31:23,679
Speaker 1: And if you just buy like, you know, if you buy furniture from like IKEA, the people at IKEA think really hard about it.

551
00:31:23,901 --> 00:31:29,900
Speaker 1: If you buy furniture from Gennaro Furniture Place, they don't have like Johnny I fucking working there, right?

552
00:31:29,960 --> 00:31:30,665
Speaker 1: They don't give a shit.

553
00:31:30,725 --> 00:31:33,040
Speaker 1: They just make furniture, you know, the way they always made it.

554
00:31:33,100 --> 00:31:37,120
Speaker 1: And they don't, you know, they don't have designers and pay attention to design trends.

555
00:31:37,180 --> 00:31:37,852
Speaker 1: They just make furniture.

556
00:31:37,872 --> 00:31:38,280
Speaker 1: They don't care.

557
00:31:38,360 --> 00:31:44,620
Speaker 0: So this is basically a video kind of covering the 101 of that sort of human interface design.

558
00:31:45,363 --> 00:31:48,280
Speaker 0: And it's all centered around this one fucking door in the Vox office.

559
00:31:48,661 --> 00:31:57,500
Speaker 0: If you're a UX designer, an aspiring UX designer, anything in this Monday stuff, if you don't already know everything in this video, you should study up.

560
00:31:57,960 --> 00:31:59,108
Speaker 1: There's a lot of money to be made.

561
00:31:59,128 --> 00:32:03,280
Speaker 1: just doing something that people already do and just doing it with good design, right?

562
00:32:03,380 --> 00:32:08,020
Speaker 1: Like if you want to make washing machines, it's like none of the washing machines.

563
00:32:08,161 --> 00:32:11,259
Speaker 1: Maybe there's some out there that are really fancily designed, but most aren't.

564
00:32:11,940 --> 00:32:18,220
Speaker 0: Remember way back, when we were in Beacon, you went to the laundromat and there was that guy trying to sell the low temperature dryers to the laundromat owner.

565
00:32:18,360 --> 00:32:19,932
Speaker 0: He was like, nah, people like it hot.

566
00:32:19,972 --> 00:32:21,020
Speaker 0: They need it to be super hot.

567
00:32:21,260 --> 00:32:24,680
Speaker 0: Like he fundamentally misunderstood the use and purpose of the dryers.

568
00:32:24,860 --> 00:32:35,540
Speaker 1: Well, I think he, you know, had these ideas about, you know, what's going to make him more or less money in his, you know, laundromat when it's really no matter what fucking dryer you have, people are going to come here to dry their clothes.

569
00:32:35,964 --> 00:32:37,459
Speaker 1: It's about just how much you charge.

570
00:32:38,141 --> 00:32:42,700
Speaker 0: And how much it costs you to run the electricity of those super hot ancient dryers that you've got.

571
00:32:43,080 --> 00:32:47,400
Speaker 1: Yeah, you could, you know, if you bought new dryers, when would they pay for themselves?

572
00:32:48,001 --> 00:32:50,820
Speaker 1: Well, you know, electric bill savings times revenue.

573
00:32:51,060 --> 00:32:56,620
Speaker 0: I feel like most people who end up designing things in practice just don't understand basic design principles.

574
00:32:57,083 --> 00:32:58,679
Speaker 0: And it's all illustrated by this door.

575
00:33:00,586 --> 00:33:01,280
Speaker 0: Got a thing of the day?

576
00:33:01,881 --> 00:33:02,707
Speaker 1: Oh, I do have a thing of the day.

577
00:33:02,747 --> 00:33:04,779
Speaker 1: Here's some guys biking, which we're going to be doing pretty soon.

578
00:33:04,860 --> 00:33:05,376
Speaker 1: Yeah.

579
00:33:06,000 --> 00:33:08,419
Speaker 1: And they're biking somewhere in a place I would not go.

580
00:33:08,720 --> 00:33:09,860
Speaker 1: Looks pretty dangerous because...

581
00:33:10,480 --> 00:33:10,800
Speaker 0: Looks pleasant.

582
00:33:11,361 --> 00:33:12,439
Speaker 0: Free roaming ostriches.

583
00:33:13,600 --> 00:33:14,016
Speaker 1: Watch out.

584
00:33:16,362 --> 00:33:19,060
Speaker 0: I just imagine an ostrich seeing you and you hear that kill bill.

585
00:33:24,167 --> 00:33:25,219
Speaker 0: This is a real good video.

586
00:33:26,180 --> 00:33:28,740
Speaker 0: In the meta moment, we're going to be, I guess, at Anime Boston.

587
00:33:28,920 --> 00:33:30,980
Speaker 0: Looks like they put the rest of our panels in pending.

588
00:33:31,160 --> 00:33:32,640
Speaker 0: So I guess they just got to actually schedule them.

589
00:33:32,740 --> 00:33:35,360
Speaker 0: So I think we're doing... I think we're doing everything we submitted.

590
00:33:35,660 --> 00:33:36,299
Speaker 1: So we're actually going?

591
00:33:36,420 --> 00:33:37,451
Speaker 0: Looks like it, so I'm going to sort it out.

592
00:33:37,471 --> 00:33:38,320
Speaker 0: We've got to get our transport.

593
00:33:38,642 --> 00:33:39,300
Speaker 1: What panels do we have to make?

594
00:33:40,383 --> 00:33:41,160
Speaker 0: I don't remember.

595
00:33:41,240 --> 00:33:41,818
Speaker 0: This thing is soon.

596
00:33:42,360 --> 00:33:42,925
Speaker 0: Yeah, it is.

597
00:33:43,147 --> 00:33:44,520
Speaker 0: It's a couple weekends from now.

598
00:33:44,620 --> 00:33:45,580
Speaker 1: It's Easter weekend, I think.

599
00:33:45,780 --> 00:33:50,220
Speaker 0: We've got to do the... I think it's all panels we can do pretty easily.

600
00:33:50,661 --> 00:33:55,000
Speaker 1: As long as we're doing the old, you know, next anime season thing.

601
00:33:55,120 --> 00:33:57,120
Speaker 0: Yeah, that's the only one that requires effort.

602
00:33:57,360 --> 00:34:02,780
Speaker 0: Because that's the only one that actually requires... There's a lot of slides you've got to make, and it's not super automatic to make that happen.

603
00:34:03,423 --> 00:34:04,459
Speaker 0: That's pretty much the only thing.

604
00:34:04,980 --> 00:34:06,479
Speaker 0: That's probably the only one we'll try to video, too.

605
00:34:06,720 --> 00:34:07,678
Speaker 0: You know, obvious reasons.

606
00:34:08,280 --> 00:34:10,777
Speaker 0: But otherwise, PAX East coming up.

607
00:34:11,179 --> 00:34:14,120
Speaker 0: We're going to be doing at least one panel at it on Sunday, maybe more.

608
00:34:14,280 --> 00:34:16,018
Speaker 0: But Atari Game Design's in there at least for now.

609
00:34:16,659 --> 00:34:18,500
Speaker 0: Kineticon, I don't know when I can open the panels.

610
00:34:18,840 --> 00:34:20,059
Speaker 0: Sorry, I got nothing for you.

611
00:34:20,400 --> 00:34:21,757
Speaker 0: It's not dependent upon me.

612
00:34:22,260 --> 00:34:24,719
Speaker 0: So I'm waiting for the go-ahead-to-open panel submissions.

613
00:34:26,024 --> 00:34:26,998
Speaker 0: And that's all I got to say about that.

614
00:34:27,620 --> 00:34:30,860
Speaker 0: The Patreon has a bunch of reviews of things by me and Emily.

615
00:34:30,860 --> 00:34:33,560
Speaker 0: We're calling it the Rimbly Review, and we're going to do a bunch more of those.

616
00:34:33,760 --> 00:34:36,339
Speaker 1: And you can save a lot of money by not paying attention to that.

617
00:34:36,600 --> 00:34:41,860
Speaker 0: But the main reason we're doing it is because Emily and I are watching a lot of media like Jessica Jones or Zootopia that Scott's not watching.

618
00:34:42,080 --> 00:34:43,100
Speaker 0: And I want to review those things.

619
00:34:43,300 --> 00:34:44,059
Speaker 1: I probably won't watch.

620
00:34:44,480 --> 00:34:45,184
Speaker 0: I don't know why you won't.

621
00:34:45,205 --> 00:34:47,438
Speaker 0: Zootopia is probably the best Disney movie to come out in 20 years.

622
00:34:48,561 --> 00:34:49,208
Speaker 1: The trailer was funny.

623
00:34:49,228 --> 00:34:49,817
Speaker 1: I'll see it one day.

624
00:34:49,898 --> 00:34:50,080
Speaker 1: Whatever.

625
00:34:50,545 --> 00:34:51,219
Speaker 0: It's real good.

626
00:34:53,022 --> 00:34:55,880
Speaker 0: So it's very rare, Scott, that I go into something with high expectations.

627
00:34:56,860 --> 00:34:58,960
Speaker 0: And they are met, let alone exceeded.

628
00:34:59,603 --> 00:35:02,818
Speaker 1: I think pretty much everyone on Earth is more easily amused than I am.

629
00:35:04,940 --> 00:35:05,161
Speaker 0: All right.

630
00:35:05,201 --> 00:35:08,580
Speaker 0: What was the best movie you've seen in the last, I don't know, five years?

631
00:35:09,442 --> 00:35:10,159
Speaker 1: Five years?

632
00:35:13,542 --> 00:35:15,940
Speaker 1: I've rewatched Rear Window within the last five years.

633
00:35:16,400 --> 00:35:18,140
Speaker 1: And that's like the best movie there is almost.

634
00:35:20,185 --> 00:35:21,516
Speaker 1: I mean, it wasn't the first time I saw it.

635
00:35:22,421 --> 00:35:23,152
Speaker 1: Does that count?

636
00:35:25,141 --> 00:35:27,840
Speaker 1: That's like, you know, if you ask me the best thing I've eaten, right.

637
00:35:27,940 --> 00:35:31,040
Speaker 1: You won't discount something if it's like, oh, I went to salt and fat again.

638
00:35:31,280 --> 00:35:31,503
Speaker 1: Right.

639
00:35:31,624 --> 00:35:33,000
Speaker 1: It's like, does that not count?

640
00:35:33,720 --> 00:35:34,300
Speaker 0: Keep going to salt and fat.

641
00:35:34,822 --> 00:35:35,206
Speaker 1: Right.

642
00:35:35,246 --> 00:35:36,658
Speaker 1: It was not the first time I went there.

643
00:35:38,964 --> 00:35:39,740
Speaker 0: I'll put it this way, Scott.

644
00:35:39,861 --> 00:35:48,479
Speaker 0: It was good, not just from like the usual metrics, but it was good from like a full on lit crit film, crit perspective in a way that most movies are not.

645
00:35:49,184 --> 00:35:50,197
Speaker 0: I was surprised by that.

646
00:35:51,366 --> 00:35:51,819
Speaker 0: It was good.

647
00:35:51,941 --> 00:35:57,740
Speaker 0: So once the hurry is, if the rest of you want to learn about all these things that I'm watching and Scott's not, it's all in the Patreon.

648
00:35:58,440 --> 00:35:58,977
Speaker 0: Me and Emily.

649
00:35:59,680 --> 00:36:00,440
Speaker 1: That word should stay.

650
00:36:00,580 --> 00:36:01,778
Speaker 0: Yeah, it is going to stay there.

651
00:36:02,280 --> 00:36:02,896
Speaker 1: You should not go.

652
00:36:05,905 --> 00:36:07,020
Speaker 0: And we'll do the book club soon.

653
00:36:07,140 --> 00:36:09,679
Speaker 0: I'm probably going to do Margaret Atwood's Oryx and Crake.

654
00:36:10,580 --> 00:36:11,967
Speaker 1: I was going to do that.

655
00:36:15,962 --> 00:36:16,396
Speaker 1: What's your hurry?

656
00:36:16,640 --> 00:36:17,337
Speaker 1: I'm just taking my idea.

657
00:36:18,721 --> 00:36:21,420
Speaker 0: But, so I'm about halfway through book 10 of The Wheel of Time.

658
00:36:21,501 --> 00:36:23,260
Speaker 1: Whoa, what is wrong with you?

659
00:36:23,600 --> 00:36:24,939
Speaker 0: Well, because some shit went down.

660
00:36:25,560 --> 00:36:29,160
Speaker 0: You know those big statues, like the crazy, big, angry, like the nonsense ones?

661
00:36:29,561 --> 00:36:30,640
Speaker 1: They finally came into play?

662
00:36:31,042 --> 00:36:32,039
Speaker 0: Someone just used them both.

663
00:36:32,280 --> 00:36:34,680
Speaker 1: Aren't they the most powerful things there are that like destroy the whole world?

664
00:36:35,443 --> 00:36:36,560
Speaker 1: And destroy the dark one even?

665
00:36:37,180 --> 00:36:43,920
Speaker 0: They could just, let's just say that they have the capacity to destroy entire continents by accident in the course of doing other things.

666
00:36:44,180 --> 00:36:44,880
Speaker 1: Yeah, that's what I thought.

667
00:36:45,060 --> 00:36:46,920
Speaker 1: That's what they, they warned me about them in book one.

668
00:36:47,120 --> 00:36:47,879
Speaker 0: Yeah, so you know what?

669
00:36:48,001 --> 00:36:50,940
Speaker 0: It's basically the gig of slave and shit goes down.

670
00:36:51,080 --> 00:36:51,660
Speaker 0: It's pretty interesting.

671
00:36:51,820 --> 00:36:53,280
Speaker 1: Aren't you supposed to use it on the dark one?

672
00:36:54,080 --> 00:36:55,520
Speaker 0: Uh, no, they did not use it on the dark one.

673
00:36:55,643 --> 00:36:55,998
Speaker 0: Why not?

674
00:36:56,200 --> 00:36:57,218
Speaker 0: They just used it to do a thing.

675
00:36:57,760 --> 00:36:58,498
Speaker 1: Use it on the dark one.

676
00:36:58,822 --> 00:37:00,280
Speaker 0: Yeah, well, it's complicated.

677
00:37:00,763 --> 00:37:01,540
Speaker 0: What is the dark one?

678
00:37:01,661 --> 00:37:02,078
Speaker 0: I don't know.

679
00:37:02,400 --> 00:37:02,860
Speaker 0: It doesn't matter.

680
00:37:02,940 --> 00:37:03,719
Speaker 1: You know where he is.

681
00:37:03,760 --> 00:37:04,740
Speaker 1: He's in this big mountain or something.

682
00:37:04,924 --> 00:37:05,300
Speaker 1: He's just flying.

683
00:37:05,460 --> 00:37:07,100
Speaker 0: Child goal and the end of the light or whatever.

684
00:37:07,440 --> 00:37:10,560
Speaker 1: So just use the full power of those things on that spot and narrow it.

685
00:37:10,780 --> 00:37:12,020
Speaker 1: Really, really point, find points.

686
00:37:12,280 --> 00:37:12,598
Speaker 0: Just blow it up.

687
00:37:12,760 --> 00:37:14,120
Speaker 0: That might just blow up the planet, I guess.

688
00:37:14,700 --> 00:37:15,857
Speaker 1: Oh, you take the dark one with you.

689
00:37:16,443 --> 00:37:18,940
Speaker 0: But I'll probably review the whole Wheel of Time.

690
00:37:19,100 --> 00:37:21,520
Speaker 0: I'm probably going to finish the whole fucking thing in the next month or two.

691
00:37:21,701 --> 00:37:22,175
Speaker 0: What's wrong with you?

692
00:37:23,060 --> 00:37:23,457
Speaker 0: You know what?

693
00:37:23,961 --> 00:37:27,860
Speaker 0: I was going to just read spoilers, but the spoilers I found.

694
00:37:27,980 --> 00:37:29,559
Speaker 1: Maybe you could just read spoilers for the final book.

695
00:37:30,120 --> 00:37:30,840
Speaker 0: So here's my problem.

696
00:37:31,240 --> 00:37:32,479
Speaker 0: I've run into this with other things too.

697
00:37:33,082 --> 00:37:35,040
Speaker 0: After I beat Undertale, I read a whole bunch of spoilers.

698
00:37:35,300 --> 00:37:37,080
Speaker 0: Just analysis and stuff.

699
00:37:37,440 --> 00:37:37,758
Speaker 0: Wikias.

700
00:37:38,141 --> 00:37:38,839
Speaker 0: Did I miss anything?

701
00:37:39,461 --> 00:37:41,380
Speaker 0: And I tried to do that for some of the Wheel of Time.

702
00:37:41,660 --> 00:37:51,999
Speaker 0: What I've discovered is that most things that people think are spoilers aren't because the people writing those things don't know shit, and they grossly misinterpret basic things.

703
00:37:53,141 --> 00:37:55,291
Speaker 1: I read some spoilers on Undertale that were literally wrong.

704
00:37:57,901 --> 00:38:03,259
Speaker 0: The person literally misunderstood who a character was, and that is just on the Wikia.

705
00:38:03,601 --> 00:38:04,919
Speaker 0: That's the kind of stuff that's out there.

706
00:38:05,260 --> 00:38:07,180
Speaker 0: Kids do not know how to do Likrit.

707
00:38:07,601 --> 00:38:15,379
Speaker 0: They don't know how to summarize, and it'll be faster for me to spoil the Wheel of Time by just finishing it than it will be to try to read that Wikia.

708
00:38:16,160 --> 00:38:18,359
Speaker 1: Finishing it will ruin it, will make it unenjoyable.

709
00:38:18,901 --> 00:38:21,059
Speaker 1: If it's so unenjoyable to read that it's just spoiling it.

710
00:38:21,740 --> 00:38:25,159
Speaker 0: It's not that unenjoyable to read, so I kind of just keep reading it, because it's so fast.

711
00:38:26,161 --> 00:38:29,319
Speaker 1: I'll stay back in my, you know, where I am.

712
00:38:31,101 --> 00:38:31,756
Speaker 1: Wherever that is.

713
00:38:32,423 --> 00:38:33,119
Speaker 0: You're never going to--.

714
00:38:33,160 --> 00:38:43,100
Speaker 1: Yeah, Scott, at one point they found-- I think they went to the mansion of the guy who used to be the knight for the queen, and then the evil guy took over, and he got kicked out.

715
00:38:43,120 --> 00:38:46,260
Speaker 0: Oh, yeah, so the white cloaks, their whole nation got collapsed.

716
00:38:46,600 --> 00:38:47,659
Speaker 1: But that was-- I mean, yeah.

717
00:38:47,900 --> 00:38:49,639
Speaker 0: Bunch of queens all over the place.

718
00:38:50,424 --> 00:38:51,700
Speaker 0: One of the queens kind-- well, anyway.

719
00:38:52,843 --> 00:38:53,490
Speaker 0: Stuff went down.

720
00:38:53,591 --> 00:38:54,280
Speaker 0: I know a lot of stuff.

721
00:38:54,580 --> 00:38:55,738
Speaker 0: They're a little bit beyond that now.

722
00:38:56,362 --> 00:39:03,220
Speaker 0: So this Apple case, I'm sure-- We're not going to try to tell you 100% of what's happening, because it's been in all the news for a long time.

723
00:39:03,240 --> 00:39:09,500
Speaker 1: And I don't care about the legal or the lawyer-y or the terrorism aspects of what's going on, right?

724
00:39:09,601 --> 00:39:15,300
Speaker 1: But there's a lot of technologies going on, and even the news that's reporting on it doesn't know.

725
00:39:15,440 --> 00:39:18,393
Speaker 1: And the congressman-- because I watched the hearing about this-- Oh, yeah.

726
00:39:18,800 --> 00:39:23,140
Speaker 1: The congressman actually knew more than I thought they did, but mostly in legal territory.

727
00:39:23,400 --> 00:39:37,419
Speaker 1: But someone had briefed them, and also the ones that I know to be wise in the technology ways, like Zoe Lofgren or whoever, is the few congressmen who actually know what a computer is, or at least I guess are paid by Google to know what a computer is.

728
00:39:37,700 --> 00:39:44,919
Speaker 0: But I see a lot of otherwise smart people, or even tech people, who seem to be on the FBI's side, or on the backdoor side.

729
00:39:45,621 --> 00:39:52,620
Speaker 0: So the overall-- we're going to talk about this in more detail, but if you need a TLDR version of this, Apple is right, the FBI is wrong.

730
00:39:53,042 --> 00:39:53,419
Speaker 1: The end.

731
00:39:53,744 --> 00:39:54,760
Speaker 0: That is the answer.

732
00:39:54,980 --> 00:39:56,819
Speaker 1: All right, so basically the situation is this.

733
00:39:57,060 --> 00:40:00,820
Speaker 1: The FBI has an iPhone belonging to a criminal, right?

734
00:40:01,320 --> 00:40:04,040
Speaker 1: They would like access to the information on the phone.

735
00:40:04,140 --> 00:40:08,040
Speaker 1: However, it is encrypted, and they cannot access this information, right?

736
00:40:08,603 --> 00:40:14,220
Speaker 1: They would be able to access it if they could type in the person's password and unlock the phone, right?

737
00:40:14,463 --> 00:40:15,400
Speaker 0: But that person is dead.

738
00:40:15,660 --> 00:40:16,439
Speaker 1: That person is dead.

739
00:40:16,762 --> 00:40:18,880
Speaker 1: And, you know, they don't know the password.

740
00:40:19,582 --> 00:40:23,378
Speaker 1: And if they try to unlock it too many times, first of all-- Which apparently they already did.

741
00:40:24,045 --> 00:40:24,176
Speaker 1: Right.

742
00:40:24,600 --> 00:40:28,720
Speaker 1: It takes a long time to try a bunch of passwords, because it'll slow you down, right?

743
00:40:28,780 --> 00:40:30,500
Speaker 1: Like, oh, wait before you enter another one.

744
00:40:30,680 --> 00:40:41,040
Speaker 1: Also, if there's a feature I've enabled on my iPhone, if you try ten times in a row and you fail ten times in a row, which is pretty hard to do, sometimes I get worried when I mess up like two or three times in a row.

745
00:40:41,381 --> 00:40:42,380
Speaker 0: Same thing on my Android.

746
00:40:42,820 --> 00:40:47,900
Speaker 1: But if you mess up ten times in a row, it'll just erase your phone, which is fine if you have a backup like me.

747
00:40:48,060 --> 00:40:54,200
Speaker 1: You know, I have an encrypted backup on my personal computer, so I can just fix it when I get home, but it'll suck for the rest of the day using a blank phone.

748
00:40:54,842 --> 00:40:56,960
Speaker 1: At least the phone calls will still work, but anyway.

749
00:40:57,621 --> 00:41:02,759
Speaker 1: So, yeah, if they mess up ten times guessing this password, then the phone will just erase itself, and they're fucked.

750
00:41:03,361 --> 00:41:08,660
Speaker 0: Now, there's a whole bunch of interesting nuance around this where the FBI basically completely mishandled the whole thing.

751
00:41:08,800 --> 00:41:11,060
Speaker 0: So the phone was also backing up to iCloud.

752
00:41:11,327 --> 00:41:11,560
Speaker 0: Yeah.

753
00:41:11,681 --> 00:41:12,378
Speaker 0: And it looks like Apple

754
00:41:12,399 --> 00:41:22,820
Speaker 1: -- They could have just gotten the backup from iCloud, and if it wasn't encrypted up there, it could have just looked at that, and you don't need to look at the phone, because if all the data from the phone was in Apple's cloud computer somewhere.

755
00:41:22,960 --> 00:41:23,820
Speaker 0: Which is interesting.

756
00:41:23,960 --> 00:41:28,740
Speaker 0: I wonder if Apple will maybe start to offer an option where the iCloud backup is also encrypted.

757
00:41:29,021 --> 00:41:34,140
Speaker 1: Yeah, I mean, you can encrypt your local desktop backup, but I haven't seen an option in iTunes for encrypting iCloud One.

758
00:41:34,201 --> 00:41:35,518
Speaker 1: I just sort of thought it was.

759
00:41:35,820 --> 00:41:38,420
Speaker 1: Which is fine, because -- I thought it was encrypted, but I don't know, whatever.

760
00:41:38,480 --> 00:41:43,500
Speaker 0: Because Apple basically provides -- So here's the two level -- You know, we did an episode a long time ago about the levels of paranoid security.

761
00:41:44,200 --> 00:41:48,300
Speaker 0: The main reason to encrypt your phone is if you lose your phone, you don't want people getting access to your shit.

762
00:41:48,560 --> 00:41:48,717
Speaker 1: Yep.

763
00:41:49,040 --> 00:41:53,820
Speaker 0: You can kind of trust Apple's security on the backups, so you kind of don't want to encrypt those.

764
00:41:53,860 --> 00:41:59,239
Speaker 1: You can actually trust Apple, I think, more than any other major technology corporation to protect -- Well, after this, definitely.

765
00:41:59,480 --> 00:42:03,779
Speaker 1: To protect your data, because for a while, for years now, Apple has been saying, "Listen.

766
00:42:04,601 --> 00:42:09,839
Speaker 1: We're charging you all this money for our fancy computers, but look at them Googles over there, right?

767
00:42:10,021 --> 00:42:11,580
Speaker 1: They're giving you phones away for cheap.

768
00:42:11,700 --> 00:42:13,239
Speaker 1: Why are they giving you phones away for cheap?

769
00:42:13,460 --> 00:42:16,760
Speaker 1: Because they're making all the extra revenues by selling your information.

770
00:42:17,281 --> 00:42:19,000
Speaker 1: We don't give a shit about your information.

771
00:42:19,081 --> 00:42:21,280
Speaker 1: We're just selling you hardware and software directly.

772
00:42:21,783 --> 00:42:22,391
Speaker 1: That's all we're

773
00:42:22,431 --> 00:42:22,776
Speaker 1: doing.".

774
00:42:23,524 --> 00:42:24,754
Speaker 1: And it's mostly true.

775
00:42:24,835 --> 00:42:25,299
Speaker 1: I believe them.

776
00:42:25,961 --> 00:42:37,799
Speaker 0: So if you're ultra-paranoid and you care about data security and the phones, like, "I lost my physical artifact security," then just turn off iCloud backups, and everything you've got is encrypted on the phone.

777
00:42:37,860 --> 00:42:39,140
Speaker 1: Yeah, and just make sure you've got your backups.

778
00:42:39,400 --> 00:42:48,959
Speaker 1: You can use iTunes to save encrypted backups of your phone to your own hard drive, and that way, if the FBI comes and takes your home computer, they'll have some brute force here.

779
00:42:49,120 --> 00:43:02,219
Speaker 0: They might have been able to just trick the phone into sending one more backup, but apparently, the FBI fucked that up by calling Apple and getting them to disable the iCloud account, and it's all fucked now.

780
00:43:02,680 --> 00:43:07,600
Speaker 0: So there's a whole side conversation we're going to avoid here around the fact that -- The specifics of this case.

781
00:43:07,740 --> 00:43:08,483
Speaker 1: Well, no, how.

782
00:43:08,543 --> 00:43:13,100
Speaker 0: the FBI apparently has terrible computer forensics capabilities.

783
00:43:13,480 --> 00:43:16,319
Speaker 0: They made a series of baby's first mistakes here.

784
00:43:16,540 --> 00:43:28,314
Speaker 1: With something that wasn't even an advance -- It wasn't like the master hacker has encrypted his computer and you managed to get it from him, and this is someone who was setting up, you know, Silk Road 5.0 or whatever.

785
00:43:30,840 --> 00:43:34,020
Speaker 1: This is a dumb-dumb non-technological person's iPhone.

786
00:43:34,360 --> 00:43:35,934
Speaker 0: Someone had an iPhone, and they said, "Yes,

787
00:43:35,974 --> 00:43:36,317
Speaker 0: encrypt.".

788
00:43:37,388 --> 00:43:38,360
Speaker 1: They pushed the "Yes" button.

789
00:43:39,641 --> 00:43:41,774
Speaker 0: That genius, that unimpeachable genius.

790
00:43:42,680 --> 00:43:43,578
Speaker 1: The FBI couldn't deal with it.

791
00:43:44,001 --> 00:43:45,860
Speaker 1: They need Apple's help because they're such dumb-dumbs.

792
00:43:47,222 --> 00:43:50,179
Speaker 1: This is why they keep going to colleges trying to hire people like us.

793
00:43:50,541 --> 00:44:02,182
Speaker 0: So what the FBI is basically demanding of Apple is that the way iPhones are architected today, as far as we can tell, the data is encrypted, but because there's a lot of things around -- Well,

794
00:44:02,322 --> 00:44:07,600
Speaker 1: any device anywhere, right, any modern device has a feature to update the software.

795
00:44:08,102 --> 00:44:12,739
Speaker 1: You can replace the operating system on an iPhone or an Android computer or anything, right?

796
00:44:13,101 --> 00:44:15,260
Speaker 1: You can replace the OS, and how do you replace the OS?

797
00:44:15,260 --> 00:44:16,719
Speaker 1: Because there's an update mechanism.

798
00:44:17,461 --> 00:44:26,219
Speaker 1: You can use the update mechanism of any device to basically gain full access because you just rewrite the OS with an OS that does whatever the fuck you want.

799
00:44:26,940 --> 00:44:46,015
Speaker 1: So FBI is like, "Hey, write us a new version of iOS that is, like, super unlocked and can just, like, let us type in the password a million times without deleting and does all these things we want, and we'll flash this iPhone with the new iOS you wrote just for us, and then it'll be totally

800
00:44:46,116 --> 00:44:46,419
Speaker 1: awesome.".

801
00:44:46,681 --> 00:44:49,140
Speaker 0: Now, this opens up a lot of interesting issues.

802
00:44:49,220 --> 00:44:59,719
Speaker 0: So, one, the fact that the FBI is asking Apple to do this means, again, that our government does not have access to the best and brightest forensics people to try to figure this out on their own.

803
00:45:00,340 --> 00:45:14,000
Speaker 1: I mean -- Couldn't you just -- Like, what I would be doing if I was them would just be open up the iPhone, break off the solid-state storage, right, attach it directly to a computer, copy all the bits over -- And just start a brute force.

804
00:45:14,161 --> 00:45:14,585
Speaker 0: Right.

805
00:45:14,625 --> 00:45:16,080
Speaker 1: It's like, "There's just zeros and ones there.

806
00:45:16,100 --> 00:45:16,785
Speaker 1: I've got them all.

807
00:45:17,127 --> 00:45:18,134
Speaker 1: Now I'm gonna do whatever I

808
00:45:18,175 --> 00:45:18,416
Speaker 1: want.".

809
00:45:19,102 --> 00:45:22,038
Speaker 0: Anyone that the government has in their employees is capable of that.

810
00:45:22,482 --> 00:45:25,480
Speaker 1: I mean, I'm capable of that, given enough time and documentation.

811
00:45:26,060 --> 00:45:29,940
Speaker 1: Give me the data sheet for the storage chip on there, whatever.

812
00:45:30,380 --> 00:45:31,860
Speaker 1: I'll hook it up to a circuit board.

813
00:45:32,020 --> 00:45:32,920
Speaker 0: So they're trying to do a lot of things.

814
00:45:33,000 --> 00:45:41,080
Speaker 0: For one, they're basically demanding that all encryption on anything that consumers can have have a back door that the government can just open on their own.

815
00:45:41,340 --> 00:45:41,441
Speaker 1: Right.

816
00:45:41,461 --> 00:45:45,420
Speaker 1: So let me make an analogy, 'cause everyone -- You know, all those congressmen tried to do analogies, right?

817
00:45:45,621 --> 00:45:49,920
Speaker 1: And all their analogies had the same problem as all other analogies, which is major problems with them.

818
00:45:50,020 --> 00:45:52,660
Speaker 1: But here's an analogy that is not mad broken, right?

819
00:45:52,860 --> 00:45:53,206
Speaker 1: It's not 100% perfect.

820
00:45:54,443 --> 00:45:55,880
Speaker 0: Well, if it was 100% perfect, it wouldn't be an analogy.

821
00:45:56,480 --> 00:45:59,680
Speaker 1: So imagine if everyone had locks on the doors to their houses.

822
00:45:59,840 --> 00:45:59,982
Speaker 1: Oh, wait.

823
00:46:00,002 --> 00:46:01,460
Speaker 1: People do, right?

824
00:46:01,841 --> 00:46:06,640
Speaker 1: But there's a way around a lock on a door to a house, which is you bash the fucking door down, right?

825
00:46:07,341 --> 00:46:12,020
Speaker 1: So imagine if people had ways to make doors on their houses that actually could not be broken down.

826
00:46:12,422 --> 00:46:18,740
Speaker 1: You absolutely need the key, and if you don't have the key, there's no way to, like, magically pick the lock or anything.

827
00:46:18,860 --> 00:46:23,640
Speaker 1: It's like all you can do is keep sitting there making keys until you make one that's the right shape.

828
00:46:23,740 --> 00:46:29,820
Speaker 1: Otherwise, there is no possible way to get through the door unless you have quantum teleportation or some other nonsense, right?

829
00:46:30,441 --> 00:46:32,760
Speaker 1: So the FBI doesn't want to sit there making keys.

830
00:46:33,082 --> 00:46:48,000
Speaker 1: They go to the person who invented this magical door that's unbreakable, and they said, "Hey, person who made this unbreakable door, make us a key that will open any of your unbreakable doors, because we're the government, and we should be allowed to go into all these houses because we have the right to search and siege when we have warrants.

831
00:46:48,360 --> 00:46:49,059
Speaker 1: We have warrants.

832
00:46:49,441 --> 00:46:52,899
Speaker 1: We need to be able to get in those houses to serve these warrants, which are totally legal.

833
00:46:53,501 --> 00:46:54,314
Speaker 1: Make us a master

834
00:46:54,354 --> 00:46:54,497
Speaker 1: key.".

835
00:46:54,720 --> 00:47:16,160
Speaker 1: It's like if we made a master key, sure, you could go in all the places, and even if we trusted you to only use it on places with warrants, if the doors can be opened by a master key, if the master key were to fall into anyone's hands or if anyone were to guess this master key, they would guess and be able to go into everyone's house ever.

836
00:47:16,320 --> 00:47:19,608
Speaker 0: Now, it's even better than that because it's a question of, "Is it

837
00:47:19,648 --> 00:47:20,195
Speaker 0: possible?".

838
00:47:20,781 --> 00:47:26,080
Speaker 0: Having a master key means a master key is theoretically possible, as opposed to not possible.

839
00:47:26,080 --> 00:47:32,220
Speaker 1: Which means now, if someone were to say an evil person wanted to break into Rym's house, they keep guessing keys, and eventually they get in Rym's house.

840
00:47:32,360 --> 00:47:35,420
Speaker 1: Okay, they guess they got lucky, and they got in Rym's house and guess number five.

841
00:47:36,605 --> 00:47:37,180
Speaker 1: That's fine.

842
00:47:37,361 --> 00:47:38,440
Speaker 1: It's not the end of the world.

843
00:47:38,480 --> 00:47:40,480
Speaker 1: It's the end of the world for Rym, but not for anyone else, really.

844
00:47:40,980 --> 00:47:48,920
Speaker 1: Imagine if someone who gets lucky, and if they know that it's possible, they're going to try to now open every door ever.

845
00:47:49,140 --> 00:47:52,760
Speaker 0: Any backdoor to encryption means there's no such thing as encryption, full stop.

846
00:47:53,100 --> 00:47:53,275
Speaker 0: Yes.

847
00:47:53,720 --> 00:48:03,860
Speaker 0: Now, this also should be moot, again, from a forensic standpoint, because no one intelligent is going to go after the encryption itself directly.

848
00:48:03,920 --> 00:48:06,220
Speaker 0: You've already lost if that's your last effort.

849
00:48:07,321 --> 00:48:12,379
Speaker 0: Encryption gets broken because people who have encrypted data have to share that data with other people.

850
00:48:13,521 --> 00:48:20,297
Speaker 0: So there's a lot of ways to spy on people, as we know NSA, listening to this right now, probably live.

851
00:48:22,243 --> 00:48:26,560
Speaker 0: There's other ways to figure out that information, or at least the relevant information.

852
00:48:27,020 --> 00:48:28,179
Speaker 0: There's a lot of ways.

853
00:48:28,560 --> 00:48:31,340
Speaker 0: There's like a billion inputs and outputs all around the encrypted core.

854
00:48:31,461 --> 00:48:33,740
Speaker 1: Most people have dumb dumb passwords, right?

855
00:48:33,820 --> 00:48:37,040
Speaker 1: Most of the evil people who you're going after are dumb dumbs, right?

856
00:48:37,581 --> 00:48:43,620
Speaker 1: It's very rare that the person you're going after is sophisticated enough to where you can't figure them out, right?

857
00:48:43,660 --> 00:48:51,860
Speaker 1: Which is, wow, the FBI gets away with having these dumb dumbs on staff because they're not going after anyone who's sophisticated, usually.

858
00:48:52,100 --> 00:48:52,720
Speaker 0: So it doesn't matter.

859
00:48:53,100 --> 00:48:55,019
Speaker 0: Not an analogy, but a literal example of this.

860
00:48:55,481 --> 00:48:55,622
Speaker 0: In the

861
00:48:55,703 --> 00:48:55,966
Speaker 0: U.S.,

862
00:48:56,006 --> 00:49:01,179
Speaker 0: you have to use a TSA approved lock if you have checked baggage on a plane, or they'll just cut your fucking lock off.

863
00:49:02,141 --> 00:49:05,940
Speaker 0: So TSA approved locks have a slot where you can put a special secret master key in.

864
00:49:06,100 --> 00:49:07,118
Speaker 0: And there's like 12 of them, I think.

865
00:49:08,221 --> 00:49:12,055
Speaker 0: And yeah, go Google for TSA master keys because you can print them in your 3D printer.

866
00:49:13,541 --> 00:49:15,979
Speaker 1: Anyone in the world-- A TSA lock is basically no lock.

867
00:49:16,760 --> 00:49:20,880
Speaker 0: Anyone in the world can print or buy all the master keys.

868
00:49:21,322 --> 00:49:22,279
Speaker 0: And you know how they were leaked?

869
00:49:22,742 --> 00:49:23,440
Speaker 0: A photograph.

870
00:49:25,621 --> 00:49:26,154
Speaker 0: So, yeah.

871
00:49:27,441 --> 00:49:31,018
Speaker 0: When I lock my luggage, I just put a zip tie on it because when they break it, oh well.

872
00:49:31,540 --> 00:49:32,300
Speaker 0: They would have broken it anyway.

873
00:49:32,520 --> 00:49:33,860
Speaker 1: You know that it was broken, right?

874
00:49:35,249 --> 00:49:36,280
Speaker 1: You can't protect your luggage.

875
00:49:37,364 --> 00:49:45,119
Speaker 1: I feel like there's no point in locking luggage because even if I had a lock on the luggage that wasn't a TSA lock, you can get into the luggage with a knife.

876
00:49:45,700 --> 00:49:46,780
Speaker 1: Just cut the luggage open.

877
00:49:47,040 --> 00:49:49,499
Speaker 0: All it does is slow down like the laziest thief.

878
00:49:49,780 --> 00:49:51,980
Speaker 1: And even if I'm using Samsonite luggage, great.

879
00:49:52,241 --> 00:49:54,380
Speaker 1: Now they use a hammer and they get into my luggage.

880
00:49:54,861 --> 00:50:00,438
Speaker 1: If I'm not going to make my luggage a titanium box and carry it around, it's like there's no point in locking it, right?

881
00:50:00,901 --> 00:50:04,662
Speaker 1: So a zip tie is perfectly fine because all that does is say, "Aha!

882
00:50:05,249 --> 00:50:06,038
Speaker 1: Somebody went into

883
00:50:06,099 --> 00:50:06,140
Speaker 1: it.".

884
00:50:06,261 --> 00:50:07,375
Speaker 1: Or, "Nope, no one went into

885
00:50:07,416 --> 00:50:07,456
Speaker 1: it.".

886
00:50:07,861 --> 00:50:19,880
Speaker 0: Now, what's interesting is that it's basically apparent that Apple could write a custom version of the OS that would severely weaken the existing security capabilities of iOS.

887
00:50:20,020 --> 00:50:23,159
Speaker 1: And they could flash it onto that phone and then, you know.

888
00:50:23,821 --> 00:50:30,920
Speaker 0: I've heard rumors that Apple is working on for a future iPhone and updated iOS that removes that possibility.

889
00:50:31,201 --> 00:50:33,460
Speaker 1: Right, now here's the technological reason behind this, right?

890
00:50:34,604 --> 00:50:37,300
Speaker 1: So jailbreaking an iPhone ain't that easy, right?

891
00:50:37,460 --> 00:50:38,126
Speaker 1: Well, why?

892
00:50:38,146 --> 00:50:39,620
Speaker 1: I mean, it has an update feature.

893
00:50:39,700 --> 00:50:40,891
Speaker 1: You can replace iOS.

894
00:50:40,911 --> 00:50:41,839
Speaker 1: You can update iOS.

895
00:50:42,100 --> 00:50:45,160
Speaker 1: Couldn't you use the update mechanism to put any OS you wanted on there?

896
00:50:45,200 --> 00:50:45,979
Speaker 0: If you got Apple's key.

897
00:50:46,240 --> 00:50:46,802
Speaker 1: That's right.

898
00:50:46,943 --> 00:50:53,234
Speaker 1: The iPhone is programmed such that if you try to give it software, say, "Hey, iPhone," and iPhone's like, "What's

899
00:50:53,335 --> 00:50:53,416
Speaker 1: up?".

900
00:50:54,181 --> 00:50:56,371
Speaker 1: "I got an OS for you to replace iOS with.

901
00:50:56,431 --> 00:50:57,034
Speaker 1: It's iOS

902
00:50:57,034 --> 00:50:58,240
Speaker 1: 20.0.".

903
00:50:58,240 --> 00:50:59,093
Speaker 1: And it's like, "All right,

904
00:50:59,174 --> 00:50:59,357
Speaker 1: cool.".

905
00:50:59,540 --> 00:51:01,800
Speaker 0: It's iOSgive.me.your.Bitcoins.

906
00:51:02,200 --> 00:51:02,321
Speaker 1: Right.

907
00:51:02,563 --> 00:51:04,880
Speaker 1: So, you know, you send it on over.

908
00:51:05,142 --> 00:51:10,258
Speaker 1: And then the iPhone takes a look at it and it decides, "Am I going to replace myself with this or

909
00:51:10,398 --> 00:51:10,580
Speaker 1: not?".

910
00:51:11,203 --> 00:51:13,956
Speaker 1: And it looks and it says, "Well, who made

911
00:51:14,016 --> 00:51:14,237
Speaker 1: this?".

912
00:51:15,184 --> 00:51:16,399
Speaker 1: How does it know who made it?

913
00:51:16,560 --> 00:51:19,500
Speaker 1: Because of digital signing, encrypted signatures.

914
00:51:19,820 --> 00:51:23,000
Speaker 0: Which is, in many ways, more important than encryption.

915
00:51:23,243 --> 00:51:23,466
Speaker 1: Right.

916
00:51:23,487 --> 00:51:32,859
Speaker 1: So, the way this works -- I think we've discussed it in the show a long time ago, signing or digital signing -- is you make a private key and you make a public key, RSA, just like you're using for SSH or for anything else, right?

917
00:51:33,361 --> 00:51:37,700
Speaker 1: You give the public key to everyone in the world because it's a public key, and you keep the private key just for yourself.

918
00:51:38,821 --> 00:51:41,340
Speaker 1: Then you can take a file or any data, right?

919
00:51:41,522 --> 00:51:43,580
Speaker 1: I take a text file I'm going to send to RIM, right?

920
00:51:43,941 --> 00:51:46,720
Speaker 1: I don't need to encrypt this text file because all it has is show ideas.

921
00:51:46,860 --> 00:51:48,117
Speaker 1: I don't care if the government looks at it.

922
00:51:48,581 --> 00:51:53,379
Speaker 1: But I want them to know these are show ideas for me and not from some listener trying to mess with him.

923
00:51:53,961 --> 00:51:55,840
Speaker 1: So, he has my public key.

924
00:51:56,040 --> 00:51:57,899
Speaker 1: He knows that's my public key because I gave it to him.

925
00:51:58,562 --> 00:52:00,859
Speaker 1: And I have my private key that no one else on earth has.

926
00:52:01,481 --> 00:52:05,400
Speaker 1: I can apply that private key to the text file, right?

927
00:52:05,561 --> 00:52:08,360
Speaker 1: And add a little bit of data to the text file, right?

928
00:52:08,483 --> 00:52:09,099
Speaker 1: It's not encrypted.

929
00:52:09,543 --> 00:52:11,020
Speaker 1: And then I give RIM the text file.

930
00:52:11,384 --> 00:52:12,399
Speaker 1: He doesn't know my private key.

931
00:52:13,121 --> 00:52:16,479
Speaker 1: But when he gets that text file, he sees a little bit of extra data on there.

932
00:52:17,062 --> 00:52:22,810
Speaker 1: And he takes my public key, and he takes that little bit of extra data, and he combines them, and he goes, "Aha!

933
00:52:23,492 --> 00:52:24,657
Speaker 1: This file is 100% guaranteed.

934
00:52:25,680 --> 00:52:34,920
Speaker 1: Whoever sent me this file, I know for sure that the person who sent me this file has Scott's private key and knows the passphrase to use Scott's private key.

935
00:52:35,220 --> 00:52:39,120
Speaker 1: Therefore, I'm like 100% sure Scott is the person who made this file.

936
00:52:39,522 --> 00:52:41,920
Speaker 1: And I don't have to, you know, it's definitely not from a listener.

937
00:52:42,241 --> 00:52:48,260
Speaker 1: The odds of that would mean that the listener has some ridiculously powerful computer and a lot of time on their hands, right?

938
00:52:48,623 --> 00:52:50,219
Speaker 1: So you can guarantee that it's from me.

939
00:52:50,600 --> 00:52:58,137
Speaker 1: Well, iOS, when it gets a new OS and decides whether or not to update it, it looks and it says, "Is this digitally

940
00:52:58,177 --> 00:52:58,599
Speaker 1: signed?"

941
00:52:58,720 --> 00:52:58,901
Speaker 1: Right?

942
00:52:59,163 --> 00:53:01,979
Speaker 1: "Am I 100% sure this is from the person it's supposed to be

943
00:53:02,019 --> 00:53:02,200
Speaker 1: from?".

944
00:53:02,240 --> 00:53:03,292
Speaker 1: And who is it supposed to be from?

945
00:53:03,818 --> 00:53:04,020
Speaker 1: Apple.

946
00:53:04,680 --> 00:53:09,939
Speaker 1: Apple has a private key at Apple headquarters, and they sign the iOS updates with it.

947
00:53:10,701 --> 00:53:16,680
Speaker 1: And your iPhone will only, your iPad will only install a new iOS if it's been digitally signed by Apple.

948
00:53:16,700 --> 00:53:21,258
Speaker 1: If it was signed by anyone else in their private key, it would say, "Fuck

949
00:53:21,299 --> 00:53:21,499
Speaker 1: no.".

950
00:53:21,640 --> 00:53:25,767
Speaker 1: And that's why it's hard to jailbreak an iPhone, because the iPhone is just like, "I'm not installing this,".

951
00:53:25,888 --> 00:53:27,560
Speaker 1: and you need to learn Apple's private key.

952
00:53:27,921 --> 00:53:34,140
Speaker 1: So the FBI can't trick the iPhone into stalling software unless Apple applies their signature to it.

953
00:53:34,240 --> 00:53:40,759
Speaker 1: Apple could apply that signature to Linux, like a Linux kernel, and you could load that onto an iPhone, but they won't.

954
00:53:41,180 --> 00:53:45,077
Speaker 0: And what the FBI is basically demanding of Apple isn't just make us a backdoor.

955
00:53:45,921 --> 00:53:54,120
Speaker 0: They're basically saying, "Do unpaid labor for us to create the mechanism of having a backdoor, and give us the product of that work, simultaneously--".

956
00:53:54,221 --> 00:53:57,220
Speaker 1: And sign it with your key so it'll actually install.

957
00:53:57,520 --> 00:53:57,620
Speaker 0: Yep.

958
00:53:57,640 --> 00:54:02,058
Speaker 0: "Simultaneously destroying the security of your device and every other iPhone on

959
00:54:02,159 --> 00:54:02,299
Speaker 0: Earth.".

960
00:54:02,440 --> 00:54:02,525
Speaker 0: Yep.

961
00:54:03,520 --> 00:54:11,280
Speaker 0: So this makes it all really hilarious that the Republicans in Congress are trying to put forth a law to ban the government from ever using Apple products again.

962
00:54:11,723 --> 00:54:12,068
Speaker 1: Sure.

963
00:54:12,353 --> 00:54:12,800
Speaker 1: Go for it.

964
00:54:12,820 --> 00:54:19,474
Speaker 0: And what they're basically saying is, "We refuse to use the only actually secure phones because we're mad that they're

965
00:54:19,494 --> 00:54:19,898
Speaker 0: secure.".

966
00:54:20,100 --> 00:54:21,760
Speaker 1: Yes, make the government very insecure.

967
00:54:22,320 --> 00:54:24,335
Speaker 0: Yes, please do this.

968
00:54:24,537 --> 00:54:24,920
Speaker 1: Yes.

969
00:54:27,442 --> 00:54:32,980
Speaker 1: So the question is, Apple wants to make it to where the phone would even be protected from Apple.

970
00:54:33,942 --> 00:54:34,915
Speaker 1: So how would you do this?

971
00:54:36,081 --> 00:54:49,480
Speaker 0: Well, you have the local device's key that it uses to encrypt, and that itself could also be signed with Apple's key, and the local phone could have the ability to destroy its own key if it's compromised.

972
00:54:49,880 --> 00:54:56,019
Speaker 1: Right, so what you would do is you'd encrypt all the data on the phone, your personal data, with a key that's on the phone.

973
00:54:56,141 --> 00:54:57,259
Speaker 0: And isn't anywhere else.

974
00:54:57,440 --> 00:54:58,639
Speaker 1: And is nowhere else, right?

975
00:54:59,001 --> 00:55:01,120
Speaker 1: And that key has a passphrase, obviously, right?

976
00:55:01,220 --> 00:55:08,280
Speaker 1: So if you were to say, "Take the SSD out of the phone and try to look at the data," it would just be an encrypted mess.

977
00:55:08,481 --> 00:55:13,980
Speaker 1: And even if the private key is there, that private key itself is encrypted with a symmetric encryption.

978
00:55:14,120 --> 00:55:16,099
Speaker 1: If you don't know the passphrase, you can't decrypt that either.

979
00:55:16,200 --> 00:55:17,479
Speaker 0: Yeah, the passphrase is part of the math.

980
00:55:17,820 --> 00:55:21,920
Speaker 1: Right, so it's like, "Good luck," unless the person has a really shitty password, right?

981
00:55:23,841 --> 00:55:34,300
Speaker 1: And now, even if Apple were to replace the OS, right, because they were forced to sign this new OS, that new OS still can't read any of the person's personal data, right?

982
00:55:34,340 --> 00:55:35,839
Speaker 1: It's all encrypted with a different key.

983
00:55:36,723 --> 00:55:40,520
Speaker 1: And then when you log into your phone and unlock it, that's when it decrypts everything.

984
00:55:41,263 --> 00:55:43,260
Speaker 1: And then when you lock your phone up, it's encrypted again.

985
00:55:43,520 --> 00:55:52,880
Speaker 0: Now, obviously, it's more complex than that, but the gist of it is that it is possible to make an iPhone that isn't even vulnerable to Apple doing custom work to break in.

986
00:55:53,820 --> 00:56:04,158
Speaker 1: Apple doing custom work can still replace the OS, but they could never access your personal data, no matter what, because that private key to access that personal data is entirely under your control and no one else's.

987
00:56:04,320 --> 00:56:10,205
Speaker 0: And I think Apple's going to do it, because their letter to the FBI, an open letter, was basically, "Fuck you.

988
00:56:10,426 --> 00:56:10,928
Speaker 0: Fuck you.

989
00:56:10,969 --> 00:56:12,356
Speaker 0: Fuck everyone involved in this.

990
00:56:12,376 --> 00:56:12,898
Speaker 0: Fuck

991
00:56:12,999 --> 00:56:13,099
Speaker 0: you.".

992
00:56:13,200 --> 00:56:13,648
Speaker 0: "We speak for the

993
00:56:13,750 --> 00:56:13,975
Speaker 0: user.".

994
00:56:15,021 --> 00:56:15,486
Speaker 0: "Fuck you.

995
00:56:15,506 --> 00:56:15,810
Speaker 0: Fuck

996
00:56:15,911 --> 00:56:16,174
Speaker 0: you.".

997
00:56:19,408 --> 00:56:20,459
Speaker 0: I could watch that movie again.

998
00:56:21,840 --> 00:56:22,658
Speaker 1: I've seen that enough times.

999
00:56:23,060 --> 00:56:34,780
Speaker 0: So if you had any doubts about who's in the right or who's in the wrong, like, not for nothing, if you still think that Apple should bend over to the FBI, I personally think you're a dangerous person.

1000
00:56:35,801 --> 00:56:36,467
Speaker 1: Dangerous person?

1001
00:56:36,487 --> 00:56:37,820
Speaker 1: They're not just a stupid person?

1002
00:56:38,280 --> 00:56:39,280
Speaker 0: Well, stupid people are dangerous.

1003
00:56:40,320 --> 00:56:40,413
Speaker 1: Sure.

1004
00:56:41,281 --> 00:56:48,000
Speaker 0: Someone who wants there to be backdoors in security that people will use to be secure.

1005
00:56:48,020 --> 00:56:49,187
Speaker 1: Take your lock off your house.

1006
00:56:49,951 --> 00:56:50,615
Speaker 1: Let everyone in.

1007
00:56:50,655 --> 00:56:51,440
Speaker 1: Who wants to come in?

1008
00:56:51,560 --> 00:56:52,168
Speaker 0: Just give me a key.

1009
00:56:52,188 --> 00:56:53,040
Speaker 0: You can trust me, right?

1010
00:56:53,220 --> 00:56:53,306
Speaker 0: Yeah.

1011
00:57:00,411 --> 00:57:02,550
Speaker 0: This has been Geek Nights with Rem and Scott.

1012
00:57:02,690 --> 00:57:07,690
Speaker 0: Special thanks to DJ Pretzel for the opening music, Kat Lee for web design, and Brando K for the logos.

1013
00:57:08,070 --> 00:57:13,090
Speaker 1: Be sure to visit our website at FrontRowCrew.com for show notes, discussion news, and more.

1014
00:57:13,410 --> 00:57:16,010
Speaker 0: Remember, Geek Nights is not one, but four different shows.

1015
00:57:16,270 --> 00:57:20,770
Speaker 0: Sci-Tech Mondays, Gaming Tuesdays, Anime Comic Wednesdays, and Indiscriminate Thursdays.

1016
00:57:21,130 --> 00:57:24,286
Speaker 1: Geek Nights is distributed under a Creative Commons Attribution 3.0 license.

1017
00:57:25,590 --> 00:57:28,590
Speaker 1: Geek Nights is recorded live with no studio and no audience.

1018
00:57:28,810 --> 00:57:31,710
Speaker 1: But unlike those other late shows, it's actually recorded at night.

1019
00:57:49,080 --> 00:57:50,980
Speaker 0: Handy patrons for this episode of Geek Nights.

1020
00:57:51,140 --> 00:58:35,120
Speaker 0: In order of the amount of money they give us on a continuing basis are... Eric Solvarod, Can't Melt Steel Beams, Kinetic Man, Aaron Cerise, Chris Midkiff, Chris Knox, I Will Now Make Rimsay, Titty Sprinkles, I've Thought Carefully and I'm Still Going to Pay, Daniel Redman, Chris Haddad, Sean Klein, Chris Reimer, and Thomas Hahn.

1021
00:58:35,780 --> 00:58:47,487
Speaker 0: The main reason, the secret, that you get smooth radio voice instead of my normal sort of yelling tone is that usually when I record this part we've already finished the show and I've already packed away all the stuff and then I remember that I have to do this.

1022
00:58:47,789 --> 00:58:51,565
Speaker 0: so I just pull the microphone out and hold it in my hand so I don't have the pop filter.

1023
00:58:51,987 --> 00:58:58,940
Speaker 0: Since I don't have the pop filter, I have to be very careful about how I speak and I have to use a very, very, very more standard radio voice.

1024
00:58:59,340 --> 00:59:06,659
Speaker 0: I also tend to get very close to it because I'm not yelling because I'm the only person in the room and it's kind of weird to yell into a microphone when you're the only person in the room.

1025
00:59:07,200 --> 00:59:11,340
Speaker 0: But as you may have noticed, the first of our videos from Packed South is up.

1026
00:59:11,681 --> 00:59:13,385
Speaker 0: The second one will be up at some point.

1027
00:59:13,846 --> 00:59:31,359
Speaker 0: The move, Adam, failed to be created for the Atari part of our Atari Game Design panel which, if it's unrecoverable, is fine because I've recorded it at MAGFest and we're doing an encore at PAX East so I'll have plenty of opportunities to re-re-re-record it but I think I will be able to repair the MP4.

1028
00:59:31,760 --> 00:59:36,680
Speaker 0: so I should get that video up as soon as I get a chance as well as episode 15 of Geek Nights Presents Utena.

1029
00:59:37,363 --> 00:59:40,740
Speaker 0: And now I leave you with something pretty weird.

1030
01:00:38,700 --> 01:00:40,004
Speaker 1: Great, great, great.

1031
01:00:47,780 --> 01:00:51,900
Speaker 1: Tonight, a monopoly to rescue your heart.

1032
01:00:52,763 --> 01:00:54,052
Speaker 0: Great, great, great.

1033
01:00:54,072 --> 01:00:55,059
Speaker 0: He's the legend of...

