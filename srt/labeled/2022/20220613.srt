1
00:00:08,700 --> 00:00:09,988
Rym: It's Monday, June 13th, 2022.

2
00:00:09,988 --> 00:00:10,209
Rym: I'm Rym.

3
00:00:13,526 --> 00:00:14,060
Scott: I'm Scott.

4
00:00:14,340 --> 00:00:15,605
Rym: And this is GeekNights!

5
00:00:15,645 --> 00:00:21,458
Rym: Tonight, AI and machine learning are a hot topic for a bunch of reasons, so we're gonna talk a little bit about that.

6
00:00:23,044 --> 00:00:24,480
Rym: And know your chatbot is not sentient.

7
00:00:26,140 --> 00:00:27,178
Scott: Let's do this.

8
00:00:29,262 --> 00:00:40,860
Rym: This was one of the longer hiatus we've ever had in GeekNights, and it was only because the NHL couldn't do as a solid and scheduled the Stanley Cup playoffs opposite GeekNights.

9
00:00:41,040 --> 00:00:41,799
Scott: They had to schedule them.

10
00:00:42,061 --> 00:00:42,503
Scott: Hold on.

11
00:00:42,563 --> 00:00:46,140
Scott: So first of all, they scheduled it the best they've ever scheduled it.

12
00:00:46,264 --> 00:00:46,598
Scott: That is true.

13
00:00:47,360 --> 00:00:48,242
Scott: Every other night, right?

14
00:00:48,543 --> 00:00:54,780
Scott: The problem was that A, we talked about this before.

15
00:00:54,880 --> 00:00:58,260
Scott: We had tickets to every home game, which we've never had in our lives.

16
00:00:58,901 --> 00:01:02,580
Scott: B, we kept fucking winning, which means we kept playing.

17
00:01:02,701 --> 00:01:07,140
Scott: And C, two series go to seven and another one go to six.

18
00:01:07,261 --> 00:01:08,499
Scott: So it's like we didn't have a break.

19
00:01:08,660 --> 00:01:12,480
Scott: If they would have swept someone, we could have had a GeekNights time in there.

20
00:01:13,020 --> 00:01:18,599
Rym: I mean, what I expected was the Rangers to crush the pens, get crushed by Carolina, and that's it.

21
00:01:19,021 --> 00:01:22,839
Rym: And then instead, I got game seven, game seven, and then a 4-2 series.

22
00:01:24,261 --> 00:01:24,462
Scott: Right.

23
00:01:24,502 --> 00:01:27,760
Scott: And then on top of that, other life shit was going on.

24
00:01:27,900 --> 00:01:31,240
Scott: I think I discussed this on all the other platforms.

25
00:01:31,421 --> 00:01:35,300
Scott: So basically, I unexpectedly got a new job.

26
00:01:35,500 --> 00:01:36,859
Scott: I was not looking for a new job.

27
00:01:37,041 --> 00:01:38,660
Scott: I was very happy with the job I had.

28
00:01:38,980 --> 00:01:44,240
Scott: If you want to work at the place I was working, there's openings and it's a good place to work.

29
00:01:44,640 --> 00:01:51,600
Scott: If the new job is a trap, which it very well could be, I would go back there, no question.

30
00:01:53,560 --> 00:02:04,860
Scott: But yeah, so, you know, recruiters, you know, they come to me and it's like, you know, I'm very sort of, you know, professional, but also somewhat aggressive with them.

31
00:02:05,040 --> 00:02:05,281
Scott: Right.

32
00:02:05,382 --> 00:02:11,760
Scott: In terms of like, you know, I'm like, look, you know, if your job is immoral, then I'm just I'm like, no, I won't work there.

33
00:02:11,960 --> 00:02:12,121
Scott: Bye.

34
00:02:12,341 --> 00:02:12,582
Scott: Right.

35
00:02:13,124 --> 00:02:18,588
Scott: If the job is morally acceptable, I'm like, all right, well, is it better be better than the job I have?

36
00:02:18,628 --> 00:02:20,120
Scott: The job I got is real good.

37
00:02:20,160 --> 00:02:21,259
Rym: And there's two things I'll take.

38
00:02:21,460 --> 00:02:24,132
Rym: Like, I would take better because it's engaging work.

39
00:02:24,313 --> 00:02:25,720
Rym: better because it's better for the world.

40
00:02:25,921 --> 00:02:29,600
Rym: Like it's activism or it pays so much more that I'll just take the money.

41
00:02:30,241 --> 00:02:30,341
Scott: Right.

42
00:02:30,361 --> 00:02:35,560
Scott: Well, I mean, some months ago, someone did make me an offer for more engaging, more.

43
00:02:35,580 --> 00:02:38,900
Scott: at least the product was, I guess, somewhat more interesting than music publishing.

44
00:02:39,280 --> 00:02:40,349
Scott: Right.

45
00:02:40,430 --> 00:02:41,599
Scott: But, you know, it was it was cool.

46
00:02:42,381 --> 00:02:47,600
Scott: And, you know, but when it came time to actually make me an offer, it was similar monies.

47
00:02:48,242 --> 00:02:52,280
Scott: And I was like, you know, this is not better than the job I have.

48
00:02:52,461 --> 00:02:57,280
Scott: And I demanded either 20 percent more monies or four days a week.

49
00:02:57,401 --> 00:02:57,602
Scott: Right.

50
00:02:57,622 --> 00:03:00,180
Scott: Which would be 20 percent less time, I said, one or the other.

51
00:03:00,340 --> 00:03:02,280
Scott: And they were like, too rich for our blood.

52
00:03:02,682 --> 00:03:02,967
Scott: Thanks.

53
00:03:03,008 --> 00:03:03,558
Scott: Have a nice day.

54
00:03:03,660 --> 00:03:04,367
Scott: And I'm like, all right.

55
00:03:04,731 --> 00:03:05,580
Scott: I still got a great job.

56
00:03:05,640 --> 00:03:06,062
Scott: I don't care.

57
00:03:06,123 --> 00:03:06,304
Scott: Yeah.

58
00:03:06,344 --> 00:03:10,045
Rym: And also, I respect a recruiter who's up front and is like, oh, OK, never mind.

59
00:03:10,186 --> 00:03:11,900
Scott: Like, well, that wasn't the recruiter.

60
00:03:11,980 --> 00:03:12,764
Scott: That was actually.

61
00:03:12,865 --> 00:03:15,759
Scott: I went through a whole interview process at the end.

62
00:03:16,622 --> 00:03:18,920
Scott: And the recruiters always inflate.

63
00:03:19,021 --> 00:03:27,405
Scott: They like to tell you it's going to be a lot of money because then they convince you to go through the interview process and then hoping that you'll feel bad and be bad at negotiating.

64
00:03:27,727 --> 00:03:29,880
Rym: And I've always taken the exact opposite.

65
00:03:30,040 --> 00:03:35,560
Rym: My strategy in interviewing my entire life is even if the salary is low, if I like the job, I would go for it anyway.

66
00:03:36,061 --> 00:03:37,860
Rym: Make that like make sure I crush the interview.

67
00:03:37,980 --> 00:03:38,643
Rym: So they love me.

68
00:03:38,663 --> 00:03:41,696
Scott: And I mean, I try to walk away a lot.

69
00:03:43,462 --> 00:03:45,799
Scott: I definitely crush the interviews and they love me.

70
00:03:46,360 --> 00:03:47,223
Scott: Right.

71
00:03:47,624 --> 00:03:57,360
Scott: It was just that, you know, while I would have taken that job in a heart attack, if I had no job, it wasn't significantly better than the job I had.

72
00:03:57,600 --> 00:03:58,163
Scott: So why change?

73
00:03:58,203 --> 00:04:01,720
Rym: Well, that's rule number two of my life is no lateral moves ever.

74
00:04:02,941 --> 00:04:03,042
Scott: Right.

75
00:04:03,062 --> 00:04:07,382
Scott: So anyway, so another place comes along where I'm going to be working starting next month.

76
00:04:08,286 --> 00:04:12,867
Scott: And, you know, it's like it's got positives, it's got negatives, but it's not evil.

77
00:04:13,130 --> 00:04:13,879
Scott: The people are nice.

78
00:04:14,303 --> 00:04:16,480
Scott: You know, I'm going through the interviews there.

79
00:04:17,122 --> 00:04:17,930
Scott: Same deal, right?

80
00:04:17,990 --> 00:04:18,959
Scott: I crush the interviews.

81
00:04:19,141 --> 00:04:19,868
Scott: They're in love with me.

82
00:04:19,908 --> 00:04:21,240
Scott: They want to hire me so badly.

83
00:04:21,742 --> 00:04:22,063
Scott: Right.

84
00:04:22,585 --> 00:04:25,334
Scott: You know, and I'm like, you know, I get to.

85
00:04:25,374 --> 00:04:27,608
Scott: finally I get to the negotiating part at the end.

86
00:04:28,156 --> 00:04:28,400
Scott: Right.

87
00:04:28,861 --> 00:04:32,099
Scott: And I'm like, you know, I'd really like to work four days a week.

88
00:04:32,541 --> 00:04:33,765
Scott: Right.

89
00:04:33,785 --> 00:04:37,970
Scott: And I'm like, I know you're going to say no, but that's where I got to start my ask.

90
00:04:38,215 --> 00:04:38,460
Scott: Right.

91
00:04:40,161 --> 00:04:48,500
Scott: And somehow I don't know how, but the offer ended up being both four days a week and 20 percent more.

92
00:04:48,641 --> 00:04:56,420
Rym: So all I got to say is if this works out, like if they're hiring and they need a product manager, just, you know, we'll see what happens.

93
00:04:56,541 --> 00:04:56,723
Scott: Right.

94
00:04:56,905 --> 00:04:58,380
Scott: You know, this could be a trap.

95
00:04:58,500 --> 00:04:59,700
Scott: This could be too good to be true.

96
00:04:59,820 --> 00:05:01,720
Scott: This could be who knows what's going to happen.

97
00:05:01,920 --> 00:05:03,617
Scott: But I certainly say no.

98
00:05:04,100 --> 00:05:04,727
Scott: It's like I didn't.

99
00:05:04,909 --> 00:05:05,799
Scott: I didn't want to go.

100
00:05:06,141 --> 00:05:09,560
Scott: I was like I didn't want to leave the job I had, but I had kind of no choice.

101
00:05:09,620 --> 00:05:10,310
Rym: You know what it's like?

102
00:05:10,350 --> 00:05:11,060
Rym: It's like a bidding.

103
00:05:11,200 --> 00:05:17,840
Rym: It's like in a bidding game or even like a train game or something where I put for some sort of like ridiculous lowball offer like in sieve.

104
00:05:17,980 --> 00:05:21,720
Rym: I'm like, hey, Scott, you gave me three luxury resources for like one nighter.

105
00:05:22,582 --> 00:05:24,179
Rym: And then the other side says, OK.

106
00:05:24,442 --> 00:05:25,174
Rym: And I'm like, wait, what?

107
00:05:25,357 --> 00:05:25,520
Rym: What?

108
00:05:25,880 --> 00:05:26,939
Rym: I didn't expect you to say yes.

109
00:05:27,660 --> 00:05:28,547
Rym: Nobody says yes to that.

110
00:05:28,588 --> 00:05:29,515
Rym: What's what's going on?

111
00:05:30,822 --> 00:05:31,507
Scott: I needed that night.

112
00:05:31,548 --> 00:05:33,300
Scott: I was the one nighter I needed to make a gun.

113
00:05:33,440 --> 00:05:34,142
Scott: Boom.

114
00:05:35,627 --> 00:05:39,960
Scott: Anyway, so, yeah, that's that took up some time and distracted from GeekNights.

115
00:05:40,040 --> 00:05:44,100
Scott: But now now we're in we're in the opposite situation.

116
00:05:44,281 --> 00:05:45,107
Scott: Right.

117
00:05:45,530 --> 00:05:46,979
Scott: Whereas I don't have to work for a whole month.

118
00:05:47,320 --> 00:05:48,002
Scott: It's summertime.

119
00:05:48,023 --> 00:05:53,743
Scott: I worked on the website most of the day to day doing lots of stuff, the GeekNights website.

120
00:05:54,205 --> 00:05:55,390
Scott: And then what?

121
00:05:55,490 --> 00:05:58,402
Scott: even when I start working next month, it won't be on Friday.

122
00:05:58,422 --> 00:06:06,200
Scott: I know there's no GeekNights on Friday, but that still is going to give me time eventually to do some kind of content or something.

123
00:06:07,161 --> 00:06:11,798
Scott: At least some of those Fridays once in a while will be spent doing something.

124
00:06:11,818 --> 00:06:14,220
Scott: GeekNights Internet content production related.

125
00:06:14,741 --> 00:06:16,028
Scott: Right.

126
00:06:16,049 --> 00:06:17,940
Scott: So that's all I'm willing to say.

127
00:06:18,500 --> 00:06:20,138
Scott: I'm not willing to talk any more than that.

128
00:06:22,422 --> 00:06:25,916
Rym: So yes, the hockey will keep watching because honestly, this is going to be.

129
00:06:26,298 --> 00:06:27,511
Scott: it's only a few games left.

130
00:06:27,796 --> 00:06:27,999
Rym: Yep.

131
00:06:29,681 --> 00:06:33,640
Rym: Also, I am very confident Colorado is going to win.

132
00:06:34,221 --> 00:06:37,580
Scott: But I said that early in the regular season.

133
00:06:37,860 --> 00:06:37,940
Rym: Yeah.

134
00:06:37,960 --> 00:06:43,969
Rym: And when we say the whole thing, whoever came out of the Eastern Conference is just going to be sacrificed before the Nidhogg.

135
00:06:44,010 --> 00:06:45,840
Rym: that is the Colorado avalanche.

136
00:06:46,401 --> 00:06:47,340
Scott: That's what's going to happen.

137
00:06:47,840 --> 00:06:52,524
Rym: We were just fighting over who gets to have the honor of being crushed by them in the finals.

138
00:06:53,332 --> 00:06:54,080
Scott: Well, we'll see.

139
00:06:54,600 --> 00:07:00,480
Rym: But the one thing so I Emily and I ended up being at the last home game because that's like how our tickets broke down.

140
00:07:00,600 --> 00:07:00,901
Scott: We were at.

141
00:07:01,162 --> 00:07:05,762
Scott: we were at the last game at Madison Square Garden, but the game was being played in Florida.

142
00:07:05,983 --> 00:07:10,902
Rym: But there is a technology angle here because something happened that I this happens.

143
00:07:11,384 --> 00:07:20,880
Rym: It always happens in the playoffs, but maybe because I haven't paid attention to the playoffs or gone to games in a long time, but also it feels like it was more than like the last time I went to playoffs games.

144
00:07:21,401 --> 00:07:31,540
Rym: The production value of the technology, the lighting and the sound and the stuff happening in the arena leveled up every round of the playoffs.

145
00:07:32,082 --> 00:07:34,579
Rym: And every time it leveled up more than I expected it to.

146
00:07:34,840 --> 00:07:39,240
Rym: So in the last round, the last game we went to, they had smoke, they had lasers.

147
00:07:39,341 --> 00:07:40,431
Rym: So not just I got the lasers.

148
00:07:41,821 --> 00:07:47,360
Rym: They had lasers that are if they hit a person in the eye would be fucking dangerous.

149
00:07:48,122 --> 00:07:48,713
Rym: They were true.

150
00:07:48,795 --> 00:07:49,020
Rym: Yes.

151
00:07:49,625 --> 00:07:50,400
Rym: So bright.

152
00:07:51,040 --> 00:07:55,199
Scott: This is a laser that was like going across MSG from one end to the other.

153
00:07:55,600 --> 00:07:55,700
Rym: Right.

154
00:07:55,720 --> 00:08:04,340
Rym: And culminating to like a spot, maybe an inch, like inch and a half in diameter that was setting the building on fire.

155
00:08:04,481 --> 00:08:07,400
Scott: But it's like they were like that hit you in the eye.

156
00:08:07,541 --> 00:08:08,940
Rym: I think you would be insta blinded.

157
00:08:09,323 --> 00:08:11,060
Rym: And it looks like they were hard.

158
00:08:11,340 --> 00:08:20,882
Scott: And also they were bright enough to be visible just in the ambient air slightly, which well, they also combine them with the smoke machine that come out from under the scoreboard.

159
00:08:20,922 --> 00:08:26,479
Rym: But some of them were occasionally touching briefly the bottom of the American flag that's hanging up there.

160
00:08:27,062 --> 00:08:29,337
Rym: And it was like a sun every time it touched it.

161
00:08:30,320 --> 00:08:41,979
Rym: So what's interesting is if you look at how they work, how they work this out, they were all aimed and I'm pretty there had better have been hardware controls, ensuring that they couldn't aim anywhere else.

162
00:08:42,421 --> 00:08:49,200
Rym: They were flying around, but they were only aiming at the boundaries between the levels in the arena, like the little wall.

163
00:08:49,560 --> 00:08:52,900
Scott: There was some that were aimed at the bottom of the boards on the ice.

164
00:08:53,061 --> 00:09:00,999
Scott: And there were some that were aimed on like, you know, the flat where the score, the horizontal scoreboards are in between the one hundred two hundred level.

165
00:09:01,644 --> 00:09:03,320
Scott: But yeah, they're basically not hitting anywhere.

166
00:09:03,421 --> 00:09:07,000
Scott: There were seats and anywhere there could be a human body.

167
00:09:08,567 --> 00:09:08,729
Rym: Right.

168
00:09:08,749 --> 00:09:10,119
Scott: We were flying or something.

169
00:09:10,381 --> 00:09:10,642
Rym: Yeah.

170
00:09:10,702 --> 00:09:29,740
Rym: Which is just a fascinating use of technology, because if there weren't hardware lockouts, if they were relying solely on the programing of the controllers for those laser lights as someone who has done formal risk analysis professionally for more than a decade, that I cannot imagine what the risk assessment was of that situation.

171
00:09:29,940 --> 00:09:32,899
Scott: I mean, they could have been doing it with software, just the software really good.

172
00:09:33,283 --> 00:09:35,159
Scott: Yeah, I just know how that shit works.

173
00:09:35,982 --> 00:09:41,060
Rym: I, I deeply distrust software interlocks on dangerous hardware.

174
00:09:41,900 --> 00:09:45,900
Scott: Yeah, but anyway, they had a lot of cool special effects going on.

175
00:09:47,481 --> 00:09:51,980
Rym: So in some actual tech news, let's talk about Web Push for Safari.

176
00:09:52,820 --> 00:09:56,560
Scott: Well, so Apple's WWDC happened, right?

177
00:09:56,720 --> 00:10:04,240
Scott: This is where at the time of year where they have a big developer conference for Apple developers and they announce all their new Apple shit.

178
00:10:04,880 --> 00:10:10,480
Scott: And usually it's all like what's going to be updated in iOS, what's going to be updated in Mac OS, all that kind of stuff.

179
00:10:10,740 --> 00:10:11,442
Scott: Right.

180
00:10:11,462 --> 00:10:17,565
Scott: And the big update they announced was this sort of something stage center stage something.

181
00:10:17,585 --> 00:10:19,300
Scott: I forget what it's called.

182
00:10:19,460 --> 00:10:22,540
Scott: Apple stage something main stage.

183
00:10:23,702 --> 00:10:25,800
Scott: No, not that stage manager.

184
00:10:25,981 --> 00:10:26,402
Scott: There it is.

185
00:10:26,482 --> 00:10:40,378
Scott: Apple stage manager, which is basically, you know, we talk about a lot of how Apple will take something that someone else has already invented and sort of just polish it and make a new UI for it and act like they invented it.

186
00:10:41,480 --> 00:10:45,900
Scott: Like when they came out with spaces or virtual desktops.

187
00:10:46,200 --> 00:10:46,301
Scott: Right.

188
00:10:46,743 --> 00:10:51,680
Scott: And it's like virtual desktops have been on Unix and, you know, X11 Windows systems for decades.

189
00:10:52,002 --> 00:10:52,203
Scott: Yeah.

190
00:10:52,223 --> 00:11:01,000
Rym: And they acted like they invented it like it was even funnier because by the time that appeared was around the time when I had two giant monitors and didn't really use virtual desktops anymore.

191
00:11:01,861 --> 00:11:08,920
Scott: But anyway, but they stage manager, which they just announced, is basically like it's effectively virtual desktops.

192
00:11:09,000 --> 00:11:09,241
Scott: Right.

193
00:11:09,522 --> 00:11:14,380
Scott: You group your windows by task and then you can sort of, you know, bring out.

194
00:11:14,640 --> 00:11:16,240
Scott: That's how I use virtual desktops anyway.

195
00:11:16,381 --> 00:11:16,542
Scott: Right.

196
00:11:16,603 --> 00:11:18,700
Scott: It's like I'll have a virtual desktop for each task.

197
00:11:19,282 --> 00:11:24,159
Scott: And then you just, you know, you swap out all your windows for a whole different set of windows to work on something else.

198
00:11:24,260 --> 00:11:24,442
Scott: Right.

199
00:11:24,462 --> 00:11:29,600
Scott: So it's like on the other virtual desktop that I'm not looking at right now, there's all the stuff to work on the GeekNights website.

200
00:11:29,742 --> 00:11:29,885
Scott: Right.

201
00:11:29,905 --> 00:11:30,598
Scott: But all that's hidden.

202
00:11:31,022 --> 00:11:34,440
Scott: Because now I'm working on my desktop where I can do a GeekNights episode.

203
00:11:35,342 --> 00:11:35,503
Scott: Right.

204
00:11:35,523 --> 00:11:42,079
Scott: So the stage manager is a new UI to effectively, you know, group applications by task and switch between tasks.

205
00:11:42,864 --> 00:11:44,300
Scott: And it's actually really nice looking.

206
00:11:45,102 --> 00:11:45,885
Scott: I kind of like it.

207
00:11:46,025 --> 00:11:49,880
Scott: I wish my computer did that, but not enough to go get a Mac.

208
00:11:51,541 --> 00:12:01,900
Scott: But they are bringing it to iPad, which is nice, but they're not bringing it to iPads that don't have M1 processors, which I think is actually a big problem.

209
00:12:02,101 --> 00:12:07,380
Scott: And that, you know, Apple has always been known for the one thing they do.

210
00:12:07,520 --> 00:12:14,280
Scott: I think the best thing Apple does is that they support really old hardware with new OS updates for a long time.

211
00:12:14,720 --> 00:12:14,921
Scott: Yeah.

212
00:12:15,322 --> 00:12:25,618
Scott: My iPhone 7 was which, you know, I guess I could have waited another year or two before updating it, you know, but I think I guess it was seven years before of lifetime.

213
00:12:26,141 --> 00:12:35,141
Scott: If you bought an iPhone 7, the day it came out, you could have used it for seven years before it stopped getting iOS updates and you could have even kept using it longer than that.

214
00:12:35,161 --> 00:12:41,800
Scott: You just eventually might run into situations where, you know, updates to iOS apps don't work because your iOS is too old.

215
00:12:41,940 --> 00:12:42,582
Scott: Right now.

216
00:12:43,384 --> 00:12:49,720
Scott: So, yeah, I only kept my iPhone 7 for five years and my Apple Watch 3.

217
00:12:49,720 --> 00:12:50,227
Scott: I bought that.

218
00:12:50,268 --> 00:12:51,079
Scott: I'm wearing it right now.

219
00:12:51,180 --> 00:12:55,080
Scott: It's still good, but it looks like later this year it's not going to be good anymore.

220
00:12:55,562 --> 00:13:01,059
Scott: They're going to have an iOS update that, you know, a watch OS update that finally puts the third watch.

221
00:13:01,900 --> 00:13:02,202
Scott: Right.

222
00:13:02,242 --> 00:13:05,838
Scott: Meanwhile, they're already on like Watch 8 or something or I forget what number they're on.

223
00:13:06,281 --> 00:13:09,620
Rym: Yeah, I have the most recent one that's actually out and it's fine.

224
00:13:10,560 --> 00:13:12,620
Scott: Yeah, I mean, it'll last you many years.

225
00:13:12,761 --> 00:13:12,921
Scott: Right.

226
00:13:12,942 --> 00:13:16,882
Scott: Whereas Android devices, they get maybe a handful of years of updates and then that's it.

227
00:13:17,063 --> 00:13:20,660
Rym: In two years, I went through two and a half Android watches.

228
00:13:21,842 --> 00:13:27,879
Scott: Yeah, I mean, I used to update my Apple phone, my iPhone every two years because the contract would be every two years.

229
00:13:28,562 --> 00:13:36,400
Scott: But and because the hardware updates were so significant, like what's iPhone 3G compared to iPhone 4, compared to iPhone 5, compared to six, compared to seven.

230
00:13:36,884 --> 00:13:37,066
Scott: Right.

231
00:13:37,106 --> 00:13:38,360
Scott: It's like those are big leaps.

232
00:13:39,022 --> 00:13:42,460
Scott: But then after seven, like the leaps weren't there anymore.

233
00:13:42,500 --> 00:13:48,574
Scott: You know, I went from seven to twelve mini, whatever the first mini one, whatever number that was.

234
00:13:48,716 --> 00:13:48,920
Scott: Right.

235
00:13:49,442 --> 00:13:50,124
Scott: So anyway.

236
00:13:51,208 --> 00:13:52,312
Scott: So, yeah, that's the.

237
00:13:52,833 --> 00:13:54,800
Scott: you know, they got their iOS updates and whatnot.

238
00:13:54,880 --> 00:13:55,041
Scott: Right.

239
00:13:55,081 --> 00:14:02,380
Scott: And the big one feature that they announced that really intrigued me was the sort of live notifications.

240
00:14:02,742 --> 00:14:03,045
Scott: Right.

241
00:14:03,630 --> 00:14:04,597
Scott: I forget what they called them.

242
00:14:06,021 --> 00:14:12,814
Scott: But, you know, there are certain situations where let's say you order something like food to be delivered.

243
00:14:13,037 --> 00:14:13,240
Scott: Right.

244
00:14:14,041 --> 00:14:18,562
Scott: Right now, if you turn on all the notifications for your food delivery app of choice.

245
00:14:18,582 --> 00:14:19,265
Scott: Right.

246
00:14:19,386 --> 00:14:19,727
Scott: What they'll?

247
00:14:19,768 --> 00:14:19,989
Scott: they'll?

248
00:14:20,190 --> 00:14:22,603
Scott: they'll message you and be like, we got your order.

249
00:14:22,846 --> 00:14:24,160
Scott: Then they'll give you another notification.

250
00:14:24,541 --> 00:14:26,800
Scott: The restaurant started working on another notification.

251
00:14:27,122 --> 00:14:28,620
Scott: They finished working on another one.

252
00:14:28,700 --> 00:14:29,550
Scott: The delivery is coming.

253
00:14:29,895 --> 00:14:30,340
Scott: Another one.

254
00:14:30,460 --> 00:14:31,368
Scott: The delivery is close.

255
00:14:31,772 --> 00:14:32,619
Scott: The delivery is here.

256
00:14:33,020 --> 00:14:35,900
Scott: You like a pile of notifications for this one thing.

257
00:14:36,341 --> 00:14:36,623
Scott: Right.

258
00:14:36,925 --> 00:14:39,100
Scott: It's like your phone is going ding, ding, ding, ding.

259
00:14:39,241 --> 00:14:39,422
Scott: Right.

260
00:14:39,906 --> 00:14:41,840
Scott: And that's not a great user experience.

261
00:14:41,840 --> 00:14:42,706
Scott: It's kind of a mess.

262
00:14:42,746 --> 00:14:44,720
Scott: And you have to clear out a pile of notifications.

263
00:14:45,481 --> 00:14:45,661
Scott: Right.

264
00:14:46,163 --> 00:14:50,176
Scott: Really, that whole process should be one notification.

265
00:14:50,537 --> 00:14:56,780
Scott: that sort of the notification itself live updates to tell you the current status when you look at it.

266
00:14:56,840 --> 00:14:58,620
Scott: And that's they added that it's basically.

267
00:14:59,161 --> 00:15:17,100
Scott: So when you if you were to now order food on a new, you know, once the new iOS comes out and the apps update themselves, you would get a note, you would order some food, a notification would come up and that notification would just stay there and always be displaying the current status of your food order.

268
00:15:17,740 --> 00:15:21,782
Scott: And when it was done, that thing would go away and it would sort of be on your home screen.

269
00:15:21,802 --> 00:15:22,043
Scott: Right.

270
00:15:22,103 --> 00:15:27,560
Scott: And you could do the same thing for other similar like time gated events, like a sporting event.

271
00:15:27,720 --> 00:15:29,480
Scott: It's like, OK, the sporting event started.

272
00:15:29,621 --> 00:15:33,960
Scott: Now there's a thing on your on your lock screen showing you the live score.

273
00:15:34,443 --> 00:15:36,360
Scott: And then when the game's over, it's gone.

274
00:15:36,561 --> 00:15:36,742
Scott: Right.

275
00:15:36,762 --> 00:15:44,020
Scott: Just one notification, one thing that knows when to come and when to go away and update itself rather than having multiple notifications.

276
00:15:44,360 --> 00:15:46,980
Scott: So that's going to be a good thing.

277
00:15:47,140 --> 00:15:47,522
Scott: I like that.

278
00:15:48,428 --> 00:15:50,500
Scott: So the thing Rym talked about the Web push.

279
00:15:50,820 --> 00:15:51,041
Scott: Right.

280
00:15:51,081 --> 00:15:56,199
Scott: So speaking of notifications, as long as they've had notifications on iOS.

281
00:15:57,061 --> 00:15:57,322
Scott: Right.

282
00:15:57,462 --> 00:16:04,900
Scott: And the way it's worked is that, you know, you had to get an app in the App Store, get an Apple developer account.

283
00:16:05,363 --> 00:16:05,626
Scott: Right.

284
00:16:05,687 --> 00:16:06,820
Scott: Get your app approved.

285
00:16:07,222 --> 00:16:08,819
Scott: Get someone to install your app.

286
00:16:09,241 --> 00:16:15,240
Scott: Get someone to give your app notification permissions, and then the app can notify them within certain bounds.

287
00:16:15,820 --> 00:16:16,082
Scott: Right.

288
00:16:16,183 --> 00:16:18,520
Scott: A behavior that Apple deemed acceptable.

289
00:16:19,482 --> 00:16:25,400
Scott: And then, you know, it's like, you know, in order to not ruin the battery life and annoy the users.

290
00:16:25,704 --> 00:16:25,866
Scott: Right.

291
00:16:25,907 --> 00:16:26,880
Scott: With lots of notifications.

292
00:16:28,522 --> 00:16:38,840
Scott: So that's the way it's been, you might notice if you're anyone who uses a normal computer still that, like even today, desktop Web browsers do notifications.

293
00:16:39,060 --> 00:16:44,980
Scott: Like you can go to a website and it'll pop up an annoying message like, hey, you want notifications from this website?

294
00:16:45,240 --> 00:16:45,320
Scott: Huh?

295
00:16:45,421 --> 00:16:45,601
Scott: Huh?

296
00:16:45,662 --> 00:16:45,883
Scott: Huh?

297
00:16:46,384 --> 00:16:55,060
Scott: And if you say yes, it's like your Web browser is basically going to be keeping open like a little thing in the background, like a little service in the background on your computer.

298
00:16:55,821 --> 00:17:09,959
Scott: And the website, just any old website can send notifications and they'll show up like in Windows or Mac, like as OS level notifications, you know, and that's, you know, it's nice when you want to use it.

299
00:17:10,099 --> 00:17:10,281
Rym: Yeah.

300
00:17:10,321 --> 00:17:13,059
Rym: Ninety nine percent of the time you don't want to use it like that.

301
00:17:13,300 --> 00:17:13,541
Scott: Right.

302
00:17:13,885 --> 00:17:15,500
Scott: When you want to use it, it's nice.

303
00:17:15,721 --> 00:17:24,160
Scott: Like, you know, let's say at the game for the website for train games, you can close your tab for train games and still get a notification in OS level.

304
00:17:24,281 --> 00:17:25,880
Scott: That's like, hey, it's your turn in the train game.

305
00:17:26,001 --> 00:17:26,182
Scott: Right.

306
00:17:26,202 --> 00:17:26,685
Scott: That's nice.

307
00:17:27,167 --> 00:17:29,020
Scott: Or Gmail, like, hey, you got an email.

308
00:17:29,142 --> 00:17:30,199
Scott: That's an important email.

309
00:17:30,842 --> 00:17:35,960
Scott: OS level notification, even though your browser might be closed or, you know, whatever.

310
00:17:36,380 --> 00:17:38,457
Scott: So it can be nice when you want to do it.

311
00:17:39,800 --> 00:17:48,339
Scott: But what Apple is going to do is they're going to enable Web push right on Mac OS and iOS via Safari.

312
00:17:48,540 --> 00:18:02,800
Scott: So this means that somebody with a Web page and no iOS app, no Mac app whatsoever can now send you notifications on iOS just from a website.

313
00:18:04,280 --> 00:18:13,421
Scott: And so there's a video here that's actually targeted towards Web developers who want to implement this feature, but it's also useful because it tells users how the feature is going to work.

314
00:18:14,284 --> 00:18:29,209
Scott: And the key here is that a they are not going to allow you to pop up a thing like, hey, you want notifications, the user must click or do some action in order to start the.

315
00:18:29,370 --> 00:18:30,717
Scott: I want that is key.

316
00:18:31,441 --> 00:18:42,981
Rym: That is very similar to how MailChimp dominated all the mailing services in the early days when they force either second verification or active subscription as opposed to opt out.

317
00:18:43,604 --> 00:18:43,824
Scott: Yep.

318
00:18:43,885 --> 00:18:52,100
Scott: So it's the example they give is like a website that has a bell icon and they're like, you can't ask the user for notification permission until they click the bell icon.

319
00:18:52,702 --> 00:18:55,560
Scott: Then they can say, yes, I want notifications from this website.

320
00:18:57,041 --> 00:19:14,020
Scott: And the second thing is that it is technically possible to do this the Web push and send messages to someone's Web browser and have JavaScript take action in response to those messages without displaying a notification to the user.

321
00:19:14,080 --> 00:19:18,580
Scott: You could just sort of, you know, get the user's computer to do processing or whatever.

322
00:19:19,142 --> 00:19:24,181
Scott: And Apple's like, yeah, you can, you know, a you can hurt someone's battery life like that.

323
00:19:24,743 --> 00:19:29,000
Scott: Be you're using someone's CPU, possibly their phone CPU.

324
00:19:29,181 --> 00:19:33,940
Scott: Hey, phone Bitcoin for me without them realizing you're doing it right.

325
00:19:34,120 --> 00:19:42,823
Scott: So Apple has a system in place where if you don't display a notification to the user quickly enough.

326
00:19:43,526 --> 00:19:49,359
Scott: after you send them a Web push and you do that like three times or something, then that's it.

327
00:19:49,480 --> 00:19:51,100
Scott: They take away your Web push permission.

328
00:19:51,504 --> 00:19:52,900
Scott: It's like you're done, right?

329
00:19:52,941 --> 00:19:53,980
Scott: You can't do it anymore.

330
00:19:54,121 --> 00:19:55,480
Scott: Your whole domain is just blocked.

331
00:19:56,021 --> 00:20:03,720
Rym: I noticed this side thing in the article you link to around service worker APIs to that what it is service work that that seems to be exactly what this is.

332
00:20:03,800 --> 00:20:04,899
Rym: This looks pretty good.

333
00:20:05,181 --> 00:20:15,040
Scott: A service worker API is basically the JavaScript that's sitting on your computer that will leap into action once a notification comes in from the Internet.

334
00:20:15,160 --> 00:20:15,361
Scott: Right.

335
00:20:15,381 --> 00:20:30,406
Scott: So it's like, you know, it's like you leave instructions on the person's phone or on their computer of like what to do if they receive a message from Web site X. And then so when Web site X sends a message, the Safari browser says, oh, where did this message come from?

336
00:20:30,708 --> 00:20:33,022
Scott: It came from Web site X. Send it.

337
00:20:33,103 --> 00:20:34,799
Scott: Do I have a service worker for Web site X?

338
00:20:34,980 --> 00:20:35,322
Scott: I do.

339
00:20:35,644 --> 00:20:38,060
Scott: And it sends the message to the service worker for Web site.

340
00:20:38,181 --> 00:20:43,240
Rym: What are the examples they give on like what is what it's supposed to do, what they expect you to use this for is actually a really good one.

341
00:20:43,621 --> 00:20:50,780
Rym: I'm just going to quote the Web site receiving centralized updates to expensive to calculate data such as geolocation or gyroscope.

342
00:20:51,102 --> 00:20:53,300
Rym: So multiple pages can make use of one set of data.

343
00:20:54,121 --> 00:20:58,040
Scott: Yeah, you could go to a Web site, click on like do something, go away.

344
00:20:58,541 --> 00:21:03,001
Scott: And then once the thing is done, you'll get a notification that's like, hey, thing is done.

345
00:21:03,122 --> 00:21:08,473
Scott: You click on it and that will take you to the Web page with the results of the thing that is now done.

346
00:21:08,493 --> 00:21:08,919
Scott: Right.

347
00:21:09,721 --> 00:21:11,900
Scott: So anyway, yeah, they're now good.

348
00:21:11,980 --> 00:21:24,640
Scott: You're now going to be able to send notifications to users of iOS devices without having to make an iOS app and get in the app store or give Apple money or make an Apple developer account or anything.

349
00:21:24,780 --> 00:21:31,820
Scott: And that is kind of a big deal because, for example, front row crew dot com could make it so that you get a notification on your phone.

350
00:21:31,980 --> 00:21:37,302
Scott: But it's a new GeekNights episode, even though that we don't have an official iPhone app whatsoever.

351
00:21:38,446 --> 00:21:43,943
Scott: So and the notifications are much more useful on the phone than they are the desktop.

352
00:21:44,204 --> 00:21:47,820
Scott: So I foresee do and a lot of people only have phones.

353
00:21:48,502 --> 00:22:00,780
Scott: So I think what you're going to see is a lot more Web sites implementing notifications, implementing them in a way that is acceptable to Apple and won't annoy you by asking you to accept notifications just every just from visiting the Web.

354
00:22:01,160 --> 00:22:07,600
Rym: This is where Apple often serves as a thought leader because they will implement and enforce design patterns that are pleasant to the user.

355
00:22:08,102 --> 00:22:15,940
Rym: And then the rest of the ecosystem is usually forced to basically follow Apple's rules because people get used to like that's old flash.

356
00:22:16,184 --> 00:22:17,160
Scott: That's how they killed flash.

357
00:22:17,280 --> 00:22:26,669
Rym: I think that's a big part of why so many people perceive Apple products to be more pleasant to use, because there's usually at least one design pattern where Apple has said, nope, that sucks.

358
00:22:26,730 --> 00:22:27,579
Rym: We're just going to ban it.

359
00:22:28,381 --> 00:22:34,262
Scott: I mean, listen, you know, if I wasn't somebody who played games like, you know, with Steam.

360
00:22:34,583 --> 00:22:34,824
Scott: Right.

361
00:22:35,025 --> 00:22:38,640
Scott: And be developed software that was like Linux software.

362
00:22:39,443 --> 00:22:39,686
Scott: Right.

363
00:22:40,012 --> 00:22:40,438
Scott: I'll get a Mac.

364
00:22:40,882 --> 00:22:41,063
Scott: Right.

365
00:22:41,124 --> 00:22:42,819
Scott: There's no reason for me to not have a Mac.

366
00:22:43,382 --> 00:22:43,727
Scott: Right.

367
00:22:43,808 --> 00:22:44,700
Scott: I've had an iPhone.

368
00:22:45,161 --> 00:22:48,380
Scott: There'll be no reason to not have a Mac as like those are the two things.

369
00:22:48,560 --> 00:22:50,359
Scott: And Mac will basically never do those things.

370
00:22:51,581 --> 00:22:57,320
Scott: So, yeah, playing games and editing like extreme video with the real Mac can do that.

371
00:22:57,340 --> 00:22:58,820
Scott: You can run all the Adobe software on there.

372
00:22:58,900 --> 00:22:59,323
Scott: No problem.

373
00:22:59,343 --> 00:22:59,605
Rym: Oh, yeah.

374
00:22:59,625 --> 00:23:01,700
Rym: What you run into is you can't get you can't get a good GPU.

375
00:23:03,803 --> 00:23:08,593
Scott: Well, I mean, if you're, you know, the GPU, if you get the Mac Studio, it's going to be for video editing.

376
00:23:08,634 --> 00:23:09,140
Scott: You'll be fine.

377
00:23:09,720 --> 00:23:09,821
Scott: Right.

378
00:23:10,948 --> 00:23:12,620
Rym: So it's in some other news.

379
00:23:12,941 --> 00:23:13,999
Rym: This is actually a big deal.

380
00:23:14,681 --> 00:23:15,264
Rym: New York.

381
00:23:15,766 --> 00:23:18,360
Rym: I think I think all this left is the governor has to sign this.

382
00:23:18,541 --> 00:23:25,570
Rym: And we have a pretty OK governor right now in considering the span of what it is better than the last governor.

383
00:23:25,590 --> 00:23:26,539
Scott: But that's not saying much.

384
00:23:26,940 --> 00:23:28,312
Rym: We're going to primary around.

385
00:23:28,332 --> 00:23:29,220
Scott: It could be a lot worse.

386
00:23:29,360 --> 00:23:30,219
Scott: It could be a lot better.

387
00:23:30,360 --> 00:23:30,761
Scott: Yeah.

388
00:23:31,082 --> 00:23:38,423
Rym: But we have our state legislature has passed a right to repair law in New York for electronics.

389
00:23:38,523 --> 00:23:42,340
Rym: And because New York is so big, this could have national ramifications.

390
00:23:43,600 --> 00:23:51,325
Rym: Basically, this law, unfortunately, it doesn't actually cover agricultural equipment or cars or most hardware.

391
00:23:51,425 --> 00:23:56,547
Rym: really only covers things like cell phones and like digital technology.

392
00:23:56,608 --> 00:23:57,759
Rym: The definition is a little bit weird.

393
00:23:58,481 --> 00:24:01,180
Scott: It's limited, but it's, you know, it's a start to nothing.

394
00:24:01,720 --> 00:24:07,320
Rym: Kind of like that week sauce gun bill that looks like it might actually pass the Senate because they got 10 Republicans on board.

395
00:24:07,861 --> 00:24:08,609
Rym: Yes, it's not.

396
00:24:08,670 --> 00:24:09,539
Scott: But it's not nothing.

397
00:24:09,861 --> 00:24:12,120
Rym: Yeah, not nothing is the best you can do sometimes.

398
00:24:12,980 --> 00:24:28,600
Rym: So the bill better than nothing requires digital electronics manufacturers to make repair instructions and parts available, not just to technicians who are independent, but also directly to consumers.

399
00:24:29,221 --> 00:24:29,361
Scott: Right.

400
00:24:29,381 --> 00:24:35,124
Scott: Well, I mean, Apple did make, you know, all this repair stuff available to everybody.

401
00:24:35,205 --> 00:24:37,559
Scott: There are some articles out there of people who did it.

402
00:24:38,061 --> 00:24:46,863
Scott: And the thing is, it's like, yeah, they'll send you all this ridiculous equipment to like fix your iPhone and you have to send it back within a limited time frame so you don't get charged for it.

403
00:24:47,225 --> 00:24:50,679
Scott: Thousands of dollars of expensive equipment and instructions like.

404
00:24:50,720 --> 00:24:51,808
Scott: you can do it yourself.

405
00:24:51,949 --> 00:24:53,078
Scott: You can succeed at it.

406
00:24:53,982 --> 00:24:57,020
Scott: But it's you know, it's still not for the faint of heart, right?

407
00:24:57,121 --> 00:24:57,705
Scott: It's really.

408
00:24:58,169 --> 00:24:58,955
Scott: it's like they do this.

409
00:24:59,800 --> 00:25:08,280
Scott: They would satisfy the law and that, yes, they'll send it to just any old person, but it's really designed for somebody who is a professional.

410
00:25:08,702 --> 00:25:11,260
Scott: You know, it's very much like they're running a repair shop.

411
00:25:11,400 --> 00:25:11,661
Scott: Right.

412
00:25:11,681 --> 00:25:19,240
Rym: Well, it's very much analogous to car repair and how you don't have to take your car to the dealership you brought it from to get it repaired.

413
00:25:19,562 --> 00:25:20,700
Rym: Go to any fucking mechanic.

414
00:25:21,022 --> 00:25:21,979
Rym: Some mechanics are good.

415
00:25:22,340 --> 00:25:23,219
Rym: Some mechanics are bad.

416
00:25:23,521 --> 00:25:24,639
Rym: Some mechanics are cheap.

417
00:25:25,043 --> 00:25:26,800
Scott: Some people can do it themselves.

418
00:25:27,540 --> 00:25:27,721
Rym: Exactly.

419
00:25:28,022 --> 00:25:31,580
Rym: But you can do that because of laws and regulations.

420
00:25:32,340 --> 00:25:39,419
Rym: And now we're applying the same principle, at least to phones and laptops and a lot of digital or digital adjacent goods.

421
00:25:40,241 --> 00:25:43,460
Scott: It's all my thing of the day from I think the last time we did shows, right?

422
00:25:43,580 --> 00:25:44,800
Scott: Stas takes fix on YouTube.

423
00:25:44,960 --> 00:25:46,240
Scott: I'm still loving that guy's channel.

424
00:25:46,460 --> 00:25:53,160
Scott: He's just, you know, this guy who admits he doesn't know much about electronics, but somehow he orders some broken shit on eBay.

425
00:25:53,563 --> 00:25:56,220
Scott: He fixes it and then he's real excited when it works.

426
00:25:56,461 --> 00:25:59,099
Scott: He's like, yeah, it's like it's really fun.

427
00:25:59,461 --> 00:26:02,460
Scott: It's like, but how did this get all about fixing stuff these days?

428
00:26:02,540 --> 00:26:03,739
Rym: Like how did this come to pass?

429
00:26:03,860 --> 00:26:10,620
Rym: Because with with any time a law passes anywhere in the US, the real question is, how did that good law make it through our government?

430
00:26:10,680 --> 00:26:11,584
Rym: Like how did that happen?

431
00:26:12,045 --> 00:26:17,020
Rym: Because if there's a pattern to that, you could make more good things happen theoretically.

432
00:26:17,120 --> 00:26:26,620
Rym: So one, it happened in an extremely blue state like this is a state that is completely controlled by the Democrats to it's only within one state.

433
00:26:26,740 --> 00:26:28,400
Rym: It just happens to be a very powerful state.

434
00:26:28,781 --> 00:26:30,267
Rym: See also how California.

435
00:26:30,749 --> 00:26:34,488
Rym: lot of companies follow California rules because they don't want to have trouble with.

436
00:26:34,508 --> 00:26:36,260
Rym: the California market is too big to ignore.

437
00:26:36,781 --> 00:26:40,832
Rym: And three, a number of companies.

438
00:26:40,872 --> 00:26:42,877
Scott: don't put gay people in their movies.

439
00:26:42,918 --> 00:26:44,148
Scott: that China market too big.

440
00:26:44,168 --> 00:26:44,579
Scott: Exactly.

441
00:26:46,281 --> 00:26:56,940
Rym: So there were a number of public groups, advocacy groups that were aggressively fighting for this in New York state for at least five years now.

442
00:26:57,020 --> 00:27:07,060
Rym: This is the culmination of years and years of constant letter writing, bothering random state senators like harassing government officials, donating money.

443
00:27:07,300 --> 00:27:15,320
Rym: This is this was a lot of work to get this bill passed from a lot of very, very dedicated activists in this very tiny space.

444
00:27:15,801 --> 00:27:25,820
Rym: So my advice to you is if you care about something about on this level, go harass like the state legislature in an important blue state.

445
00:27:26,060 --> 00:27:29,420
Rym: That is probably the best for a long period of time.

446
00:27:29,601 --> 00:27:30,960
Scott: Yeah, a lot of help, right?

447
00:27:31,221 --> 00:27:32,359
Scott: That's how you can make it happen.

448
00:27:32,560 --> 00:27:42,559
Rym: You've got to be annoying to the point to where a state government in the United States will would rather do what you want to make you go away than continue to listen to you.

449
00:27:45,162 --> 00:27:50,280
Rym: It's a strategy in some other news, that's a good segue into the main bit.

450
00:27:51,120 --> 00:27:58,460
Rym: We can't ignore Dolly because it is the new wordle in terms of completely filling my Twitter stream with.

451
00:27:58,760 --> 00:28:00,332
Scott: Yeah, it's this little A.I.

452
00:28:01,540 --> 00:28:10,120
Scott: ish right machine learning image generating model app that is available to the general public on some sort of throttled basis.

453
00:28:10,340 --> 00:28:14,522
Scott: You can't, you know, just use it because it takes quite a bit of computing power to make it go.

454
00:28:14,683 --> 00:28:14,924
Rym: Yep.

455
00:28:15,185 --> 00:28:18,943
Rym: And what we're playing with is the mini version, not the full on version, of course.

456
00:28:19,386 --> 00:28:21,380
Scott: And it's free to try to use, though.

457
00:28:21,400 --> 00:28:34,480
Scott: You just got to get through the throttling and yeah, you can type in any old text and it will search its database of images and it will try to combine those images into new images that match your description.

458
00:28:34,621 --> 00:28:40,860
Scott: So you could write like, you know, Rym jumping up and down and it will try to make pictures of Rym jumping up and down.

459
00:28:40,980 --> 00:28:41,201
Scott: Right.

460
00:28:41,241 --> 00:28:41,563
Scott: For what?

461
00:28:41,603 --> 00:28:52,920
Scott: However, it understands those words and you can have a lot of fun creating a lot of really interesting images by typing lots of pop culture combinations into this thing.

462
00:28:53,220 --> 00:28:57,702
Rym: I just got it to make Shrek riding a bicycle through Penn Station and it pretty much nailed it.

463
00:28:58,405 --> 00:29:04,720
Scott: Yeah, it's you know, you got to sort of learn you're in like what kind of things it does a good job with and what kinds of things it doesn't do.

464
00:29:04,940 --> 00:29:07,599
Rym: I found and if you read their website for the full A.I.

465
00:29:07,860 --> 00:29:13,322
Rym: model, it's interesting because it really focuses on natural language and detailed descriptions.

466
00:29:13,422 --> 00:29:20,304
Rym: So you'd like an example of a good search would be a storefront that has the word open A.I.

467
00:29:20,445 --> 00:29:31,800
Rym: written on it or an armchair in the shape of an avocado, the exact same cat on the top as a sketch on the bottom, like things like that.

468
00:29:32,121 --> 00:29:34,678
Rym: The more specific you are, the better it is.

469
00:29:35,221 --> 00:29:39,120
Rym: And it basically it seems like it's very similar.

470
00:29:39,340 --> 00:29:41,620
Rym: Actually, now that I'm scrolling down to the website, they even call it out.

471
00:29:41,680 --> 00:29:42,611
Rym: They say it's similar.

472
00:29:42,651 --> 00:29:43,339
Rym: So it's not just me.

473
00:29:44,020 --> 00:29:55,243
Rym: This is basically doing the same kind of thing that GPT-3 does, which is a natural language neural network where you can tell it things like write an article about this new cell phone.

474
00:29:55,304 --> 00:29:58,036
Rym: It'll write a kind of shitty article about this new cell phone.

475
00:29:58,076 --> 00:30:01,160
Rym: that's good enough, like it's enough to start with.

476
00:30:01,240 --> 00:30:02,256
Rym: It gives you a starting point.

477
00:30:04,360 --> 00:30:09,020
Scott: These it's a lot of fun and there's a lot of good memes people have generated with it.

478
00:30:09,501 --> 00:30:09,742
Rym: Yep.

479
00:30:10,244 --> 00:30:14,179
Rym: And we're going to edge into a little bit of people are talking about this.

480
00:30:16,321 --> 00:30:20,940
Rym: I've seen people having such ridiculous takes as this will destroy jobs.

481
00:30:21,720 --> 00:30:22,462
Rym: This A.I.

482
00:30:22,502 --> 00:30:25,612
Rym: will take away my source of like it's people are.

483
00:30:26,034 --> 00:30:28,906
Rym: no people are overreacting to the wrong.

484
00:30:29,128 --> 00:30:30,679
Scott: Those are tongue in cheek.

485
00:30:31,444 --> 00:30:32,412
Scott: This will take my job.

486
00:30:32,453 --> 00:30:33,240
Scott: I don't think it's serious.

487
00:30:33,560 --> 00:30:44,920
Rym: I've seen a few serious people, but it's more serious in the same way as the people who would complain that digital animation destroyed like my ability to make money painting cells.

488
00:30:47,321 --> 00:30:53,940
Rym: Anyway, anyway, things of the day.

489
00:30:54,161 --> 00:31:03,098
Rym: So I'm on a big Star Trek kick lately and my cup overflow it because there's a lot of new Star Trek, also a lot of old Star Trek that I never fully watched.

490
00:31:03,118 --> 00:31:09,280
Rym: that I'm digging through currently near the end of a rewatch slash first watch of Deep Space Nine.

491
00:31:09,861 --> 00:31:22,862
Rym: And my thing of the day is, if you're familiar in the original Star Trek or TOS, as nerds will call it, the original series, there was a parallel sort of sister show that was official.

492
00:31:22,962 --> 00:31:28,006
Rym: It's canonical, but it was animated, filmation animation, like really limited animation.

493
00:31:28,026 --> 00:31:29,539
Rym: It's a full on Star Trek.

494
00:31:30,202 --> 00:31:32,657
Rym: You've seen it out there, but I've seen it.

495
00:31:33,341 --> 00:31:34,380
Scott: But, you know, I don't a lot.

496
00:31:34,520 --> 00:31:36,760
Scott: I know that many people like watch the whole thing.

497
00:31:36,861 --> 00:31:38,579
Scott: I've definitely seen that it exists.

498
00:31:38,680 --> 00:31:46,560
Rym: But what's notable about it, other than the fact that it is fully canonical, is that it has a distinctive style.

499
00:31:47,400 --> 00:31:53,700
Rym: It is like it's hard to describe the style in full, but it uses a lot of like weird, almost Dutch angles.

500
00:31:54,302 --> 00:32:02,580
Rym: It has a lot of close ups of a face just reacting to something and not saying anything while music plays for like five or six seconds at a time.

501
00:32:03,001 --> 00:32:10,439
Rym: Clearly, it's a lot of the things they were doing to reduce how much animation they had to make in a Star Trek to crank this show out.

502
00:32:10,903 --> 00:32:12,600
Rym: But it has a very distinctive style.

503
00:32:13,081 --> 00:32:14,278
Scott: It was very super frenzy.

504
00:32:14,760 --> 00:32:16,039
Scott: Yeah, I felt so.

505
00:32:17,180 --> 00:32:30,700
Rym: This video is from some fans of this Grazelle Automations, and they have made a five and a half minute long video that is fully animated.

506
00:32:31,141 --> 00:32:38,480
Rym: It is a basically an episode of Star Trek Voyager condensed into an animated in this style.

507
00:32:39,460 --> 00:32:48,860
Rym: And what makes this sublime is they didn't just take the audio from this episode of Star Trek Voyager and just like animate it in the animation style.

508
00:32:49,421 --> 00:32:50,435
Rym: They reworked it.

509
00:32:51,161 --> 00:32:55,340
Rym: It has a soundtrack like the original animation instead of like Voyager.

510
00:32:55,843 --> 00:32:57,719
Rym: They added all the weird pregnant pauses.

511
00:32:58,222 --> 00:32:59,720
Rym: They moved the dialogue around.

512
00:33:00,501 --> 00:33:06,000
Rym: It mimics completely everything that made that original animated show distinctive.

513
00:33:06,462 --> 00:33:10,359
Rym: It's kind of amazing how like they hit it out of the park.

514
00:33:12,043 --> 00:33:13,780
Scott: I mean, I do appreciate that, right?

515
00:33:13,960 --> 00:33:18,338
Scott: I talk a lot about how like, hey, you know, why not make something new in the old style, right?

516
00:33:18,480 --> 00:33:24,559
Scott: Why not just make, you know, a noir movie with some black and white film as it, you know, as if it was nineteen.

517
00:33:25,100 --> 00:33:27,260
Rym: Scott, you ever see a movie called Brick?

518
00:33:29,061 --> 00:33:30,165
Scott: No, really, it's a.

519
00:33:30,266 --> 00:33:37,619
Rym: it's a straight up film noir and it's about a brick of cocaine, but it's set in like a dumpy high school, like earnestly.

520
00:33:38,120 --> 00:33:41,280
Rym: And it's this film is I can't believe you've never seen it.

521
00:33:41,360 --> 00:33:43,440
Scott: I'm sure there are people who do it, but it's not common.

522
00:33:43,760 --> 00:33:44,439
Rym: Oh, yeah, it's not common.

523
00:33:45,382 --> 00:33:50,600
Scott: Yeah, you know, old movies, they always open with like, you know, credits and like a classical music playing.

524
00:33:50,900 --> 00:33:55,658
Scott: And it's like, you know, it's like what if you went to the movie theater today and a movie started like that?

525
00:33:55,960 --> 00:33:56,243
Scott: It's like.

526
00:33:56,729 --> 00:33:57,357
Scott: no one does that.

527
00:33:58,340 --> 00:33:58,978
Rym: Yeah, it is rare.

528
00:33:59,380 --> 00:34:07,900
Rym: Case in point, Hateful Eight, like that movie, that's what it was like to go to an old timey four and a half hour long movie with an intermission a little bit.

529
00:34:09,167 --> 00:34:10,199
Scott: Anyway, so the thing of the day.

530
00:34:10,400 --> 00:34:12,020
Scott: So here's his book that's coming out.

531
00:34:12,199 --> 00:34:17,340
Scott: The book is not actually coming out until September, September this year.

532
00:34:17,620 --> 00:34:17,860
Scott: Right.

533
00:34:18,001 --> 00:34:33,940
Scott: But there is currently a free chapter one preview and you can get an early access e-book for thirty two dollars and you can preorder the print book and that will also include the early access e-book for forty dollars.

534
00:34:34,260 --> 00:34:39,560
Scott: And the book is actually you might think, ah, it's an electronics book because it's called Open Circuits.

535
00:34:39,641 --> 00:34:41,980
Scott: But actually this is a photography book.

536
00:34:42,181 --> 00:34:42,442
Scott: Right.

537
00:34:43,103 --> 00:34:47,280
Scott: It's a. it's a book where basically they take electronic components.

538
00:34:47,842 --> 00:34:48,063
Scott: Right.

539
00:34:48,105 --> 00:34:49,940
Scott: The free chapter is passive components.

540
00:34:50,081 --> 00:34:54,500
Scott: But then the other chapters, you have semiconductors, electromechanics, cables and connectors, et cetera.

541
00:34:54,620 --> 00:34:54,821
Scott: Right.

542
00:34:55,643 --> 00:35:08,780
Scott: And I each chapter is basically photographs of not just the components because like, you know, what a resistor looks like, but mostly cross sections of the components.

543
00:35:09,281 --> 00:35:09,482
Scott: Right.

544
00:35:09,562 --> 00:35:18,000
Scott: It's like you might know that like, yeah, a capacitor is like, you know, you like you know, in the theory and principle of what's in there.

545
00:35:18,121 --> 00:35:18,343
Scott: Right.

546
00:35:18,383 --> 00:35:20,120
Scott: There's maybe some oil in there or something.

547
00:35:20,421 --> 00:35:20,703
Scott: Right.

548
00:35:21,125 --> 00:35:24,300
Scott: But like if if I said, all right.

549
00:35:24,340 --> 00:35:27,480
Scott: I'm about to cut open a capacitor right in half and show you the inside.

550
00:35:27,602 --> 00:35:28,233
Scott: It's discharge.

551
00:35:28,274 --> 00:35:28,640
Scott: Don't worry.

552
00:35:29,140 --> 00:35:29,302
Scott: Right.

553
00:35:29,524 --> 00:35:31,078
Scott: Would you know what you're about to see?

554
00:35:32,220 --> 00:35:32,724
Rym: I would.

555
00:35:33,813 --> 00:35:34,680
Rym: But most people wouldn't.

556
00:35:34,820 --> 00:35:37,552
Rym: And I wouldn't for most electronic components, just I happen to.

557
00:35:38,074 --> 00:35:41,399
Rym: I used to desolder and resolder capacitors as a job.

558
00:35:41,682 --> 00:35:43,280
Scott: But did you cut a capacitor in half?

559
00:35:43,901 --> 00:35:53,319
Rym: Yes, actually, because I had to prove that capacitors I was taking out of computer motherboards were defective because of the electrolyte solution inside of them.

560
00:35:54,381 --> 00:35:59,500
Scott: Anyway, so the point is doing that at IBM, it's a book of photography and it's all.

561
00:35:59,520 --> 00:36:04,240
Scott: the photographs are basically the insides and cross sections of electronic components.

562
00:36:04,400 --> 00:36:06,360
Scott: You can see what the hell is actually in their school.

563
00:36:06,661 --> 00:36:08,740
Scott: This is pretty fascinating.

564
00:36:09,322 --> 00:36:12,600
Scott: I recommend you download the free first chapter.

565
00:36:12,841 --> 00:36:19,640
Scott: And if you like what you see, well, there's the beef, not free options that will, you know, you can preorder for September.

566
00:36:19,801 --> 00:36:22,676
Scott: Oh, and look, it says use coupon code preorder to get 25 percent off.

567
00:36:23,885 --> 00:36:24,880
Scott: I never understood that.

568
00:36:25,020 --> 00:36:26,860
Scott: It's like I'm on the purchasing page.

569
00:36:27,301 --> 00:36:29,400
Scott: Why are you writing the coupon code right there?

570
00:36:29,520 --> 00:36:32,036
Scott: Why not just lower the price by 25 percent?

571
00:36:32,980 --> 00:36:36,180
Rym: I can explain that with modern pricing theory.

572
00:36:36,900 --> 00:36:37,544
Rym: There is a very.

573
00:36:37,624 --> 00:36:40,724
Rym: I talked to us in GeekNights a while ago, but this is a direct example of it.

574
00:36:41,088 --> 00:36:42,220
Rym: This is a simple example.

575
00:36:42,341 --> 00:36:46,080
Rym: But the idea is it's the same reason coupons exist in the modern world.

576
00:36:46,780 --> 00:36:47,164
Rym: You do that.

577
00:36:47,224 --> 00:36:48,960
Scott: Coupon is right on the page.

578
00:36:49,140 --> 00:36:50,286
Rym: Here's what you're up.

579
00:36:50,306 --> 00:36:53,020
Rym: So despite it being right on the page.

580
00:36:53,481 --> 00:37:09,880
Rym: The data on how many people copy paste it right there when they buy it versus just don't bother is extremely useful data, because terrifyingly, that is how you generate lists of people who care more about time than money or more about money than time.

581
00:37:10,060 --> 00:37:12,940
Scott: People who know how to read generate a list of people.

582
00:37:13,160 --> 00:37:15,080
Rym: Not all I'm going to I'll put it diplomatically.

583
00:37:15,680 --> 00:37:29,915
Rym: You can infer a lot about the future behavior of someone on an e-commerce site based on whether or not they will copy paste the word preorder and all caps from one part of a page into another part of a page to get 25 percent off.

584
00:37:30,920 --> 00:37:33,199
Rym: You can also infer a lot about someone who will not do that.

585
00:37:33,820 --> 00:37:34,726
Rym: I'll just leave it at that.

586
00:37:35,088 --> 00:37:37,080
Rym: That is something they teach product managers.

587
00:37:38,921 --> 00:37:44,566
Scott: Anyway, you took this one pricing like like training like one time and like you just keep going on and on.

588
00:37:44,748 --> 00:37:46,120
Rym: I've taken away more than one training.

589
00:37:46,300 --> 00:37:48,559
Rym: I've been this is something I've been studying for the last five years.

590
00:37:48,701 --> 00:37:50,077
Scott: But anyway, uh huh.

591
00:37:50,580 --> 00:37:50,661
Scott: Yeah.

592
00:37:50,721 --> 00:37:50,984
Scott: OK.

593
00:37:51,004 --> 00:37:52,398
Scott: You went to a training five years ago.

594
00:37:55,201 --> 00:38:01,959
Rym: But anyway, in the bad moment, the Geek Guys Book Club book, Nine Fox Gambit, the next Thursday episode, we are doing the show.

595
00:38:02,402 --> 00:38:05,580
Rym: If you start reading it now, you can easily finish by next Thursday.

596
00:38:05,640 --> 00:38:08,640
Rym: The book is not long, nor is it hard to read.

597
00:38:09,441 --> 00:38:19,199
Rym: And if you bounce off of the first two paragraphs, that's a bunch of proper nouns and cylindrical heresy, you'll you'll figure out what's going on real quick.

598
00:38:21,682 --> 00:38:22,365
Rym: And I'll leave it at that.

599
00:38:22,525 --> 00:38:26,360
Rym: I already started the second book because that's how I roll.

600
00:38:27,180 --> 00:38:28,239
Scott: I really met a moment.

601
00:38:28,441 --> 00:38:30,080
Scott: Also, I guess I'm working on the website.

602
00:38:31,022 --> 00:38:32,440
Scott: It's coming along very nicely.

603
00:38:32,600 --> 00:38:34,087
Scott: I'm doing I'm basically right now.

604
00:38:34,127 --> 00:38:48,240
Scott: I'm working on a thing for uploading new episodes, which is going to work a lot like the way, you know, you upload videos on YouTube, obviously less fancy, but, you know, drag an MP3 in, click, upload, type in a bunch of metadata, push, save that whole that whole thing.

605
00:38:48,582 --> 00:38:48,663
Rym: Yeah.

606
00:38:48,683 --> 00:38:48,865
Rym: Right.

607
00:38:49,108 --> 00:38:50,120
Rym: I also I got a little bit.

608
00:38:50,482 --> 00:38:52,080
Rym: So I started this up again.

609
00:38:52,140 --> 00:38:56,403
Rym: I had been doing it for a while because we hadn't really been reviewing a lot of tabletop games.

610
00:38:56,905 --> 00:39:05,143
Rym: I guess something happened in the world over the last three years that prevented us from going to a bunch of conventions and playing a ton of tabletop games that we normally would.

611
00:39:05,184 --> 00:39:06,919
Rym: that fuels a good chunk of GeekNights.

612
00:39:07,462 --> 00:39:15,180
Rym: But it turns out, since we stopped playing a lot of board games, we had done five more episodes where we reviewed a tabletop game.

613
00:39:15,741 --> 00:39:18,800
Rym: So I have now cut them out and put them on YouTube.

614
00:39:19,040 --> 00:39:20,499
Rym: Just the main bits, just the reviews.

615
00:39:22,102 --> 00:39:34,044
Rym: And I'm going to link to, if you don't know, there's a playlist out there that has 78 of our GeekNights tabletop game reviews with no news, no main bits, none of that other GeekNights crap.

616
00:39:34,104 --> 00:39:36,519
Rym: It's just the review and nothing else.

617
00:39:37,402 --> 00:39:40,419
Scott: What I think is really interesting and probably a mistake that we made.

618
00:39:41,282 --> 00:39:41,664
Scott: Right.

619
00:39:42,226 --> 00:39:45,381
Scott: You know, given that we don't really care about marketing, putting the main bit at the end.

620
00:39:45,602 --> 00:39:51,068
Scott: And well, no, is that if you look around and say like the tabletop gaming community.

621
00:39:51,088 --> 00:39:51,291
Scott: Right.

622
00:39:52,220 --> 00:39:53,130
Scott: Nobody can sit.

623
00:39:53,211 --> 00:39:54,019
Scott: Nobody like knows.

624
00:39:54,381 --> 00:39:56,320
Scott: But you never see them saying like, oh, best you see.

625
00:39:56,400 --> 00:39:57,809
Scott: I'll see occasionally threads like best.

626
00:39:57,849 --> 00:39:59,660
Scott: tabletop podcasts were never mentioned ever.

627
00:39:59,861 --> 00:40:00,081
Scott: Right.

628
00:40:00,623 --> 00:40:04,980
Scott: But 78 episodes is more episodes than most tabletop podcasts in total.

629
00:40:05,201 --> 00:40:05,481
Scott: Right.

630
00:40:06,103 --> 00:40:14,739
Rym: So it's only like not even 10 percent of we have more Wednesday episodes than most anime podcasts that have ever existed.

631
00:40:16,105 --> 00:40:16,959
Scott: Yeah, pretty much.

632
00:40:17,180 --> 00:40:17,381
Scott: Yeah.

633
00:40:17,843 --> 00:40:37,060
Rym: But anyway, I think I've been doing this for the tabletop content because I find that even more so than anything else we do, a succinct, deep review of a tabletop game is the kind of thing that someone will send to a friend to try to convince them to play a And they're useful to travel around independent of the rest of the GeekNights' crap.

634
00:40:37,700 --> 00:40:43,300
Rym: You don't need to hear about us talking about biking to work for 20 minutes just to find out if Princess of Florence is worth playing.

635
00:40:43,941 --> 00:40:44,799
Rym: So this is out there.

636
00:40:45,220 --> 00:40:47,960
Rym: And before you ask, why aren't you doing this for the anime reviews?

637
00:40:48,421 --> 00:40:48,902
Rym: I was.

638
00:40:49,303 --> 00:40:57,400
Rym: I stopped because even a fucking anime review that had no screenshots, no content of any kind from the anime.

639
00:40:57,782 --> 00:41:00,239
Rym: It's just the audio of me and Scott talking about it.

640
00:41:00,581 --> 00:41:03,020
Rym: Got a copyright takedown on YouTube and I was just fucking done.

641
00:41:03,782 --> 00:41:04,899
Rym: Anime is dead to me on YouTube.

642
00:41:06,161 --> 00:41:07,971
Scott: OK, well, because they just look they're not.

643
00:41:07,991 --> 00:41:11,700
Scott: they're just indiscriminately, you know, they're not actually checking for a copyright violation.

644
00:41:11,900 --> 00:41:16,240
Scott: They just see the name of an anime in the title or description of the thing.

645
00:41:16,742 --> 00:41:18,720
Scott: And then they just like, you know, they just send you a thing.

646
00:41:18,880 --> 00:41:19,607
Scott: They just check.

647
00:41:19,687 --> 00:41:21,000
Scott: It's amazing in your video.

648
00:41:21,260 --> 00:41:27,100
Rym: My YouTube channel only gets this for anime content and literally nothing else.

649
00:41:27,280 --> 00:41:27,381
Scott: Right.

650
00:41:27,401 --> 00:41:29,680
Scott: Well, that's because the anime we all know this.

651
00:41:29,700 --> 00:41:31,300
Scott: How many times have we talked about this?

652
00:41:31,600 --> 00:41:35,060
Scott: Japan is really wild with their copyright enforcement.

653
00:41:35,681 --> 00:41:41,420
Scott: You know, like people in Japan are like going to actual criminal prison for like pirating some mangas or whatever.

654
00:41:41,561 --> 00:41:41,762
Scott: Right.

655
00:41:41,802 --> 00:41:45,880
Scott: They are super strict about it, you know, and obsessed with enforcing it.

656
00:41:45,940 --> 00:41:48,480
Scott: Look at the way Nintendo, you know, what they did to the Bowser.

657
00:41:48,983 --> 00:41:49,350
Scott: Oh, yeah.

658
00:41:49,513 --> 00:41:49,798
Scott: Not this.

659
00:41:50,160 --> 00:41:59,720
Scott: Not the not the look what Nintendo did, not the fictional character and not the president of Nintendo of America, but the pirate guy, Bowser.

660
00:42:00,860 --> 00:42:06,209
Rym: Well, we'll talk about that on the next Tuesday show, because there's that situation is kind of fucked up.

661
00:42:06,331 --> 00:42:06,820
Rym: But anyway.

662
00:42:07,240 --> 00:42:07,441
Scott: Yeah.

663
00:42:07,924 --> 00:42:09,171
Scott: Anyway, so A.I.

664
00:42:10,720 --> 00:42:21,880
Rym: has been in the news a bunch, not just because of, well, just the the industry moving fast and not just because of Dali, but because something a little bit.

665
00:42:22,962 --> 00:42:29,072
Rym: I don't want to say silly, but the story we're about to talk about, there's a lot of facets to it, and we're not going to be able to cover,

666
00:42:29,092 --> 00:42:36,260
Scott: you know, and there's so much we don't know, we can only talk about what we do know as, you know, people who read a couple articles and some blog posts.

667
00:42:36,540 --> 00:42:38,317
Rym: But I'm going to head off before we get into it.

668
00:42:39,781 --> 00:42:39,921
Rym: The

669
00:42:40,062 --> 00:42:40,443
Rym: A.I.,

670
00:42:40,503 --> 00:42:47,777
Rym: the chat bot that is at the center of this cluster of stories is is absolutely not sentience.

671
00:42:48,283 --> 00:42:49,640
Rym: I want to be clear about that.

672
00:42:49,801 --> 00:42:51,300
Scott: I mean, what is sentient?

673
00:42:51,300 --> 00:42:52,760
Scott: You want to have a philosophy episode?

674
00:42:53,164 --> 00:42:53,880
Rym: Oh, we could.

675
00:42:54,361 --> 00:42:55,665
Rym: I am glad you.

676
00:42:56,227 --> 00:43:02,800
Rym: if there's one argument that I am well prepared to have and go on forever about is something appears to be sentient.

677
00:43:03,101 --> 00:43:04,080
Scott: Is it not sentient?

678
00:43:04,320 --> 00:43:05,337
Scott: If no one can tell.

679
00:43:05,840 --> 00:43:06,222
Rym: Yep.

680
00:43:06,402 --> 00:43:12,600
Rym: But there is a lot of hard evidence that this is not sentient in any meaningful way.

681
00:43:14,700 --> 00:43:16,979
Rym: Basically, I'm going to super summarize this.

682
00:43:17,621 --> 00:43:30,120
Rym: An engineer who works at Google believed and then claimed publicly that a Lambda chat bot is sentient and deserves sentient rights.

683
00:43:31,440 --> 00:43:31,560
Scott: Right.

684
00:43:32,042 --> 00:43:37,700
Scott: So my opinion on its sentience, because I can actually say why it's not sentient in my opinion.

685
00:43:38,220 --> 00:43:38,320
Scott: Right.

686
00:43:38,361 --> 00:43:49,130
Rym: I have a few articles queued up that also say in a hard science reason why even there's a there's a real simple reason that I alone, according to what I think, counts as sentient.

687
00:43:49,171 --> 00:43:49,314
Scott: Right.

688
00:43:49,720 --> 00:43:51,185
Scott: Do not consider it to be sentient.

689
00:43:52,689 --> 00:43:58,999
Scott: is that this chat bot, the way it works, is you send some text to it and then it sends some text back.

690
00:43:59,381 --> 00:43:59,643
Scott: Right.

691
00:44:00,286 --> 00:44:03,240
Scott: It never just sends some text on its own.

692
00:44:03,562 --> 00:44:05,660
Scott: It doesn't just say things out of nowhere.

693
00:44:06,043 --> 00:44:06,225
Scott: Right.

694
00:44:06,306 --> 00:44:07,780
Scott: It's just responding to an input.

695
00:44:08,062 --> 00:44:08,303
Scott: Right.

696
00:44:09,371 --> 00:44:10,540
Scott: It won't just say something.

697
00:44:11,182 --> 00:44:18,680
Scott: And you might think, well, you could easily just sort of have some sort of clock hooked up to it so that it would say things on its own.

698
00:44:19,120 --> 00:44:19,220
Scott: Right.

699
00:44:19,240 --> 00:44:31,859
Rym: And there are humans who have particular kinds of executive dysfunctions where they are a human and no one's going to argue that a human is not sentient, but they will not take any action on their own.

700
00:44:32,261 --> 00:44:40,160
Rym: But if you stimulate their brain in a specific way, they will spontaneously begin taking actions like a normal seeming person.

701
00:44:40,660 --> 00:44:40,861
Scott: Sure.

702
00:44:41,223 --> 00:44:44,640
Scott: But regardless, even if you did that to this bot.

703
00:44:44,961 --> 00:44:45,222
Scott: Right.

704
00:44:45,323 --> 00:44:46,510
Scott: It's like, well, what are you going to?

705
00:44:46,651 --> 00:44:47,416
Scott: you still have to like.

706
00:44:47,496 --> 00:44:53,176
Scott: you can't just be like, hey, there's no way to be like, hey, say something you have to actually say something at it.

707
00:44:53,258 --> 00:44:53,380
Scott: Right.

708
00:44:53,440 --> 00:44:53,641
Scott: It can.

709
00:44:53,742 --> 00:44:53,862
Scott: it's.

710
00:44:53,983 --> 00:44:57,220
Scott: all it's doing is processing text.

711
00:44:57,360 --> 00:44:57,702
Scott: You give it.

712
00:44:57,782 --> 00:45:00,820
Scott: if you don't give it any text, then it doesn't do anything.

713
00:45:00,901 --> 00:45:01,999
Scott: It's just a text process.

714
00:45:02,461 --> 00:45:17,022
Scott: I go even I go even though even though its responses to text may very well imitate thought, right, or give the illusion of thought, there is no thought on its own to happen there.

715
00:45:17,103 --> 00:45:27,519
Rym: Well, case in point, imagine, imagine a Turing test, you know, in a chat room, the typical could chat with something and then decide if it is sentient or not or decide if it is human or not.

716
00:45:28,281 --> 00:45:46,667
Rym: If you train a model with humans administering Turing tests and responding to Turing tests, you will get very human sounding responses, because a lot of humans will respond to the kinds of questions humans ask bots to see if they're human the way a human would like.

717
00:45:46,789 --> 00:45:47,719
Rym: It's a self.

718
00:45:48,682 --> 00:45:50,719
Rym: It's basically a self reinforcing model.

719
00:45:51,483 --> 00:45:53,780
Scott: Well, no, but I think that, you know, let's say you did a Turing test.

720
00:45:53,861 --> 00:45:54,305
Scott: Right.

721
00:45:54,346 --> 00:45:55,740
Scott: The oldest test in the book.

722
00:45:55,921 --> 00:45:56,141
Scott: Right.

723
00:45:56,583 --> 00:46:04,378
Scott: What if you did a Turing test against this bot, no matter what text it sent back to you, no matter how convincing that text was, it would always be.

724
00:46:05,102 --> 00:46:09,080
Scott: I send a sentence that sends back a sentence one, one, one, one.

725
00:46:09,200 --> 00:46:15,003
Scott: It would never send back like two or three or send me back one when I didn't send it anything.

726
00:46:15,164 --> 00:46:15,466
Scott: There are.

727
00:46:15,506 --> 00:46:17,700
Rym: there are some chat bots that can do things like that, though.

728
00:46:18,260 --> 00:46:18,421
Scott: Sure.

729
00:46:18,481 --> 00:46:28,840
Scott: But if I was conversing with a human, even if the human was very illiterate and bad at writing, there wouldn't be the sort of one one one one pattern going on.

730
00:46:28,960 --> 00:46:38,020
Scott: And therefore that would be a strong indicator that it would be human compared to the one one one one where that even if the text was vastly superior.

731
00:46:38,560 --> 00:46:38,781
Scott: Right.

732
00:46:39,242 --> 00:46:44,378
Scott: So anyway, so there's this guy, this guy will just I don't.

733
00:46:44,418 --> 00:46:45,020
Scott: there's too much.

734
00:46:45,200 --> 00:46:46,979
Scott: I never heard of this person before this week.

735
00:46:47,140 --> 00:46:47,321
Scott: Yeah.

736
00:46:47,562 --> 00:46:53,299
Scott: And the person the research I did on the Internet indicates this is a very interesting person.

737
00:46:53,380 --> 00:46:56,600
Scott: Have it a lot complicated with a complicated history.

738
00:46:57,344 --> 00:46:58,215
Scott: And you know what?

739
00:46:59,583 --> 00:47:01,340
Scott: You know, it's not all positive.

740
00:47:01,782 --> 00:47:05,680
Scott: It's not all negative, but congratulations on having a really interesting life.

741
00:47:06,201 --> 00:47:06,872
Scott: This person.

742
00:47:06,892 --> 00:47:07,400
Scott: Right.

743
00:47:07,680 --> 00:47:08,303
Scott: What was their name?

744
00:47:08,604 --> 00:47:12,060
Rym: Oh, their name is Blake Lemoine.

745
00:47:13,340 --> 00:47:25,780
Scott: OK, so, yeah, very interesting individual who at least up until very recently was an employee at Google and was doing some sort of AI ethics type research.

746
00:47:26,321 --> 00:47:27,004
Scott: Right.

747
00:47:27,024 --> 00:47:39,020
Scott: And the it is well known that Google has gotten rid of many of its ethics people, which is not great considering the AI stuff they're doing, even whether you want to argue it's actually AI or not.

748
00:47:39,300 --> 00:47:41,700
Scott: You know, the point is machine learning type stuff.

749
00:47:41,860 --> 00:47:47,647
Scott: There's still a lot of ethical issues that you need somebody to, you know, keep a watchful eye on.

750
00:47:47,668 --> 00:47:49,060
Scott: That's not just Google.

751
00:47:49,400 --> 00:47:49,645
Scott: Yeah.

752
00:47:50,116 --> 00:47:50,320
Scott: Right.

753
00:47:50,920 --> 00:47:53,560
Rym: We'll talk about what those ethical issues actually are in a moment.

754
00:47:54,561 --> 00:47:54,781
Scott: Sure.

755
00:47:54,801 --> 00:48:05,940
Scott: So, yeah, this person was working there and they believe that this particular chat bot that I just said is not sentient, they believe otherwise.

756
00:48:06,923 --> 00:48:10,060
Scott: And they went through all kinds of processes and H.R.

757
00:48:10,260 --> 00:48:13,520
Scott: stuff and, you know, raising issues and getting help.

758
00:48:13,720 --> 00:48:17,440
Rym: Looks like they tried to get a lawyer even to represent the AI.

759
00:48:18,322 --> 00:48:18,522
Scott: Right.

760
00:48:18,542 --> 00:48:31,460
Scott: They're doing all kinds of stuff that they documented and also their newspapers reporting on it that they talk to and all kinds of stuff that, you know, we really don't know anything about other than what's been published to report it to Google.

761
00:48:32,222 --> 00:48:38,319
Scott: And the end result was that Google put them on administrative leave, which means we're going to you're weird and we're going to fire you soon.

762
00:48:38,420 --> 00:48:38,685
Scott: Yeah.

763
00:48:38,725 --> 00:48:38,888
Scott: Right.

764
00:48:38,908 --> 00:48:39,579
Scott: That's what it means.

765
00:48:40,161 --> 00:48:53,260
Scott: And you based on the available information and the checkered interesting history of the individual, it's kind of hard to, you know, not read it like both ways.

766
00:48:53,420 --> 00:48:53,641
Scott: Right.

767
00:48:53,701 --> 00:48:58,900
Scott: It's like, well, this person is just, you know, losing it on the one hand.

768
00:48:59,281 --> 00:49:11,265
Scott: But B, you know, you could ease easily also believe that, yeah, Google's up to no good, even while you don't necessarily have to agree that the chat bot is sent out to believe that Google's up to no good.

769
00:49:11,305 --> 00:49:18,259
Rym: But I guess the situation here, like I'm going to actually quote the way article because it because this pretty much sums it up.

770
00:49:19,080 --> 00:49:23,999
Rym: The reporting around this was this engineer thinks it's sentient and Google denies it.

771
00:49:24,440 --> 00:49:41,202
Rym: But to quote WAPO, most academics and practitioners say the words and images generated by artificial intelligence systems such as Lambda produce responses based on what humans have already posted on Wikipedia, Reddit, message boards and every other corner of the Internet.

772
00:49:41,564 --> 00:49:44,600
Rym: And that doesn't signify that the model understands meaning.

773
00:49:45,001 --> 00:49:52,559
Rym: Quote, we now have machines that can mindlessly generate words, but we haven't learned how to stop imagining a mind behind them.

774
00:49:54,622 --> 00:49:59,764
Rym: And it goes on, there's a lot of discussion around this, but basically this person seems to have now gone to the press.

775
00:50:00,146 --> 00:50:11,300
Rym: There's a lot of public conversation around this and the public conversation is getting further and further away from the actual technology involved here and the reality.

776
00:50:11,702 --> 00:50:16,463
Scott: And, you know, because there's so many blanks that are not filled in due to things that have not.

777
00:50:16,543 --> 00:50:34,660
Scott: It's interesting because enough has been reported and enough has been blogged by the person in question that there's a lot of information available relative to what you would normally have in these sort of scenarios where someone's like, let go more than there were in the you know, the previous researchers who got let go from Google.

778
00:50:34,780 --> 00:50:41,825
Scott: Yeah, but still plenty of unknowns and plenty of people talking who don't understand technology, like Rym's said.

779
00:50:42,206 --> 00:50:48,951
Scott: So that empty space of knowledge gets filled with conspiracies, conjecture, rumors, nonsense,

780
00:50:49,011 --> 00:50:49,514
Scott: B.S.,

781
00:50:49,554 --> 00:50:51,778
Scott: et cetera, et cetera, that you all got to watch out for all that.

782
00:50:51,960 --> 00:50:57,579
Rym: So first, let's talk about these like linguistic models and sort of how they work or what they mean, because basically what they do.

783
00:50:58,001 --> 00:51:02,519
Rym: And again, we're super, super generalizing for a lot of reasons.

784
00:51:03,901 --> 00:51:18,087
Rym: You feed in a ton of human language and then they will make predictions based on trends and relationships and statistics from that set of language so that if there is a set of text, generate more text.

785
00:51:18,127 --> 00:51:23,240
Scott: that's like the text that I was given or related to the text or, you know, whatever.

786
00:51:23,620 --> 00:51:25,006
Rym: Now, what these what these are

787
00:51:25,247 --> 00:51:25,528
Scott: often

788
00:51:25,850 --> 00:51:38,187
Rym: actually used for in the real world are things like when you try to get customer service and a human doesn't respond, a chat bot responds and it tries to get information out of you and get it get help you as much as possible before

789
00:51:38,429 --> 00:51:39,480
Scott: those aren't even using that.

790
00:51:39,500 --> 00:51:40,859
Scott: A lot of those just straight up scripted.

791
00:51:40,980 --> 00:51:47,548
Rym: Yeah, they're scripted or they have extremely simplistic models, usually just looking for keywords.

792
00:51:48,070 --> 00:51:50,380
Rym: Beyond that, they're often used.

793
00:51:51,101 --> 00:52:02,800
Rym: There's a burgeoning industry of using them to prewrite simplistic articles, news articles, reviews, anything like low hanging fruit, press release nonsense.

794
00:52:04,521 --> 00:52:17,560
Rym: Basically, you feed in the all the all the text around a situation into these models and they'll tell it what to write about and they'll spit out an article that clearly is unreadable, but it gets the broad strokes down.

795
00:52:17,600 --> 00:52:20,540
Rym: It almost like it writes a crappy review of a phone.

796
00:52:20,962 --> 00:52:30,078
Rym: And then you hire a person who's maybe way less skilled than someone who would write an entire article with research from nothing to clean it up and make it look like a human wrote it and publish it.

797
00:52:30,563 --> 00:52:33,120
Rym: And that is a worrying and growing trend in journalism.

798
00:52:33,564 --> 00:52:34,919
Rym: But that's a use for these things.

799
00:52:35,541 --> 00:52:40,562
Scott: Yeah, I mean, the number one use is everybody knows is the series, Alexa's and OK, Google's.

800
00:52:40,642 --> 00:52:40,863
Scott: Right.

801
00:52:40,944 --> 00:52:44,240
Scott: Those are the that's the number one use consumers are aware of.

802
00:52:44,380 --> 00:52:44,581
Scott: Right.

803
00:52:44,621 --> 00:52:45,584
Scott: For all this nonsense,

804
00:52:46,968 --> 00:53:07,040
Rym: a dangerous use that is coming more and more to the forefront, but already has existed in a practicable form for a long time with text is having something that is able to write novel content in the recognizable style of a known person, but without any input from said person.

805
00:53:07,581 --> 00:53:09,668
Rym: You could get this to write an article.

806
00:53:10,090 --> 00:53:13,120
Rym: that sounds a lot like how I write an article.

807
00:53:13,700 --> 00:53:17,099
Rym: Look, if you read it, you think, wow, Rimm wrote that, but I didn't write it.

808
00:53:18,942 --> 00:53:19,103
Scott: Mm hmm.

809
00:53:19,605 --> 00:53:25,880
Scott: If you just if you just take all the Stephen King books, throw them into this thing and then say, write a Stephen King book and you'll be surprised.

810
00:53:26,600 --> 00:53:30,160
Rym: Yeah, it comes out, especially if you look at some later Stephen King books.

811
00:53:30,460 --> 00:53:31,680
Rym: But I digress.

812
00:53:33,143 --> 00:53:34,840
Scott: Maybe those were written by now.

813
00:53:34,960 --> 00:53:49,908
Rym: The other ethical side of this coin is I absolutely believe we will build machine intelligences that are equivalent to superior that to or at least analogous to human intelligence.

814
00:53:49,928 --> 00:53:50,760
Rym: Like we're not special.

815
00:53:50,820 --> 00:53:55,720
Rym: We're just a biological machine, but I don't think we're anywhere near that point so far.

816
00:53:55,881 --> 00:53:57,160
Scott: Not in our lifetime, probably.

817
00:53:57,862 --> 00:54:00,540
Rym: And of course, at least not as we approach that point.

818
00:54:01,261 --> 00:54:01,702
Rym: There are.

819
00:54:01,743 --> 00:54:09,243
Rym: there is a huge body of ethics around what is ethical to do to and with such a intelligence.

820
00:54:09,543 --> 00:54:14,560
Rym: But we are so, so impossibly far from that technology.

821
00:54:15,220 --> 00:54:15,320
Scott: Right.

822
00:54:15,340 --> 00:54:22,158
Scott: Then what we have now is basically just a giant spreadsheet and then fancy spreadsheet lookups.

823
00:54:22,500 --> 00:54:22,600
Scott: Right.

824
00:54:22,661 --> 00:54:28,919
Scott: It's like there's no moral or ethical problem with turning off the computer or deleting the spreadsheet or anything like that.

825
00:54:29,120 --> 00:54:29,260
Scott: Right.

826
00:54:29,782 --> 00:54:36,980
Scott: But there are lots and lots and lots of ethical and moral issues regards to using the giant spreadsheet.

827
00:54:37,502 --> 00:54:37,804
Scott: Right.

828
00:54:37,824 --> 00:54:42,591
Scott: You know, you've seen plenty of people talking about how like image recognition is racist.

829
00:54:42,632 --> 00:54:43,219
Scott: That's true.

830
00:54:43,940 --> 00:54:47,440
Scott: You know, automatic like photo editing stuff racist.

831
00:54:47,700 --> 00:54:55,560
Rym: Also true, because the data encodes the implicit biases and dimensionality of our existing flawed society.

832
00:54:55,821 --> 00:54:56,062
Scott: Right.

833
00:54:56,403 --> 00:55:02,352
Scott: This spreadsheet was filled with data from humanity and therefore the flaws of humanity.

834
00:55:02,698 --> 00:55:02,820
Scott: Right.

835
00:55:02,920 --> 00:55:06,800
Scott: And the systems we already have make their way into the system.

836
00:55:06,941 --> 00:55:09,720
Scott: So it's like, you know, they don't just make their way into the system.

837
00:55:09,921 --> 00:55:11,460
Rym: They define the system.

838
00:55:11,621 --> 00:55:16,200
Rym: And then because of the scale of this computing, they amplify those flaws.

839
00:55:17,382 --> 00:55:27,399
Scott: Yeah, it's like you could be like, you know, let's say you wanted to have something that like the computer helped you to, you know, find the like, you know, decide to give someone mortgages or not.

840
00:55:27,541 --> 00:55:31,603
Scott: And it's like, well, the people who've been deciding to give mortgages or not all this time are racist.

841
00:55:32,045 --> 00:55:36,625
Scott: So if you fill it with the history of all mortgages, hand it out forever.

842
00:55:36,645 --> 00:55:38,060
Scott: And guess what?

843
00:55:38,442 --> 00:55:39,699
Scott: It's going to keep being racist.

844
00:55:39,901 --> 00:55:40,123
Scott: Right.

845
00:55:40,204 --> 00:55:41,939
Scott: And only white people are going to get mortgage.

846
00:55:42,581 --> 00:55:43,484
Rym: Yes.

847
00:55:43,504 --> 00:55:47,982
Rym: So and this leads into like you remember a way back, we barely talked about some geek guys.

848
00:55:48,023 --> 00:56:01,481
Rym: It was a long time ago, but Google Assistant, they announced a feature long time ago where Google Assistant could if you like used an app or like clicked on a restaurant and Google Maps and you wanted to make a reservation or like put in an order.

849
00:56:02,004 --> 00:56:21,540
Rym: And the only way you could interact with that business was via a phone number like they didn't have like a grubhub or like a web form to fill out or anything, then it would call that business and a human sounding voice would just interact with the person on the other end of the line to answer your question.

850
00:56:21,641 --> 00:56:22,920
Rym: Like, are you open today?

851
00:56:23,141 --> 00:56:26,698
Rym: It's Memorial Day and get the answer and give it back to you.

852
00:56:27,462 --> 00:56:29,660
Rym: And that opened a whole shitstorm.

853
00:56:29,740 --> 00:56:36,540
Rym: On one hand, businesses were afraid that they would be inundated with thousands and thousands of calls that were automated.

854
00:56:37,940 --> 00:56:42,583
Rym: Luddites were afraid that this would replace human communication, even though I kind of wanted to.

855
00:56:42,623 --> 00:56:46,364
Rym: Like, I don't want to waste a human's time calling them to make a dinner reservation.

856
00:56:46,424 --> 00:56:49,000
Rym: Like, I don't I don't want to waste humans sorting that out.

857
00:56:49,181 --> 00:56:51,020
Rym: We could sort that out easily with technology.

858
00:56:51,720 --> 00:56:52,320
Rym: It could be an interesting.

859
00:56:52,580 --> 00:56:54,239
Scott: Get a better website and it won't have to call you.

860
00:56:54,621 --> 00:56:55,239
Scott: Yeah, come on.

861
00:56:55,320 --> 00:56:55,723
Rym: Exactly.

862
00:56:55,784 --> 00:56:57,760
Rym: But most businesses don't have a better website.

863
00:56:58,000 --> 00:57:01,740
Rym: I mean, that's why things like Grubhub exist in the first place, unfortunately.

864
00:57:02,460 --> 00:57:03,160
Scott: Get your shit together.

865
00:57:03,301 --> 00:57:03,542
Rym: Yep.

866
00:57:03,963 --> 00:57:12,480
Rym: But in the end, Google kind of backed away from a lot of this and they agreed to, among other things, if Google Assistant calls a business, it will tell you it's a robot.

867
00:57:12,860 --> 00:57:16,479
Rym: It will like explicitly say what it is before it interacts with you.

868
00:57:17,361 --> 00:57:21,440
Rym: And that opened a whole bunch of ethical concerns that are still not fully explored.

869
00:57:21,862 --> 00:57:25,120
Rym: And that is like baby's first use of this kind of technology.

870
00:57:26,302 --> 00:57:26,583
Scott: Right.

871
00:57:27,205 --> 00:57:33,465
Scott: So regarding the actual news story guy, right, with the chatbot that is not sentient, but they believe it is.

872
00:57:33,485 --> 00:57:44,960
Scott: the major ethical concern that I see with that particular thing is that, you know, even though this person involved is, you know, I guess we'll keep using the word interesting.

873
00:57:45,121 --> 00:57:45,344
Scott: Yeah.

874
00:57:45,404 --> 00:57:46,171
Scott: Right.

875
00:57:46,252 --> 00:57:47,100
Scott: Unique individual.

876
00:57:48,802 --> 00:57:51,800
Scott: They are convinced that it's sentient, right?

877
00:57:52,562 --> 00:57:54,618
Rym: If it's all that, I would argue so.

878
00:57:55,441 --> 00:58:01,020
Rym: This person may have failed what I'm going to now call the reverse Turing test.

879
00:58:02,021 --> 00:58:16,340
Scott: Maybe, but regardless, this bot thing, right, that is not sentient is software that is capable of convincing some percentage of the world population that it is sentient.

880
00:58:16,740 --> 00:58:29,759
Rym: So if I loaded up the history of everything I've ever typed into this thing, say someone else did it nefarious, they could probably make a discord bot that could talk like if Scott messaged it, it would respond and very plausibly seem like me.

881
00:58:31,041 --> 00:58:37,620
Scott: Yeah, but I mean, this that in and of itself could be used for nefarious things.

882
00:58:37,820 --> 00:58:38,504
Rym: I mean, look at what.

883
00:58:38,544 --> 00:58:38,986
Rym: look at.

884
00:58:39,006 --> 00:58:43,080
Rym: look at the online discourse right now, the massive sea of disinformation, Russian bots everywhere.

885
00:58:43,641 --> 00:58:47,613
Rym: There are a lot of people who are fooled by bots on social media.

886
00:58:47,794 --> 00:58:56,463
Rym: already when other people will look at one sentence from one of those bots and their brain immediately says, wow, that's a bot.

887
00:58:57,005 --> 00:59:01,420
Rym: But this technology makes more people susceptible to being fooled.

888
00:59:02,600 --> 00:59:08,440
Scott: So here's a very you know, I think the I guess the saving grace is that it's very expensive to run this bot, at least for now.

889
00:59:09,000 --> 00:59:15,779
Scott: Now, but but here's here's a really simple case that's just so obvious of how someone could use this for evil.

890
00:59:16,301 --> 00:59:16,542
Scott: Right.

891
00:59:16,904 --> 00:59:22,500
Scott: You know, there's all those people who call up old people and try to trick them into thinking, you know, tech support or something.

892
00:59:22,580 --> 00:59:23,353
Scott: They see all their money.

893
00:59:23,556 --> 00:59:23,780
Scott: Right.

894
00:59:24,581 --> 00:59:26,840
Scott: And they have these call centers, you know, mostly in India.

895
00:59:27,182 --> 00:59:29,920
Scott: You can watch on YouTube all those channels or they bust those people.

896
00:59:30,260 --> 00:59:30,762
Scott: Right.

897
00:59:30,782 --> 00:59:38,320
Scott: And it's like the call centers are full of real humans right in India, you know, calling up, you know, citizens of other countries trying to scam them.

898
00:59:38,400 --> 00:59:44,964
Scott: And while this is effectively a mechanical bot, if you could get the bot to make those calls, you could get.

899
00:59:45,284 --> 00:59:55,140
Scott: now one person could scam the whole world, keep all the money and not have to hire any call center people or rent any office space.

900
00:59:55,740 --> 00:59:55,982
Scott: Right.

901
00:59:56,003 --> 00:59:57,680
Scott: Same thing that happened with spam.

902
00:59:58,440 --> 01:00:03,743
Rym: Spam appeared because you could just automate sending ridiculous emails to millions and millions of people.

903
01:00:04,487 --> 01:00:07,604
Rym: And if only one percent of them fall for it, you still make a ton of money.

904
01:00:07,624 --> 01:00:08,480
Rym: It costs you nothing.

905
01:00:08,842 --> 01:00:10,800
Rym: It's way cheaper than mass mailers.

906
01:00:10,860 --> 01:00:12,660
Rym: That used to be the way scams propagated.

907
01:00:13,580 --> 01:00:13,681
Scott: Right.

908
01:00:13,701 --> 01:00:15,880
Scott: And then how are the anti scam people?

909
01:00:16,220 --> 01:00:16,647
Scott: How are they?

910
01:00:16,708 --> 01:00:17,339
Scott: what are they going to do?

911
01:00:17,723 --> 01:00:17,845
Scott: How?

912
01:00:17,885 --> 01:00:18,776
Scott: what can they do about it?

913
01:00:19,120 --> 01:00:19,302
Scott: Right.

914
01:00:19,383 --> 01:00:21,600
Scott: It's like, you know, it's going to be really hard.

915
01:00:21,881 --> 01:00:23,792
Scott: It's not like you can call up, you know, the.

916
01:00:23,953 --> 01:00:25,300
Scott: I can't really be intimidated.

917
01:00:26,141 --> 01:00:34,260
Rym: Imagine these eyes start texting every old person in America pretending to be their son and saying that and I'm like, I'm in jail.

918
01:00:34,561 --> 01:00:35,719
Rym: I need 50 bucks for bail.

919
01:00:36,261 --> 01:00:37,106
Rym: Grandpa, help me.

920
01:00:37,871 --> 01:00:39,079
Scott: And it actually believed.

921
01:00:39,442 --> 01:00:39,603
Scott: Right.

922
01:00:39,623 --> 01:00:42,080
Scott: It actually talked like their grandchild.

923
01:00:42,441 --> 01:00:42,662
Scott: Right.

924
01:00:42,722 --> 01:00:48,419
Scott: Not just like a grandchild, but there's specifically like maybe new things about their grandchild.

925
01:00:48,540 --> 01:01:00,007
Rym: I mean, I have already had to block people I otherwise liked on social media networks like Twitter because I would see them in my mentions arguing with bots like, how do you not see?

926
01:01:00,028 --> 01:01:00,639
Rym: it's not a bot?

927
01:01:01,100 --> 01:01:07,099
Rym: The better this technology gets, the more people will fail that reverse Turing test and interact with bots they think are people.

928
01:01:08,763 --> 01:01:10,980
Scott: I think you could also see situations, right.

929
01:01:11,380 --> 01:01:17,068
Scott: You know, how many online platforms, right, whether it's a game or a social network or whatever.

930
01:01:17,088 --> 01:01:17,190
Scott: Right.

931
01:01:18,440 --> 01:01:19,465
Scott: You know, it's like you have.

932
01:01:19,666 --> 01:01:24,360
Scott: if you try to start something these days, it's really hard because like, you know, it's like I'll make a video platform.

933
01:01:24,540 --> 01:01:25,088
Scott: It's like YouTube exists.

934
01:01:25,108 --> 01:01:25,939
Scott: What are you going to do?

935
01:01:26,041 --> 01:01:26,182
Scott: Right.

936
01:01:26,202 --> 01:01:26,565
Scott: You need to.

937
01:01:26,868 --> 01:01:28,280
Scott: you got to have the network effect.

938
01:01:28,380 --> 01:01:28,522
Scott: Right.

939
01:01:28,542 --> 01:01:29,819
Scott: Get this user base up.

940
01:01:30,421 --> 01:01:30,702
Scott: Right.

941
01:01:30,722 --> 01:01:34,020
Scott: To then get your, you know, to have your platform be worth using.

942
01:01:34,060 --> 01:01:38,302
Scott: You can make the best video site in the universe, but it's not the software that makes it great.

943
01:01:38,443 --> 01:01:40,680
Scott: It's all the users and the content that they're uploading.

944
01:01:41,083 --> 01:01:42,279
Scott: And YouTube already has that.

945
01:01:42,401 --> 01:01:44,020
Scott: So it's like, you know, right.

946
01:01:44,320 --> 01:01:44,883
Scott: Or it's not?

947
01:01:45,043 --> 01:01:46,128
Scott: it's not the.

948
01:01:46,208 --> 01:01:50,320
Scott: I can make a Twitter really easily, but it doesn't have all the people that are on Twitter.

949
01:01:50,480 --> 01:01:51,303
Scott: So whatever.

950
01:01:52,065 --> 01:02:04,360
Scott: But what if I artificially filled up my brand new app, right, with a bunch of AI generated garbage, but it was actually legitimate content that people found interesting and believed was real people.

951
01:02:04,901 --> 01:02:16,882
Scott: And that caused I basically fake the network effect to give myself fake customers to defraud investors into thinking I have users and that I don't make a social media network.

952
01:02:17,584 --> 01:02:21,480
Rym: Dolly version 2.0 is putting great illustrations on it.

953
01:02:21,880 --> 01:02:31,280
Rym: And then your GPT 10,000 algorithm has a million people risk sharing and resharing and interacting with and responding to that original bot.

954
01:02:31,920 --> 01:02:34,317
Scott: You got your Twitch channel and you're trying to get whatever status.

955
01:02:34,942 --> 01:02:35,084
Scott: Right.

956
01:02:35,104 --> 01:02:36,740
Scott: You don't have enough users, but you could do it.

957
01:02:37,204 --> 01:02:38,378
Scott: The people do the botting.

958
01:02:38,640 --> 01:02:39,125
Scott: But guess what?

959
01:02:39,165 --> 01:02:40,720
Scott: The botting, you know, is kind of obvious.

960
01:02:40,961 --> 01:02:48,990
Scott: But what if all those bots are chatting in Twitch chat in a human believable way and nobody can tell that those are not Twitch?

961
01:02:49,090 --> 01:02:51,860
Rym: and we are already seeing this today.

962
01:02:52,401 --> 01:02:58,679
Rym: It's just if you listen to GeekNights, you're probably the kind of person who recognizes when a bot comes into the chat.

963
01:02:59,241 --> 01:03:02,180
Rym: But a lot of people already today don't.

964
01:03:03,321 --> 01:03:03,502
Scott: Nope.

965
01:03:04,144 --> 01:03:21,565
Scott: And so, yeah, the fact that the bot is now advanced to the level where the AI researcher person, whatever issues they may have, believes it's sentient, means it's too advanced to probably be safe for anyone to be using without oversight and regulation or at all.

966
01:03:21,766 --> 01:03:23,293
Scott: Like, what do you what's even?

967
01:03:23,373 --> 01:03:24,760
Scott: what's the legitimate use of this thing?

968
01:03:25,441 --> 01:03:25,561
Scott: Right.

969
01:03:25,581 --> 01:03:30,380
Rym: It's like, yeah, I so I can so forget ethics.

970
01:03:30,683 --> 01:03:32,160
Rym: Here are some fascinating uses.

971
01:03:32,761 --> 01:03:38,360
Rym: One, scaling out various types of entrapment schemes from law enforcement at scale, like.

972
01:03:38,360 --> 01:03:39,163
Rym: that's a way to do it.

973
01:03:39,665 --> 01:03:55,000
Rym: They go out to find people who are prone to, say, engaging in right wing extremism and then basically act like you're basically pulling them into a fake network of similar minded people when actually it's all just bots until eventually they do something.

974
01:03:55,160 --> 01:03:55,800
Scott: Why don't you do that then?

975
01:03:56,100 --> 01:03:57,598
Rym: I mean, you could probably could.

976
01:03:58,064 --> 01:03:59,059
Scott: I'm saying why don't you do that?

977
01:03:59,644 --> 01:04:00,439
Rym: Because I got a day job.

978
01:04:00,680 --> 01:04:01,610
Scott: Good idea.

979
01:04:01,651 --> 01:04:02,500
Rym: I'm not saying it's a good idea.

980
01:04:02,580 --> 01:04:04,759
Rym: I'm saying it's these are practicable ideas.

981
01:04:06,923 --> 01:04:13,559
Rym: OK, an example of an app you could sell, sell an app where you load it with all your own information and you use it.

982
01:04:14,080 --> 01:04:18,658
Rym: You pick people on your contact list, like, say, you're the kind of person who doesn't like to talk to your mom.

983
01:04:19,561 --> 01:04:26,398
Rym: You tell it to maintain your relationship with your mom and just give you a feed of updates of what your mom's up to.

984
01:04:26,761 --> 01:04:30,398
Rym: But it talks to your mom on a day to day basis and she doesn't realize it.

985
01:04:33,006 --> 01:04:33,859
Scott: It seems like a bad idea.

986
01:04:34,282 --> 01:04:35,540
Rym: I didn't say it's a good idea.

987
01:04:35,560 --> 01:04:39,440
Rym: I said it's a practicable idea that I think you can make money selling such an app.

988
01:04:39,520 --> 01:04:40,818
Rym: And I think a lot of people would buy it.

989
01:04:42,601 --> 01:04:44,979
Scott: Yeah, and then it tells your mom something not great.

990
01:04:45,540 --> 01:04:49,293
Rym: Yeah, well, that's when the person who wrote who made the app.

991
01:04:49,915 --> 01:04:55,239
Rym: one starts deep faking you and all your friends and scams them for money and to blackmail you.

992
01:04:57,340 --> 01:05:06,280
Scott: So this technology, you also have to realize that, like, you know, if you any such at least with the current levels of technology, performance wise, right.

993
01:05:06,961 --> 01:05:10,740
Scott: Such a bot, if you were to use it in that way, would have to run in the cloud.

994
01:05:10,800 --> 01:05:12,519
Scott: You could run the whole thing on someone's job.

995
01:05:13,060 --> 01:05:13,201
Scott: Right.

996
01:05:13,562 --> 01:05:23,340
Scott: So you're going to have to send all your chats and all your mom's chats, all your full mom conversation to this cloud service unencrypted so that they can process it.

997
01:05:23,682 --> 01:05:23,924
Scott: Oh, yeah.

998
01:05:23,945 --> 01:05:25,560
Scott: And now you're basically right.

999
01:05:25,680 --> 01:05:39,960
Scott: So now whoever runs this service, even if the service itself and what it does for the customer, we agree is legitimate and ethical, you now have a company that has access to all of these private conversations in an unencrypted fashion.

1000
01:05:40,701 --> 01:05:41,002
Scott: Right.

1001
01:05:41,363 --> 01:05:48,540
Scott: And even if they have a good privacy policy and abide by it, it is dangerous to have all of that data in a place.

1002
01:05:48,620 --> 01:05:48,922
Scott: Yep.

1003
01:05:49,083 --> 01:05:52,280
Rym: And there are technological solutions to that problem.

1004
01:05:52,541 --> 01:06:00,519
Rym: But if the Internet of Things has taught us anything, no controls will ever be put on top of pretty much anything anyone builds.

1005
01:06:03,461 --> 01:06:06,120
Scott: Anyway, do we have anything else to say because I'm hungry?

1006
01:06:06,845 --> 01:06:07,880
Rym: I think we can wrap up there.

1007
01:06:08,060 --> 01:06:11,119
Rym: So we should be on our pretty much regular GeekNights schedule for the foreseeable future.

1008
01:06:12,963 --> 01:06:16,399
Rym: unless the Rangers make the Stanley Cup again in the foreseeable future.

1009
01:06:16,600 --> 01:06:17,919
Scott: It's not going to happen for another 12 months.

1010
01:06:18,401 --> 01:06:18,483
Scott: Yeah.

1011
01:06:18,625 --> 01:06:19,619
Scott: If it happens at all.

1012
01:06:20,040 --> 01:06:20,361
Scott: Right.

1013
01:06:20,381 --> 01:06:27,260
Rym: So this has been GeekNights with Rym and Scott.

1014
01:06:27,440 --> 01:06:32,563
Rym: Special thanks to DJ Pretzel for the opening music, Kat Lee for web design and Brando K for the logos.

1015
01:06:32,764 --> 01:06:37,778
Scott: Be sure to visit our website at FrontRowCrew.com for show notes, discussion, news and more.

1016
01:06:38,100 --> 01:06:45,439
Rym: Remember, GeekNights is not one, but four different shows, SciTech Mondays, Gaming Tuesdays, Anime Comic Wednesdays and Indiscriminate Thursdays.

1017
01:06:45,860 --> 01:06:48,996
Scott: GeekNights is distributed under a Creative Commons Attribution 3.0 license.

1018
01:06:50,280 --> 01:06:53,320
Scott: GeekNights is recorded live with no studio and no audience.

1019
01:06:53,520 --> 01:06:56,420
Scott: But unlike those other late shows, it's actually recorded at night.

1020
01:06:56,740 --> 01:07:05,800
Rym: And the Patreon patrons for this episode of GeekNights are Alan Joyce, Link Eiji, Dread Loony, Tenebrae and a bunch of people who really don't want me to say their names.

1021
01:07:06,220 --> 01:07:16,823
Rym: Chris Adon, Chris Reimer, Clinton, Walton, Dex, Finn, Kishaya85, Rebecca Dunn, Review Mad Bull, 34 Cowards, Sam Erickson, Sharon Von Hurl, Stop All the Downloading, Taylor Braun.

1022
01:07:16,843 --> 01:07:18,800
Rym: The next station is DuPont, DuPont Station.

1023
01:07:19,523 --> 01:07:21,240
Rym: And several people whose names I will not say.

1024
01:07:21,640 --> 01:07:23,520
Rym: I simply leave you with an old classic.

1025
01:07:27,840 --> 01:07:30,280
Scott: Good afternoon and welcome to Brandy Park.

1026
01:07:30,480 --> 01:07:38,000
Scott: And you join us just as the competitors are running out onto the field on this lovely winter's afternoon with the going firm underfoot and very little sign of rain.

1027
01:07:38,460 --> 01:07:44,699
Rym: And it looks as though we're in for a splendid afternoon sport on this, the 127th Upper Class Twit of the Year show.

1028
01:07:45,260 --> 01:07:48,740
Scott: And there's a big crowd here today to see these prize idiots in action.

1029
01:07:49,080 --> 01:07:50,159
Scott: Vivian Smith by Smith.

1030
01:07:50,620 --> 01:07:52,719
Scott: He's in the Grenadier Guards and he can count up to four.

1031
01:07:53,240 --> 01:07:54,620
Scott: Simon Zink Trumpet Harris.

1032
01:07:54,721 --> 01:07:57,300
Scott: He's an old Etonian and married to a very attractive table lamp.

1033
01:07:57,700 --> 01:08:00,100
Scott: Nigel Incubator Jones, his best friend as a tree.

1034
01:08:00,421 --> 01:08:02,399
Scott: And in his spare time, he's a stockbroker.

1035
01:08:02,960 --> 01:08:04,220
Scott: Gervais Brooke-Hamster.

1036
01:08:04,440 --> 01:08:07,600
Scott: He's in the wine trade and his father uses him as a waste paper basket.

1037
01:08:08,280 --> 01:08:11,100
Scott: And finally, Oliver Singin-Mollusk, another old Etonian.

1038
01:08:11,320 --> 01:08:13,720
Scott: His father was a cabinet minister and his mother won the Derby.

1039
01:08:13,860 --> 01:08:16,380
Scott: And he's thought by many to be this year's Outstanding Twit.

1040
01:08:17,439 --> 01:08:19,819
Scott: And now the Twits are moving up to the starting line.

1041
01:08:19,899 --> 01:08:21,859
Scott: And any moment now, they'll be under starters orders.

1042
01:08:22,823 --> 01:08:24,540
Scott: I'm afraid they're facing the wrong way at the moment.

1043
01:08:24,540 --> 01:08:26,038
Scott: But the starter will soon sort this out.

1044
01:08:26,581 --> 01:08:28,800
Scott: And any moment now, we're going to have the big off.

1045
01:08:28,899 --> 01:08:30,580
Scott: This is always a tense moment.

1046
01:08:31,903 --> 01:08:32,799
Scott: And they're off!

